{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Using cached elasticsearch-7.8.0-py2.py3-none-any.whl (188 kB)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from elasticsearch) (1.25.9)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from elasticsearch) (2020.4.5.2)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.8.0\n",
      "Collecting elasticsearch_dsl\n",
      "  Using cached elasticsearch_dsl-7.2.1-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (2.8.1)\n",
      "Requirement already satisfied: elasticsearch<8.0.0,>=7.0.0 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (7.8.0)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch_dsl) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch_dsl) (1.25.9)\n",
      "Installing collected packages: elasticsearch-dsl\n",
      "Successfully installed elasticsearch-dsl-7.2.1\n",
      "Collecting pymed\n",
      "  Using cached pymed-0.8.9-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from pymed) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (2.9)\n",
      "Installing collected packages: pymed\n",
      "Successfully installed pymed-0.8.9\n",
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "Processing /home/ubuntu/.cache/pip/wheels/a4/9b/d5/85705a7ab783cd6f7bd718f01d3b1396272f30044e3c36401a/smart_open-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.14.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.5 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.5->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.5->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-2.1.0\n",
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "USE_SERVER = True\n",
    "if USE_SERVER:\n",
    "    !pip install elasticsearch\n",
    "    !pip install elasticsearch_dsl\n",
    "    !pip install pymed\n",
    "    !pip install gensim\n",
    "    !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, \\\n",
    "Input, Lambda, RepeatVector, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import utils\n",
    "import os\n",
    "from utils import PROJECT_ROOT, DATA_PATH\n",
    "import yuval_module.paper_source as PaperSource\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import py_4.get_mesh_vec as get_mesh_vec\n",
    "import py_3.sim_matrix_3 as sim_matrix_3\n",
    "import py_4.get_all_features as get_all_features \n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PULLING FROM S3\n",
      "FILE PULLED\n"
     ]
    }
   ],
   "source": [
    "FILE = \"enriched_labeled_dataset_large\" \n",
    "if os.path.exists(PROJECT_ROOT + DATA_PATH + FILE):\n",
    "    print(\"READING FROM LOCAL\")\n",
    "    if FILE.split(\".\")[1] == \"json\":\n",
    "        df = pd.read_json(PROJECT_ROOT + DATA_PATH + FILE)\n",
    "    else:\n",
    "        df = pd.read_csv(PROJECT_ROOT + DATA_PATH + FILE)\n",
    "    ps = PaperSource()\n",
    "else:\n",
    "    print(\"PULLING FROM S3\")\n",
    "    ps = sim_matrix_3.load_dataset(FILE)\n",
    "    df = ps.get_dataset()\n",
    "\n",
    "df.drop(columns=[\"last_author_country\"],inplace=True)\n",
    "df.rename(columns={'ORG_STATE':'last_author_country'},inplace=True)\n",
    "\n",
    "print(\"FILE PULLED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP_branch\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_core = pd.read_csv(\"data/train_set_author_names.csv\")[\"0\"]\n",
    "auth_eps = pd.read_csv(\"data/val_set_author_names.csv\")[\"0\"]\n",
    "auth_usecase = pd.read_csv(\"data/test_set_author_names.csv\")[\"0\"]\n",
    "\n",
    "selection_train = list(set(df['last_author_name']) - set(auth_usecase))[:4000]\n",
    "selection_test = list(set(df['last_author_name']) - set(auth_usecase))[4000:4500]\n",
    "\n",
    "df_nonan = df[np.invert(df['mesh'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/ubuntu/AYP/code/models/names_epochs_2_vectorSize_64_window_2.model' already exits. Using existing model to re-generate results.\n",
      "'/home/ubuntu/AYP/code/models/co_authors_epochs_2_vectorSize_64_window_2.model' already exits. Using existing model to re-generate results.\n"
     ]
    }
   ],
   "source": [
    "vae_features = get_all_features.VAE_Features(df_nonan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP_branch/code/py_4/get_all_features.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['co_authors']=df.authors.apply( lambda x: [i['name'] for i in x] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old scaler\n"
     ]
    }
   ],
   "source": [
    "train_examples = vae_features.get_all_features(df_nonan[df_nonan['last_author_name'].isin(selection_train)])\n",
    "\n",
    "X_train = tf.convert_to_tensor(train_examples)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19930"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tf.Tensor 'Cast_15:0' shape=(32, 64) dtype=float32>, <tf.Tensor 'Cast_16:0' shape=(32, 64) dtype=float32>]]\n"
     ]
    }
   ],
   "source": [
    "def sent_generator(X_train, batchsize):\n",
    "    # Create iterator that reads dataset file batch by batch \n",
    "    #your code here\n",
    "    #Suffle the data file\n",
    "#     indexes = np.random.choice (range(len(full)),len(full),False)\n",
    "    \n",
    "#     shuffled = full.iloc[indexes]\n",
    "\n",
    "    reader = [X_train[batchsize*i: batchsize*i +batchsize,:] for i in range(int(np.ceil(int(X_train.shape[0])//batchsize)))]\n",
    "\n",
    "    \n",
    "    for df in reader:\n",
    "        yield [tf.cast(df,dtype = tf.float32), tf.cast(df,dtype = tf.float32)] \n",
    "        \n",
    "print([a for a in sent_generator(train_examples,  32)][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = train_examples.shape[0]\n",
    "input_dim = train_examples.shape[1]\n",
    "hidden_dim = int(np.sqrt(input_dim))\n",
    "n_z = 8\n",
    "\n",
    "n_epoch = 100\n",
    "m = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples:  19930\n",
      "The input dim:  64\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of samples: \", num_sample)\n",
    "print(\"The input dim: \", input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(input_dim,))\n",
    "h_q = Dense(hidden_dim, activation='relu')(inputs)\n",
    "mu = Dense(n_z, activation='linear')(h_q)\n",
    "log_sigma = Dense(n_z, activation='linear')(h_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(args):\n",
    "    mu, log_sigma = args\n",
    "    eps = K.random_normal(shape=(m, n_z), mean=0., stddev=1.)\n",
    "    return mu + K.exp(log_sigma / 2) * eps\n",
    "\n",
    "\n",
    "# Sample z ~ Q(z|X)\n",
    "z = Lambda(sample_z)([mu, log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(X|z) -- decoder\n",
    "decoder_hidden = Dense(hidden_dim, activation='relu')\n",
    "decoder_out = Dense(input_dim, activation='linear')\n",
    "\n",
    "h_p = decoder_hidden(z)\n",
    "outputs = decoder_out(h_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_loss(y_true, y_pred):\n",
    "    # Return tensor filled with ones with shape equal generated sequence shape\n",
    "    return K.zeros_like(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        # Create tensor (batch_size, max_sequence_len) filled with ones to consider all elements of generated sequence \n",
    "        self.target_weights = tf.constant(np.ones((batch_size, input_dim)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # Compute sequence reconstruction loss\n",
    "        recon = tf.keras.losses.MSE(x,x_decoded_mean)\n",
    "        # Compute KL-divergence as Variational loss \n",
    "        kl_loss = -0.5 * K.sum(1 + log_sigma - K.square(mu) - K.exp(log_sigma), axis=-1)\n",
    "        # Composite loss (reconstruction loss + Variational loss)\n",
    "        return recon + kl_loss\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0] # input sequence\n",
    "        x_decoded_mean = inputs[1] # reconstructed sequence\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean) # Compute loss of the model\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape\n",
    "        return K.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64) (64, 64)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 8)            520         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 8)            72          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 8)            72          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (64, 8)              0           dense_18[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (64, 8)              72          lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (64, 64)             576         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_6 (Cus [(None, 64), (64, 64 0           input_6[0][0]                    \n",
      "                                                                 dense_21[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,312\n",
      "Trainable params: 1,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create custom layer for loss computing\n",
    "loss_layer = CustomVariationalLayer()([inputs, outputs])\n",
    "\n",
    "vae = Model(inputs, [loss_layer])\n",
    "# Use Adam optimizer with learning rate = 0.01\n",
    "opt = Adam(lr=0.01)\n",
    "vae.compile(optimizer=opt, loss=[zero_loss])\n",
    "# Show model structure\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(model_name):\n",
    "    filepath = f\"models/{model_name}.h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        # Check if directory exists\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        # If directory doesn't exist, create the directory\n",
    "        os.mkdir(directory)\n",
    "    # Save model states\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
    "    return checkpointer\n",
    "\n",
    "# Create model checkpointer\n",
    "checkpointer = create_model_checkpoint('vae_seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch:  0 -------\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len is not well defined for symbolic Tensors. (strided_slice_2488:0) Please call `x.shape` rather than `len(x)` for shape information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-0bba9798cea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Train model. Test and save model after every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     vae.fit_generator(sent_generator(X_train, batch_size),\n\u001b[0;32m----> 7\u001b[0;31m                       steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer])\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'vae_lstmFull32dim96hid.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;31m# build batch logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                     \u001b[0;31m# Handle data tensors support when no input given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;31m# step-size = 1 for data tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m     raise TypeError(\"len is not well defined for symbolic Tensors. ({}) \"\n\u001b[1;32m    740\u001b[0m                     \u001b[0;34m\"Please call `x.shape` rather than `len(x)` for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                     \"shape information.\".format(self.name))\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len is not well defined for symbolic Tensors. (strided_slice_2488:0) Please call `x.shape` rather than `len(x)` for shape information."
     ]
    }
   ],
   "source": [
    "nb_epoch = 25 # number of epochs for model training\n",
    "n_steps = X_train.shape[0] // batch_size # Number of steps per epoch\n",
    "for counter in range(nb_epoch):\n",
    "    print('-------epoch: ', counter, '-------')\n",
    "    # Train model. Test and save model after every epoch\n",
    "    vae.fit_generator(sent_generator(X_train, batch_size),\n",
    "                      steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer])\n",
    "vae.save(r'vae_lstmFull32dim96hid.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTHER OPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Casts a tensor to a new type.\n",
       "\n",
       "The operation casts `x` (in case of `Tensor`) or `x.values`\n",
       "(in case of `SparseTensor` or `IndexedSlices`) to `dtype`.\n",
       "\n",
       "For example:\n",
       "\n",
       "```python\n",
       "x = tf.constant([1.8, 2.2], dtype=tf.float32)\n",
       "tf.dtypes.cast(x, tf.int32)  # [1, 2], dtype=tf.int32\n",
       "```\n",
       "\n",
       "The operation supports data types (for `x` and `dtype`) of\n",
       "`uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`,\n",
       "`float16`, `float32`, `float64`, `complex64`, `complex128`, `bfloat16`.\n",
       "In case of casting from complex types (`complex64`, `complex128`) to real\n",
       "types, only the real part of `x` is returned. In case of casting from real\n",
       "types to complex types (`complex64`, `complex128`), the imaginary part of the\n",
       "returned value is set to `0`. The handling of complex types here matches the\n",
       "behavior of numpy.\n",
       "\n",
       "Args:\n",
       "  x: A `Tensor` or `SparseTensor` or `IndexedSlices` of numeric type. It could\n",
       "    be `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`,\n",
       "    `int64`, `float16`, `float32`, `float64`, `complex64`, `complex128`,\n",
       "    `bfloat16`.\n",
       "  dtype: The destination type. The list of supported dtypes is the same as\n",
       "    `x`.\n",
       "  name: A name for the operation (optional).\n",
       "\n",
       "Returns:\n",
       "  A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` and\n",
       "    same type as `dtype`.\n",
       "\n",
       "Raises:\n",
       "  TypeError: If `x` cannot be cast to the `dtype`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tf.cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.convert_to_tensor(train_examples, dtype = tf.float32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_examples)\n",
    "iter_ds = train_dataset.make_initializable_iterator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-2b2206fe2b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_logvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mz_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_logvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Sampling from random z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m       \u001b[0;31m# Default: V1-style Graph execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterating over `tf.Tensor`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    521\u001b[0m     raise errors.OperatorNotAllowedInGraphError(\n\u001b[1;32m    522\u001b[0m         \u001b[0;34m\"{} is not allowed in Graph execution. Use Eager execution or decorate\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \" this function with @tf.function.\".format(task))\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "mb_size = 64\n",
    "z_dim = 8\n",
    "X_dim = y_dim = X_train.shape[1]\n",
    "h_dim = 32\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = 64.0\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim // 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# =============================== Q(z|X) ======================================\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)\n",
    "    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu\n",
    "    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var // 2) * eps\n",
    "\n",
    "\n",
    "# =============================== P(X|z) ======================================\n",
    "\n",
    "P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "def P(z):\n",
    "    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)\n",
    "    logits = tf.matmul(h, P_W2) + P_b2\n",
    "    return logits\n",
    "\n",
    "\n",
    "# =============================== TRAINING ====================================\n",
    "\n",
    "z_mu, z_logvar = Q(X)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "_, logits = P(z_sample)\n",
    "\n",
    "# Sampling from random z\n",
    "X_samples, _ = P(z)\n",
    "\n",
    "# E[log P(X|z)]\n",
    "recon_loss = tf.keras.losses.MSE(X,logits)\n",
    "\n",
    "# D_KL(Q(z|X) || P(z)); calculate in closed form as both dist. are Gaussian\n",
    "kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "# VAE loss\n",
    "vae_loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "\n",
    "solver = tf.train.AdamOptimizer().minimize(vae_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(1000000):\n",
    "    X_mb, _ = iter_ds.get_next(mb_size)\n",
    "\n",
    "    _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_tensorflow_p36)",
   "language": "python",
   "name": "conda_aws_neuron_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
