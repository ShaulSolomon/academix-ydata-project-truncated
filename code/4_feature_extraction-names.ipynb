{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB for exploring feature extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "from py_4.feature_helper import get_names\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "from os import makedirs, path\n",
    "%load_ext autoreload\n",
    "%autoreload 2%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "epoches=2\n",
    "vector_size=20\n",
    "window=2\n",
    "parameters = (epoches,vector_size,window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, vector_size, window):\n",
    "    documents = [TaggedDocument(list(doc), [i]) for i, doc in enumerate(get_names())]\n",
    "    #print(documents)\n",
    "    model = Doc2Vec(documents, epochs=epochs, vector_size=vector_size, window=window, workers=1)\n",
    "    print(len(documents))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[TaggedDocument(words=['R', 'o', 'y', ' ', 'G', 'r', 'a', 'n', 'i', 't'], tags=[0]),\n TaggedDocument(words=['S', 'h', 'a', 'u', 'l', ' ', 'S', 'o', 'l', 'o', 'm', 'o', 'n'], tags=[1]),\n TaggedDocument(words=['c', 'l', 'a', 'r', 'k', 'e'], tags=[2]),\n TaggedDocument(words=['d', 'a', 'v', 'i', 'e', 's'], tags=[3]),\n TaggedDocument(words=['d', 'a', 'v', 'i', 's'], tags=[4]),\n TaggedDocument(words=['k', 'e', 'l', 'l', 'e', 'y'], tags=[5]),\n TaggedDocument(words=['k', 'e', 'l', 'l', 'y'], tags=[6]),\n TaggedDocument(words=['w', 'o', 'o', 'd'], tags=[7]),\n TaggedDocument(words=['m', 'a', 'c', 'd', 'o', 'n', 'a', 'l', 'd'], tags=[8]),\n TaggedDocument(words=['w', 'o', 'o', 'd', 's'], tags=[9]),\n TaggedDocument(words=['m', 'c', 'd', 'o', 'n', 'a', 'l', 'd'], tags=[10]),\n TaggedDocument(words=['r', 'o', 'g', 'e', 'r', 's'], tags=[11]),\n TaggedDocument(words=['t', 'h', 'o', 'm', 'p', 's', 'o', 'n'], tags=[12]),\n TaggedDocument(words=['r', 'o', 'd', 'g', 'e', 'r', 's'], tags=[13]),\n TaggedDocument(words=['c', 'o', 'o', 'k', 'e'], tags=[14]),\n TaggedDocument(words=['c', 'o', 'o', 'k'], tags=[15]),\n TaggedDocument(words=['s', 't', 'e', 'v', 'e', 'n', 's'], tags=[16])]"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "[TaggedDocument(list(doc), [i]) for i, doc in enumerate(get_names())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Generating model with epochs=2 vector_size=20 window=2\n19676\nSaved model to models/names_epochs_2_vectorSize_20_window_2.model\n"
    }
   ],
   "source": [
    "#create model if it doesn't already exist\n",
    "makedirs('models', exist_ok=True)\n",
    "\n",
    "model_path = 'models/names_epochs_%d_vectorSize_%d_window_%d.model' % parameters\n",
    "\n",
    "if path.exists(model_path):\n",
    "    print(f\"'{model_path}' already exits. Using existing model to re-generate results.\")\n",
    "    model = Doc2Vec.load(model_path)\n",
    "else:\n",
    "    print('Generating model with epochs=%d vector_size=%d window=%d' % parameters)\n",
    "    model = train_model(*parameters)\n",
    "    model.save(model_path)\n",
    "    print(f'Saved model to {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-0.0363353 ,  0.02401276, -0.05405439, -0.01508737,  0.00028305,\n        0.00556437, -0.05705069,  0.02164698,  0.02014845,  0.0292481 ,\n       -0.03013219,  0.00459367,  0.01585584,  0.04888349,  0.05226859,\n       -0.00317838, -0.01010966,  0.00289698, -0.0176934 , -0.01804718],\n      dtype=float32)"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "model.infer_vector([\"A\",\"r\",\"v\",\"a\",\"n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitnlpcondaedb4459334ba4ee28e4936a12647cfbd",
   "display_name": "Python 3.6.8 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}