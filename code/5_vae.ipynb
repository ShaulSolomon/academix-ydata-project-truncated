{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Using cached elasticsearch-7.8.0-py2.py3-none-any.whl (188 kB)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch) (1.25.8)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.8.0\n",
      "Collecting elasticsearch_dsl\n",
      "  Using cached elasticsearch_dsl-7.2.1-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (1.14.0)\n",
      "Requirement already satisfied: elasticsearch<8.0.0,>=7.0.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (7.8.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (2.8.1)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch_dsl) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch_dsl) (1.25.8)\n",
      "Installing collected packages: elasticsearch-dsl\n",
      "Successfully installed elasticsearch-dsl-7.2.1\n",
      "Collecting pymed\n",
      "  Using cached pymed-0.8.9-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pymed) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (3.0.4)\n",
      "Installing collected packages: pymed\n",
      "Successfully installed pymed-0.8.9\n",
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 29.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Using cached smart_open-2.1.0.tar.gz (116 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.14.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.5->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.5->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110317 sha256=de83e5c41cbe349a05fa85db4cd05793b9eb1c02dc87a18c399f3b5b0096e369\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a4/9b/d5/85705a7ab783cd6f7bd718f01d3b1396272f30044e3c36401a\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-2.1.0\n",
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "USE_SERVER = True\n",
    "if USE_SERVER:\n",
    "    !pip install elasticsearch\n",
    "    !pip install elasticsearch_dsl\n",
    "    !pip install pymed\n",
    "    !pip install gensim\n",
    "    !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import utils\n",
    "import os\n",
    "from utils import PROJECT_ROOT, DATA_PATH\n",
    "import yuval_module.paper_source as PaperSource\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import py_4.get_mesh_vec as get_mesh_vec\n",
    "import py_3.sim_matrix_3 as sim_matrix_3\n",
    "import py_4.get_all_features as get_all_features \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_embed=get_mesh_vec.MeshEmbeddings(PROJECT_ROOT + \"data/mesh_data/MeSHFeatureGeneratedByDeepWalk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PULLING FROM S3\n",
      "FILE PULLED\n"
     ]
    }
   ],
   "source": [
    "FILE = \"enriched_labeled_dataset_large\" \n",
    "if os.path.exists(PROJECT_ROOT + DATA_PATH + FILE):\n",
    "    print(\"READING FROM LOCAL\")\n",
    "    if FILE.split(\".\")[1] == \"json\":\n",
    "        df = pd.read_json(PROJECT_ROOT + DATA_PATH + FILE)\n",
    "    else:\n",
    "        df = pd.read_csv(PROJECT_ROOT + DATA_PATH + FILE)\n",
    "    #ps = PaperSource()\n",
    "else:\n",
    "    print(\"PULLING FROM S3\")\n",
    "    ps = sim_matrix_3.load_dataset(FILE)\n",
    "    df = ps.get_dataset()\n",
    "\n",
    "df.drop(columns=[\"last_author_country\"],inplace=True)\n",
    "df.rename(columns={'ORG_STATE':'last_author_country'},inplace=True)\n",
    "\n",
    "print(\"FILE PULLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_core = pd.read_csv(\"data/train_set_author_names.csv\")[\"0\"]\n",
    "auth_eps = pd.read_csv(\"data/val_set_author_names.csv\")[\"0\"]\n",
    "auth_usecase = pd.read_csv(\"data/test_set_author_names.csv\")[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_train = list(set(df['last_author_name']) - set(auth_usecase))[:2000]\n",
    "selection_test = list(set(df['last_author_name']) - set(auth_usecase))[2000:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#settings\n",
    "\n",
    "BATCH_SIZE= 8\n",
    "EPOCHS = 30\n",
    "cuda = torch.cuda.is_available()\n",
    "seed = 42\n",
    "log_interval = 5\n",
    "num_workers = 2\n",
    "\n",
    "#check for cuda\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDS(Dataset):\n",
    "    def __init__(self,df,selection,vae_features = None):\n",
    "        super().__init__()\n",
    "        self.df = df[df['last_author_name'].isin(selection)]\n",
    "        if vae_features is None:\n",
    "            print(\"Creating new VAE FEATURES\")\n",
    "            self.vae_features = get_all_features.VAE_Features(self.df)\n",
    "        else:\n",
    "            print(\"Using pre-defined VAE FEATURES\")\n",
    "            self.vae_features = vae_features\n",
    "        self.features = self.vae_features.get_all_features(self.df)\n",
    "        print(list(self.vae_features.mesh_features.mesh_missing))\n",
    "        self.input_dim = self.vae_features.input_dims\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        return features\n",
    "    \n",
    "    def __getvae__(self):\n",
    "        return self.vae_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new VAE FEATURES\n",
      "'/home/ubuntu/AYP/code/models/names_epochs_2_vectorSize_64_window_2.model' already exits. Using existing model to re-generate results.\n",
      "'/home/ubuntu/AYP/code/models/co_authors_epochs_2_vectorSize_64_window_2.model' already exits. Using existing model to re-generate results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP/code/py_4/get_all_features.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['co_authors']=df.authors.apply( lambda x: [i['name'] for i in x] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining new scaler\n",
      "['MESH NAME NOT FOUND: Mitophagy', 'MESH NAME NOT FOUND: Female', \"MESH NAME NOT FOUND: Practice Patterns, Physicians'\", 'MESH NAME NOT FOUND: Broadly Neutralizing Antibodies', 'MESH NAME NOT FOUND: B-Cell Lymphoma 3 Protein', 'MESH NAME NOT FOUND: Conditioning, Psychological', 'MESH NAME NOT FOUND: Inhibition, Psychological', 'MESH NAME NOT FOUND: Chlorocebus aethiops', 'MESH NAME NOT FOUND: Confounding Factors, Epidemiologic', 'MESH NAME NOT FOUND: Male', 'MESH NAME NOT FOUND: Recognition, Psychology', 'MESH NAME NOT FOUND: Phosphoinositide-3 Kinase Inhibitors', 'MESH NAME NOT FOUND: Infections', 'MESH NAME NOT FOUND: Stimuli Responsive Polymers', 'MESH NAME NOT FOUND: Copper-Transporting ATPases']\n"
     ]
    }
   ],
   "source": [
    "train_set = ToyDS(df, selection_train)\n",
    "train_loader=DataLoader(dataset= train_set, batch_size = batch_size, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features:  64\n",
      "The number of train data:  11051\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of features: \",train_set.input_dim)\n",
    "print(\"The number of train data: \",train_set.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim = train_set.input_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = nn.Linear(self.input_dim, 64)\n",
    "        self.fc21 = nn.Linear(64, 32)\n",
    "        self.fc22 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 64)\n",
    "        self.fc4 = nn.Linear(64, self.input_dim)\n",
    "        \n",
    "        #Want to initialize logvar weights to 0\n",
    "#         self.fc22.weight.data.fill_(0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar,epoch):\n",
    "    #BCE = F.binary_cross_entropy(recon_x, x.view(-1, 64), reduction='sum')\n",
    "\n",
    "    MSE = F.mse_loss(recon_x, x.view(-1, train_set.input_dim))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD /= BATCH_SIZE * train_set.input_dim\n",
    "    \n",
    "    gamma = min(1.0,np.log(epoch+1))\n",
    "    \n",
    "    gamma = 0\n",
    "\n",
    "    return MSE + gamma*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]           4,160\n",
      "            Linear-2                   [-1, 32]           2,080\n",
      "            Linear-3                   [-1, 32]           2,080\n",
      "            Linear-4                   [-1, 64]           2,112\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "================================================================\n",
      "Total params: 14,592\n",
      "Trainable params: 14,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (20,train_set.input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(tr_loader, model, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    train_log = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Started training epoch no. {}\".format(epoch+1))\n",
    "        train_loss= 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data.to(device, dtype=torch.float32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar, z = model(data)\n",
    "            loss = criterion(recon_batch, data, mu, logvar,epoch)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data)))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}\\n'.format(\n",
    "            epoch, train_loss / len(train_loader.dataset)))\n",
    "        train_log.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss / len(train_loader)})\n",
    "    return train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader, model):\n",
    "    model.eval()\n",
    "    recon_batchs = []\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device, dtype=torch.float32)\n",
    "            recon_batch, mu, logvar, _ = model(data)\n",
    "            if eval:\n",
    "                recon_batchs.extend(recon_batch.cpu().detach().numpy())\n",
    "            else:\n",
    "                 recon_batchs.extend(recon_batch.detach().numpy())\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar,0).item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return np.array(recon_batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training epoch no. 1\n",
      "Train Epoch: 0 [0/11051 (0%)]\tLoss: 0.087897\n",
      "Train Epoch: 0 [80/11051 (1%)]\tLoss: 0.147597\n",
      "Train Epoch: 0 [160/11051 (1%)]\tLoss: 0.182933\n",
      "Train Epoch: 0 [240/11051 (2%)]\tLoss: 0.137266\n",
      "Train Epoch: 0 [320/11051 (3%)]\tLoss: 0.100427\n",
      "Train Epoch: 0 [400/11051 (4%)]\tLoss: 0.088912\n",
      "Train Epoch: 0 [480/11051 (4%)]\tLoss: 0.082911\n",
      "Train Epoch: 0 [560/11051 (5%)]\tLoss: 0.054997\n",
      "Train Epoch: 0 [640/11051 (6%)]\tLoss: 0.124801\n",
      "Train Epoch: 0 [720/11051 (7%)]\tLoss: 0.118962\n",
      "Train Epoch: 0 [800/11051 (7%)]\tLoss: 0.058551\n",
      "Train Epoch: 0 [880/11051 (8%)]\tLoss: 0.093100\n",
      "Train Epoch: 0 [960/11051 (9%)]\tLoss: 0.105863\n",
      "Train Epoch: 0 [1040/11051 (9%)]\tLoss: 0.059365\n",
      "Train Epoch: 0 [1120/11051 (10%)]\tLoss: 0.119079\n",
      "Train Epoch: 0 [1200/11051 (11%)]\tLoss: 0.041897\n",
      "Train Epoch: 0 [1280/11051 (12%)]\tLoss: 0.076726\n",
      "Train Epoch: 0 [1360/11051 (12%)]\tLoss: 0.112718\n",
      "Train Epoch: 0 [1440/11051 (13%)]\tLoss: 0.131187\n",
      "Train Epoch: 0 [1520/11051 (14%)]\tLoss: 0.089719\n",
      "Train Epoch: 0 [1600/11051 (14%)]\tLoss: 0.085727\n",
      "Train Epoch: 0 [1680/11051 (15%)]\tLoss: 0.055993\n",
      "Train Epoch: 0 [1760/11051 (16%)]\tLoss: 0.101858\n",
      "Train Epoch: 0 [1840/11051 (17%)]\tLoss: 0.074542\n",
      "Train Epoch: 0 [1920/11051 (17%)]\tLoss: 0.117166\n",
      "Train Epoch: 0 [2000/11051 (18%)]\tLoss: 0.052642\n",
      "Train Epoch: 0 [2080/11051 (19%)]\tLoss: 0.073981\n",
      "Train Epoch: 0 [2160/11051 (20%)]\tLoss: 0.094348\n",
      "Train Epoch: 0 [2240/11051 (20%)]\tLoss: 0.050088\n",
      "Train Epoch: 0 [2320/11051 (21%)]\tLoss: 0.081278\n",
      "Train Epoch: 0 [2400/11051 (22%)]\tLoss: 0.067175\n",
      "Train Epoch: 0 [2480/11051 (22%)]\tLoss: 0.090156\n",
      "Train Epoch: 0 [2560/11051 (23%)]\tLoss: 0.069412\n",
      "Train Epoch: 0 [2640/11051 (24%)]\tLoss: 0.079161\n",
      "Train Epoch: 0 [2720/11051 (25%)]\tLoss: 0.079082\n",
      "Train Epoch: 0 [2800/11051 (25%)]\tLoss: 0.060580\n",
      "Train Epoch: 0 [2880/11051 (26%)]\tLoss: 0.104857\n",
      "Train Epoch: 0 [2960/11051 (27%)]\tLoss: 0.110436\n",
      "Train Epoch: 0 [3040/11051 (27%)]\tLoss: 0.067679\n",
      "Train Epoch: 0 [3120/11051 (28%)]\tLoss: 0.089836\n",
      "Train Epoch: 0 [3200/11051 (29%)]\tLoss: 0.105440\n",
      "Train Epoch: 0 [3280/11051 (30%)]\tLoss: 0.053804\n",
      "Train Epoch: 0 [3360/11051 (30%)]\tLoss: 0.091389\n",
      "Train Epoch: 0 [3440/11051 (31%)]\tLoss: 0.087056\n",
      "Train Epoch: 0 [3520/11051 (32%)]\tLoss: 0.066532\n",
      "Train Epoch: 0 [3600/11051 (33%)]\tLoss: 0.050393\n",
      "Train Epoch: 0 [3680/11051 (33%)]\tLoss: 0.062445\n",
      "Train Epoch: 0 [3760/11051 (34%)]\tLoss: 0.045563\n",
      "Train Epoch: 0 [3840/11051 (35%)]\tLoss: 0.038310\n",
      "Train Epoch: 0 [3920/11051 (35%)]\tLoss: 0.067415\n",
      "Train Epoch: 0 [4000/11051 (36%)]\tLoss: 0.056580\n",
      "Train Epoch: 0 [4080/11051 (37%)]\tLoss: 0.095888\n",
      "Train Epoch: 0 [4160/11051 (38%)]\tLoss: 0.098113\n",
      "Train Epoch: 0 [4240/11051 (38%)]\tLoss: 0.027145\n",
      "Train Epoch: 0 [4320/11051 (39%)]\tLoss: 0.069591\n",
      "Train Epoch: 0 [4400/11051 (40%)]\tLoss: 0.059570\n",
      "Train Epoch: 0 [4480/11051 (41%)]\tLoss: 0.093273\n",
      "Train Epoch: 0 [4560/11051 (41%)]\tLoss: 0.075875\n",
      "Train Epoch: 0 [4640/11051 (42%)]\tLoss: 0.057462\n",
      "Train Epoch: 0 [4720/11051 (43%)]\tLoss: 0.058979\n",
      "Train Epoch: 0 [4800/11051 (43%)]\tLoss: 0.077257\n",
      "Train Epoch: 0 [4880/11051 (44%)]\tLoss: 0.146560\n",
      "Train Epoch: 0 [4960/11051 (45%)]\tLoss: 0.049367\n",
      "Train Epoch: 0 [5040/11051 (46%)]\tLoss: 0.058252\n",
      "Train Epoch: 0 [5120/11051 (46%)]\tLoss: 0.114834\n",
      "Train Epoch: 0 [5200/11051 (47%)]\tLoss: 0.053597\n",
      "Train Epoch: 0 [5280/11051 (48%)]\tLoss: 0.038883\n",
      "Train Epoch: 0 [5360/11051 (48%)]\tLoss: 0.064028\n",
      "Train Epoch: 0 [5440/11051 (49%)]\tLoss: 0.050774\n",
      "Train Epoch: 0 [5520/11051 (50%)]\tLoss: 0.066148\n",
      "Train Epoch: 0 [5600/11051 (51%)]\tLoss: 0.061729\n",
      "Train Epoch: 0 [5680/11051 (51%)]\tLoss: 0.036349\n",
      "Train Epoch: 0 [5760/11051 (52%)]\tLoss: 0.058286\n",
      "Train Epoch: 0 [5840/11051 (53%)]\tLoss: 0.022982\n",
      "Train Epoch: 0 [5920/11051 (54%)]\tLoss: 0.042953\n",
      "Train Epoch: 0 [6000/11051 (54%)]\tLoss: 0.052539\n",
      "Train Epoch: 0 [6080/11051 (55%)]\tLoss: 0.052942\n",
      "Train Epoch: 0 [6160/11051 (56%)]\tLoss: 0.108361\n",
      "Train Epoch: 0 [6240/11051 (56%)]\tLoss: 0.044219\n",
      "Train Epoch: 0 [6320/11051 (57%)]\tLoss: 0.046637\n",
      "Train Epoch: 0 [6400/11051 (58%)]\tLoss: 0.036660\n",
      "Train Epoch: 0 [6480/11051 (59%)]\tLoss: 0.021303\n",
      "Train Epoch: 0 [6560/11051 (59%)]\tLoss: 0.044698\n",
      "Train Epoch: 0 [6640/11051 (60%)]\tLoss: 0.069850\n",
      "Train Epoch: 0 [6720/11051 (61%)]\tLoss: 0.017095\n",
      "Train Epoch: 0 [6800/11051 (62%)]\tLoss: 0.051131\n",
      "Train Epoch: 0 [6880/11051 (62%)]\tLoss: 0.025989\n",
      "Train Epoch: 0 [6960/11051 (63%)]\tLoss: 0.031307\n",
      "Train Epoch: 0 [7040/11051 (64%)]\tLoss: 0.058059\n",
      "Train Epoch: 0 [7120/11051 (64%)]\tLoss: 0.052823\n",
      "Train Epoch: 0 [7200/11051 (65%)]\tLoss: 0.063584\n",
      "Train Epoch: 0 [7280/11051 (66%)]\tLoss: 0.023545\n",
      "Train Epoch: 0 [7360/11051 (67%)]\tLoss: 0.028869\n",
      "Train Epoch: 0 [7440/11051 (67%)]\tLoss: 0.051974\n",
      "Train Epoch: 0 [7520/11051 (68%)]\tLoss: 0.049664\n",
      "Train Epoch: 0 [7600/11051 (69%)]\tLoss: 0.038422\n",
      "Train Epoch: 0 [7680/11051 (69%)]\tLoss: 0.064044\n",
      "Train Epoch: 0 [7760/11051 (70%)]\tLoss: 0.035389\n",
      "Train Epoch: 0 [7840/11051 (71%)]\tLoss: 0.066879\n",
      "Train Epoch: 0 [7920/11051 (72%)]\tLoss: 0.047115\n",
      "Train Epoch: 0 [8000/11051 (72%)]\tLoss: 0.039514\n",
      "Train Epoch: 0 [8080/11051 (73%)]\tLoss: 0.060134\n",
      "Train Epoch: 0 [8160/11051 (74%)]\tLoss: 0.040777\n",
      "Train Epoch: 0 [8240/11051 (75%)]\tLoss: 0.040338\n",
      "Train Epoch: 0 [8320/11051 (75%)]\tLoss: 0.025186\n",
      "Train Epoch: 0 [8400/11051 (76%)]\tLoss: 0.051813\n",
      "Train Epoch: 0 [8480/11051 (77%)]\tLoss: 0.029284\n",
      "Train Epoch: 0 [8560/11051 (77%)]\tLoss: 0.032120\n",
      "Train Epoch: 0 [8640/11051 (78%)]\tLoss: 0.086118\n",
      "Train Epoch: 0 [8720/11051 (79%)]\tLoss: 0.092946\n",
      "Train Epoch: 0 [8800/11051 (80%)]\tLoss: 0.080792\n",
      "Train Epoch: 0 [8880/11051 (80%)]\tLoss: 0.063997\n",
      "Train Epoch: 0 [8960/11051 (81%)]\tLoss: 0.035747\n",
      "Train Epoch: 0 [9040/11051 (82%)]\tLoss: 0.016701\n",
      "Train Epoch: 0 [9120/11051 (82%)]\tLoss: 0.041615\n",
      "Train Epoch: 0 [9200/11051 (83%)]\tLoss: 0.033329\n",
      "Train Epoch: 0 [9280/11051 (84%)]\tLoss: 0.024176\n",
      "Train Epoch: 0 [9360/11051 (85%)]\tLoss: 0.074653\n",
      "Train Epoch: 0 [9440/11051 (85%)]\tLoss: 0.055641\n",
      "Train Epoch: 0 [9520/11051 (86%)]\tLoss: 0.051027\n",
      "Train Epoch: 0 [9600/11051 (87%)]\tLoss: 0.058646\n",
      "Train Epoch: 0 [9680/11051 (88%)]\tLoss: 0.080126\n",
      "Train Epoch: 0 [9760/11051 (88%)]\tLoss: 0.042612\n",
      "Train Epoch: 0 [9840/11051 (89%)]\tLoss: 0.060063\n",
      "Train Epoch: 0 [9920/11051 (90%)]\tLoss: 0.056450\n",
      "Train Epoch: 0 [10000/11051 (90%)]\tLoss: 0.075409\n",
      "Train Epoch: 0 [10080/11051 (91%)]\tLoss: 0.019049\n",
      "Train Epoch: 0 [10160/11051 (92%)]\tLoss: 0.061153\n",
      "Train Epoch: 0 [10240/11051 (93%)]\tLoss: 0.076523\n",
      "Train Epoch: 0 [10320/11051 (93%)]\tLoss: 0.060027\n",
      "Train Epoch: 0 [10400/11051 (94%)]\tLoss: 0.030508\n",
      "Train Epoch: 0 [10480/11051 (95%)]\tLoss: 0.022219\n",
      "Train Epoch: 0 [10560/11051 (96%)]\tLoss: 0.011613\n",
      "Train Epoch: 0 [10640/11051 (96%)]\tLoss: 0.039404\n",
      "Train Epoch: 0 [10720/11051 (97%)]\tLoss: 0.049809\n",
      "Train Epoch: 0 [10800/11051 (98%)]\tLoss: 0.051218\n",
      "Train Epoch: 0 [10880/11051 (98%)]\tLoss: 0.036438\n",
      "Train Epoch: 0 [10960/11051 (99%)]\tLoss: 0.048724\n",
      "Train Epoch: 0 [11040/11051 (100%)]\tLoss: 0.078343\n",
      "====> Epoch: 0 Average loss: 0.0646\n",
      "\n",
      "Started training epoch no. 2\n",
      "Train Epoch: 1 [0/11051 (0%)]\tLoss: 0.031140\n",
      "Train Epoch: 1 [80/11051 (1%)]\tLoss: 0.029379\n",
      "Train Epoch: 1 [160/11051 (1%)]\tLoss: 0.035698\n",
      "Train Epoch: 1 [240/11051 (2%)]\tLoss: 0.030786\n",
      "Train Epoch: 1 [320/11051 (3%)]\tLoss: 0.024462\n",
      "Train Epoch: 1 [400/11051 (4%)]\tLoss: 0.026415\n",
      "Train Epoch: 1 [480/11051 (4%)]\tLoss: 0.064226\n",
      "Train Epoch: 1 [560/11051 (5%)]\tLoss: 0.051364\n",
      "Train Epoch: 1 [640/11051 (6%)]\tLoss: 0.062182\n",
      "Train Epoch: 1 [720/11051 (7%)]\tLoss: 0.086367\n",
      "Train Epoch: 1 [800/11051 (7%)]\tLoss: 0.073023\n",
      "Train Epoch: 1 [880/11051 (8%)]\tLoss: 0.031117\n",
      "Train Epoch: 1 [960/11051 (9%)]\tLoss: 0.013687\n",
      "Train Epoch: 1 [1040/11051 (9%)]\tLoss: 0.029281\n",
      "Train Epoch: 1 [1120/11051 (10%)]\tLoss: 0.012451\n",
      "Train Epoch: 1 [1200/11051 (11%)]\tLoss: 0.045902\n",
      "Train Epoch: 1 [1280/11051 (12%)]\tLoss: 0.022081\n",
      "Train Epoch: 1 [1360/11051 (12%)]\tLoss: 0.039977\n",
      "Train Epoch: 1 [1440/11051 (13%)]\tLoss: 0.045813\n",
      "Train Epoch: 1 [1520/11051 (14%)]\tLoss: 0.107432\n",
      "Train Epoch: 1 [1600/11051 (14%)]\tLoss: 0.065002\n",
      "Train Epoch: 1 [1680/11051 (15%)]\tLoss: 0.035963\n",
      "Train Epoch: 1 [1760/11051 (16%)]\tLoss: 0.021283\n",
      "Train Epoch: 1 [1840/11051 (17%)]\tLoss: 0.072031\n",
      "Train Epoch: 1 [1920/11051 (17%)]\tLoss: 0.050627\n",
      "Train Epoch: 1 [2000/11051 (18%)]\tLoss: 0.039922\n",
      "Train Epoch: 1 [2080/11051 (19%)]\tLoss: 0.025514\n",
      "Train Epoch: 1 [2160/11051 (20%)]\tLoss: 0.037848\n",
      "Train Epoch: 1 [2240/11051 (20%)]\tLoss: 0.038975\n",
      "Train Epoch: 1 [2320/11051 (21%)]\tLoss: 0.057805\n",
      "Train Epoch: 1 [2400/11051 (22%)]\tLoss: 0.027028\n",
      "Train Epoch: 1 [2480/11051 (22%)]\tLoss: 0.045639\n",
      "Train Epoch: 1 [2560/11051 (23%)]\tLoss: 0.035475\n",
      "Train Epoch: 1 [2640/11051 (24%)]\tLoss: 0.043054\n",
      "Train Epoch: 1 [2720/11051 (25%)]\tLoss: 0.036889\n",
      "Train Epoch: 1 [2800/11051 (25%)]\tLoss: 0.039235\n",
      "Train Epoch: 1 [2880/11051 (26%)]\tLoss: 0.026073\n",
      "Train Epoch: 1 [2960/11051 (27%)]\tLoss: 0.059849\n",
      "Train Epoch: 1 [3040/11051 (27%)]\tLoss: 0.030482\n",
      "Train Epoch: 1 [3120/11051 (28%)]\tLoss: 0.039274\n",
      "Train Epoch: 1 [3200/11051 (29%)]\tLoss: 0.042128\n",
      "Train Epoch: 1 [3280/11051 (30%)]\tLoss: 0.048384\n",
      "Train Epoch: 1 [3360/11051 (30%)]\tLoss: 0.065427\n",
      "Train Epoch: 1 [3440/11051 (31%)]\tLoss: 0.031443\n",
      "Train Epoch: 1 [3520/11051 (32%)]\tLoss: 0.018855\n",
      "Train Epoch: 1 [3600/11051 (33%)]\tLoss: 0.030274\n",
      "Train Epoch: 1 [3680/11051 (33%)]\tLoss: 0.028308\n",
      "Train Epoch: 1 [3760/11051 (34%)]\tLoss: 0.041405\n",
      "Train Epoch: 1 [3840/11051 (35%)]\tLoss: 0.030614\n",
      "Train Epoch: 1 [3920/11051 (35%)]\tLoss: 0.039506\n",
      "Train Epoch: 1 [4000/11051 (36%)]\tLoss: 0.031352\n",
      "Train Epoch: 1 [4080/11051 (37%)]\tLoss: 0.040403\n",
      "Train Epoch: 1 [4160/11051 (38%)]\tLoss: 0.046161\n",
      "Train Epoch: 1 [4240/11051 (38%)]\tLoss: 0.051049\n",
      "Train Epoch: 1 [4320/11051 (39%)]\tLoss: 0.023704\n",
      "Train Epoch: 1 [4400/11051 (40%)]\tLoss: 0.045515\n",
      "Train Epoch: 1 [4480/11051 (41%)]\tLoss: 0.032626\n",
      "Train Epoch: 1 [4560/11051 (41%)]\tLoss: 0.017163\n",
      "Train Epoch: 1 [4640/11051 (42%)]\tLoss: 0.080567\n",
      "Train Epoch: 1 [4720/11051 (43%)]\tLoss: 0.065149\n",
      "Train Epoch: 1 [4800/11051 (43%)]\tLoss: 0.046369\n",
      "Train Epoch: 1 [4880/11051 (44%)]\tLoss: 0.032614\n",
      "Train Epoch: 1 [4960/11051 (45%)]\tLoss: 0.030230\n",
      "Train Epoch: 1 [5040/11051 (46%)]\tLoss: 0.031574\n",
      "Train Epoch: 1 [5120/11051 (46%)]\tLoss: 0.026312\n",
      "Train Epoch: 1 [5200/11051 (47%)]\tLoss: 0.046445\n",
      "Train Epoch: 1 [5280/11051 (48%)]\tLoss: 0.032518\n",
      "Train Epoch: 1 [5360/11051 (48%)]\tLoss: 0.018580\n",
      "Train Epoch: 1 [5440/11051 (49%)]\tLoss: 0.032755\n",
      "Train Epoch: 1 [5520/11051 (50%)]\tLoss: 0.032255\n",
      "Train Epoch: 1 [5600/11051 (51%)]\tLoss: 0.017147\n",
      "Train Epoch: 1 [5680/11051 (51%)]\tLoss: 0.027953\n",
      "Train Epoch: 1 [5760/11051 (52%)]\tLoss: 0.040923\n",
      "Train Epoch: 1 [5840/11051 (53%)]\tLoss: 0.049700\n",
      "Train Epoch: 1 [5920/11051 (54%)]\tLoss: 0.053062\n",
      "Train Epoch: 1 [6000/11051 (54%)]\tLoss: 0.031520\n",
      "Train Epoch: 1 [6080/11051 (55%)]\tLoss: 0.023510\n",
      "Train Epoch: 1 [6160/11051 (56%)]\tLoss: 0.047533\n",
      "Train Epoch: 1 [6240/11051 (56%)]\tLoss: 0.025746\n",
      "Train Epoch: 1 [6320/11051 (57%)]\tLoss: 0.012335\n",
      "Train Epoch: 1 [6400/11051 (58%)]\tLoss: 0.038061\n",
      "Train Epoch: 1 [6480/11051 (59%)]\tLoss: 0.044489\n",
      "Train Epoch: 1 [6560/11051 (59%)]\tLoss: 0.066158\n",
      "Train Epoch: 1 [6640/11051 (60%)]\tLoss: 0.040723\n",
      "Train Epoch: 1 [6720/11051 (61%)]\tLoss: 0.044619\n",
      "Train Epoch: 1 [6800/11051 (62%)]\tLoss: 0.038627\n",
      "Train Epoch: 1 [6880/11051 (62%)]\tLoss: 0.041856\n",
      "Train Epoch: 1 [6960/11051 (63%)]\tLoss: 0.043846\n",
      "Train Epoch: 1 [7040/11051 (64%)]\tLoss: 0.019731\n",
      "Train Epoch: 1 [7120/11051 (64%)]\tLoss: 0.014966\n",
      "Train Epoch: 1 [7200/11051 (65%)]\tLoss: 0.038497\n",
      "Train Epoch: 1 [7280/11051 (66%)]\tLoss: 0.022899\n",
      "Train Epoch: 1 [7360/11051 (67%)]\tLoss: 0.062055\n",
      "Train Epoch: 1 [7440/11051 (67%)]\tLoss: 0.037300\n",
      "Train Epoch: 1 [7520/11051 (68%)]\tLoss: 0.049476\n",
      "Train Epoch: 1 [7600/11051 (69%)]\tLoss: 0.025691\n",
      "Train Epoch: 1 [7680/11051 (69%)]\tLoss: 0.029200\n",
      "Train Epoch: 1 [7760/11051 (70%)]\tLoss: 0.040663\n",
      "Train Epoch: 1 [7840/11051 (71%)]\tLoss: 0.050994\n",
      "Train Epoch: 1 [7920/11051 (72%)]\tLoss: 0.055447\n",
      "Train Epoch: 1 [8000/11051 (72%)]\tLoss: 0.073012\n",
      "Train Epoch: 1 [8080/11051 (73%)]\tLoss: 0.013615\n",
      "Train Epoch: 1 [8160/11051 (74%)]\tLoss: 0.030875\n",
      "Train Epoch: 1 [8240/11051 (75%)]\tLoss: 0.024196\n",
      "Train Epoch: 1 [8320/11051 (75%)]\tLoss: 0.028127\n",
      "Train Epoch: 1 [8400/11051 (76%)]\tLoss: 0.029639\n",
      "Train Epoch: 1 [8480/11051 (77%)]\tLoss: 0.040052\n",
      "Train Epoch: 1 [8560/11051 (77%)]\tLoss: 0.030462\n",
      "Train Epoch: 1 [8640/11051 (78%)]\tLoss: 0.046481\n",
      "Train Epoch: 1 [8720/11051 (79%)]\tLoss: 0.052699\n",
      "Train Epoch: 1 [8800/11051 (80%)]\tLoss: 0.019865\n",
      "Train Epoch: 1 [8880/11051 (80%)]\tLoss: 0.056908\n",
      "Train Epoch: 1 [8960/11051 (81%)]\tLoss: 0.046013\n",
      "Train Epoch: 1 [9040/11051 (82%)]\tLoss: 0.024112\n",
      "Train Epoch: 1 [9120/11051 (82%)]\tLoss: 0.017295\n",
      "Train Epoch: 1 [9200/11051 (83%)]\tLoss: 0.041524\n",
      "Train Epoch: 1 [9280/11051 (84%)]\tLoss: 0.043262\n",
      "Train Epoch: 1 [9360/11051 (85%)]\tLoss: 0.020779\n",
      "Train Epoch: 1 [9440/11051 (85%)]\tLoss: 0.014078\n",
      "Train Epoch: 1 [9520/11051 (86%)]\tLoss: 0.029483\n",
      "Train Epoch: 1 [9600/11051 (87%)]\tLoss: 0.040046\n",
      "Train Epoch: 1 [9680/11051 (88%)]\tLoss: 0.030616\n",
      "Train Epoch: 1 [9760/11051 (88%)]\tLoss: 0.019655\n",
      "Train Epoch: 1 [9840/11051 (89%)]\tLoss: 0.052738\n",
      "Train Epoch: 1 [9920/11051 (90%)]\tLoss: 0.042614\n",
      "Train Epoch: 1 [10000/11051 (90%)]\tLoss: 0.016532\n",
      "Train Epoch: 1 [10080/11051 (91%)]\tLoss: 0.014386\n",
      "Train Epoch: 1 [10160/11051 (92%)]\tLoss: 0.025942\n",
      "Train Epoch: 1 [10240/11051 (93%)]\tLoss: 0.025712\n",
      "Train Epoch: 1 [10320/11051 (93%)]\tLoss: 0.049952\n",
      "Train Epoch: 1 [10400/11051 (94%)]\tLoss: 0.010686\n",
      "Train Epoch: 1 [10480/11051 (95%)]\tLoss: 0.013553\n",
      "Train Epoch: 1 [10560/11051 (96%)]\tLoss: 0.024393\n",
      "Train Epoch: 1 [10640/11051 (96%)]\tLoss: 0.029737\n",
      "Train Epoch: 1 [10720/11051 (97%)]\tLoss: 0.033963\n",
      "Train Epoch: 1 [10800/11051 (98%)]\tLoss: 0.013065\n",
      "Train Epoch: 1 [10880/11051 (98%)]\tLoss: 0.036117\n",
      "Train Epoch: 1 [10960/11051 (99%)]\tLoss: 0.021207\n",
      "Train Epoch: 1 [11040/11051 (100%)]\tLoss: 0.019251\n",
      "====> Epoch: 1 Average loss: 0.0357\n",
      "\n",
      "Started training epoch no. 3\n",
      "Train Epoch: 2 [0/11051 (0%)]\tLoss: 0.017694\n",
      "Train Epoch: 2 [80/11051 (1%)]\tLoss: 0.015661\n",
      "Train Epoch: 2 [160/11051 (1%)]\tLoss: 0.027777\n",
      "Train Epoch: 2 [240/11051 (2%)]\tLoss: 0.024760\n",
      "Train Epoch: 2 [320/11051 (3%)]\tLoss: 0.043931\n",
      "Train Epoch: 2 [400/11051 (4%)]\tLoss: 0.035084\n",
      "Train Epoch: 2 [480/11051 (4%)]\tLoss: 0.024190\n",
      "Train Epoch: 2 [560/11051 (5%)]\tLoss: 0.007264\n",
      "Train Epoch: 2 [640/11051 (6%)]\tLoss: 0.039813\n",
      "Train Epoch: 2 [720/11051 (7%)]\tLoss: 0.029439\n",
      "Train Epoch: 2 [800/11051 (7%)]\tLoss: 0.043578\n",
      "Train Epoch: 2 [880/11051 (8%)]\tLoss: 0.090259\n",
      "Train Epoch: 2 [960/11051 (9%)]\tLoss: 0.031044\n",
      "Train Epoch: 2 [1040/11051 (9%)]\tLoss: 0.029330\n",
      "Train Epoch: 2 [1120/11051 (10%)]\tLoss: 0.029516\n",
      "Train Epoch: 2 [1200/11051 (11%)]\tLoss: 0.015797\n",
      "Train Epoch: 2 [1280/11051 (12%)]\tLoss: 0.016989\n",
      "Train Epoch: 2 [1360/11051 (12%)]\tLoss: 0.015686\n",
      "Train Epoch: 2 [1440/11051 (13%)]\tLoss: 0.030397\n",
      "Train Epoch: 2 [1520/11051 (14%)]\tLoss: 0.049325\n",
      "Train Epoch: 2 [1600/11051 (14%)]\tLoss: 0.032515\n",
      "Train Epoch: 2 [1680/11051 (15%)]\tLoss: 0.018939\n",
      "Train Epoch: 2 [1760/11051 (16%)]\tLoss: 0.017382\n",
      "Train Epoch: 2 [1840/11051 (17%)]\tLoss: 0.027072\n",
      "Train Epoch: 2 [1920/11051 (17%)]\tLoss: 0.077726\n",
      "Train Epoch: 2 [2000/11051 (18%)]\tLoss: 0.017037\n",
      "Train Epoch: 2 [2080/11051 (19%)]\tLoss: 0.016424\n",
      "Train Epoch: 2 [2160/11051 (20%)]\tLoss: 0.018458\n",
      "Train Epoch: 2 [2240/11051 (20%)]\tLoss: 0.029632\n",
      "Train Epoch: 2 [2320/11051 (21%)]\tLoss: 0.013766\n",
      "Train Epoch: 2 [2400/11051 (22%)]\tLoss: 0.031872\n",
      "Train Epoch: 2 [2480/11051 (22%)]\tLoss: 0.021261\n",
      "Train Epoch: 2 [2560/11051 (23%)]\tLoss: 0.023647\n",
      "Train Epoch: 2 [2640/11051 (24%)]\tLoss: 0.026626\n",
      "Train Epoch: 2 [2720/11051 (25%)]\tLoss: 0.033881\n",
      "Train Epoch: 2 [2800/11051 (25%)]\tLoss: 0.032969\n",
      "Train Epoch: 2 [2880/11051 (26%)]\tLoss: 0.017832\n",
      "Train Epoch: 2 [2960/11051 (27%)]\tLoss: 0.027325\n",
      "Train Epoch: 2 [3040/11051 (27%)]\tLoss: 0.065539\n",
      "Train Epoch: 2 [3120/11051 (28%)]\tLoss: 0.006565\n",
      "Train Epoch: 2 [3200/11051 (29%)]\tLoss: 0.022173\n",
      "Train Epoch: 2 [3280/11051 (30%)]\tLoss: 0.040272\n",
      "Train Epoch: 2 [3360/11051 (30%)]\tLoss: 0.047133\n",
      "Train Epoch: 2 [3440/11051 (31%)]\tLoss: 0.020962\n",
      "Train Epoch: 2 [3520/11051 (32%)]\tLoss: 0.050398\n",
      "Train Epoch: 2 [3600/11051 (33%)]\tLoss: 0.039113\n",
      "Train Epoch: 2 [3680/11051 (33%)]\tLoss: 0.042620\n",
      "Train Epoch: 2 [3760/11051 (34%)]\tLoss: 0.022548\n",
      "Train Epoch: 2 [3840/11051 (35%)]\tLoss: 0.029550\n",
      "Train Epoch: 2 [3920/11051 (35%)]\tLoss: 0.041975\n",
      "Train Epoch: 2 [4000/11051 (36%)]\tLoss: 0.025200\n",
      "Train Epoch: 2 [4080/11051 (37%)]\tLoss: 0.026395\n",
      "Train Epoch: 2 [4160/11051 (38%)]\tLoss: 0.031882\n",
      "Train Epoch: 2 [4240/11051 (38%)]\tLoss: 0.029438\n",
      "Train Epoch: 2 [4320/11051 (39%)]\tLoss: 0.038803\n",
      "Train Epoch: 2 [4400/11051 (40%)]\tLoss: 0.022521\n",
      "Train Epoch: 2 [4480/11051 (41%)]\tLoss: 0.019726\n",
      "Train Epoch: 2 [4560/11051 (41%)]\tLoss: 0.016079\n",
      "Train Epoch: 2 [4640/11051 (42%)]\tLoss: 0.023846\n",
      "Train Epoch: 2 [4720/11051 (43%)]\tLoss: 0.024024\n",
      "Train Epoch: 2 [4800/11051 (43%)]\tLoss: 0.022076\n",
      "Train Epoch: 2 [4880/11051 (44%)]\tLoss: 0.020163\n",
      "Train Epoch: 2 [4960/11051 (45%)]\tLoss: 0.027073\n",
      "Train Epoch: 2 [5040/11051 (46%)]\tLoss: 0.031418\n",
      "Train Epoch: 2 [5120/11051 (46%)]\tLoss: 0.011803\n",
      "Train Epoch: 2 [5200/11051 (47%)]\tLoss: 0.035400\n",
      "Train Epoch: 2 [5280/11051 (48%)]\tLoss: 0.021862\n",
      "Train Epoch: 2 [5360/11051 (48%)]\tLoss: 0.017990\n",
      "Train Epoch: 2 [5440/11051 (49%)]\tLoss: 0.036710\n",
      "Train Epoch: 2 [5520/11051 (50%)]\tLoss: 0.030565\n",
      "Train Epoch: 2 [5600/11051 (51%)]\tLoss: 0.038523\n",
      "Train Epoch: 2 [5680/11051 (51%)]\tLoss: 0.021831\n",
      "Train Epoch: 2 [5760/11051 (52%)]\tLoss: 0.036272\n",
      "Train Epoch: 2 [5840/11051 (53%)]\tLoss: 0.030413\n",
      "Train Epoch: 2 [5920/11051 (54%)]\tLoss: 0.020613\n",
      "Train Epoch: 2 [6000/11051 (54%)]\tLoss: 0.013491\n",
      "Train Epoch: 2 [6080/11051 (55%)]\tLoss: 0.037248\n",
      "Train Epoch: 2 [6160/11051 (56%)]\tLoss: 0.045247\n",
      "Train Epoch: 2 [6240/11051 (56%)]\tLoss: 0.028393\n",
      "Train Epoch: 2 [6320/11051 (57%)]\tLoss: 0.024503\n",
      "Train Epoch: 2 [6400/11051 (58%)]\tLoss: 0.020601\n",
      "Train Epoch: 2 [6480/11051 (59%)]\tLoss: 0.034519\n",
      "Train Epoch: 2 [6560/11051 (59%)]\tLoss: 0.022750\n",
      "Train Epoch: 2 [6640/11051 (60%)]\tLoss: 0.023026\n",
      "Train Epoch: 2 [6720/11051 (61%)]\tLoss: 0.029445\n",
      "Train Epoch: 2 [6800/11051 (62%)]\tLoss: 0.012864\n",
      "Train Epoch: 2 [6880/11051 (62%)]\tLoss: 0.013133\n",
      "Train Epoch: 2 [6960/11051 (63%)]\tLoss: 0.036618\n",
      "Train Epoch: 2 [7040/11051 (64%)]\tLoss: 0.018514\n",
      "Train Epoch: 2 [7120/11051 (64%)]\tLoss: 0.025096\n",
      "Train Epoch: 2 [7200/11051 (65%)]\tLoss: 0.013911\n",
      "Train Epoch: 2 [7280/11051 (66%)]\tLoss: 0.046414\n",
      "Train Epoch: 2 [7360/11051 (67%)]\tLoss: 0.008327\n",
      "Train Epoch: 2 [7440/11051 (67%)]\tLoss: 0.036221\n",
      "Train Epoch: 2 [7520/11051 (68%)]\tLoss: 0.030756\n",
      "Train Epoch: 2 [7600/11051 (69%)]\tLoss: 0.019730\n",
      "Train Epoch: 2 [7680/11051 (69%)]\tLoss: 0.017650\n",
      "Train Epoch: 2 [7760/11051 (70%)]\tLoss: 0.037326\n",
      "Train Epoch: 2 [7840/11051 (71%)]\tLoss: 0.030168\n",
      "Train Epoch: 2 [7920/11051 (72%)]\tLoss: 0.024001\n",
      "Train Epoch: 2 [8000/11051 (72%)]\tLoss: 0.010546\n",
      "Train Epoch: 2 [8080/11051 (73%)]\tLoss: 0.015822\n",
      "Train Epoch: 2 [8160/11051 (74%)]\tLoss: 0.030465\n",
      "Train Epoch: 2 [8240/11051 (75%)]\tLoss: 0.014716\n",
      "Train Epoch: 2 [8320/11051 (75%)]\tLoss: 0.047586\n",
      "Train Epoch: 2 [8400/11051 (76%)]\tLoss: 0.025198\n",
      "Train Epoch: 2 [8480/11051 (77%)]\tLoss: 0.015622\n",
      "Train Epoch: 2 [8560/11051 (77%)]\tLoss: 0.024093\n",
      "Train Epoch: 2 [8640/11051 (78%)]\tLoss: 0.031881\n",
      "Train Epoch: 2 [8720/11051 (79%)]\tLoss: 0.020800\n",
      "Train Epoch: 2 [8800/11051 (80%)]\tLoss: 0.028972\n",
      "Train Epoch: 2 [8880/11051 (80%)]\tLoss: 0.035708\n",
      "Train Epoch: 2 [8960/11051 (81%)]\tLoss: 0.029041\n",
      "Train Epoch: 2 [9040/11051 (82%)]\tLoss: 0.039529\n",
      "Train Epoch: 2 [9120/11051 (82%)]\tLoss: 0.025423\n",
      "Train Epoch: 2 [9200/11051 (83%)]\tLoss: 0.039519\n",
      "Train Epoch: 2 [9280/11051 (84%)]\tLoss: 0.029765\n",
      "Train Epoch: 2 [9360/11051 (85%)]\tLoss: 0.023124\n",
      "Train Epoch: 2 [9440/11051 (85%)]\tLoss: 0.027269\n",
      "Train Epoch: 2 [9520/11051 (86%)]\tLoss: 0.011540\n",
      "Train Epoch: 2 [9600/11051 (87%)]\tLoss: 0.037239\n",
      "Train Epoch: 2 [9680/11051 (88%)]\tLoss: 0.030900\n",
      "Train Epoch: 2 [9760/11051 (88%)]\tLoss: 0.018933\n",
      "Train Epoch: 2 [9840/11051 (89%)]\tLoss: 0.020696\n",
      "Train Epoch: 2 [9920/11051 (90%)]\tLoss: 0.015867\n",
      "Train Epoch: 2 [10000/11051 (90%)]\tLoss: 0.014493\n",
      "Train Epoch: 2 [10080/11051 (91%)]\tLoss: 0.050018\n",
      "Train Epoch: 2 [10160/11051 (92%)]\tLoss: 0.028636\n",
      "Train Epoch: 2 [10240/11051 (93%)]\tLoss: 0.036072\n",
      "Train Epoch: 2 [10320/11051 (93%)]\tLoss: 0.031029\n",
      "Train Epoch: 2 [10400/11051 (94%)]\tLoss: 0.016466\n",
      "Train Epoch: 2 [10480/11051 (95%)]\tLoss: 0.042303\n",
      "Train Epoch: 2 [10560/11051 (96%)]\tLoss: 0.030070\n",
      "Train Epoch: 2 [10640/11051 (96%)]\tLoss: 0.043933\n",
      "Train Epoch: 2 [10720/11051 (97%)]\tLoss: 0.027703\n",
      "Train Epoch: 2 [10800/11051 (98%)]\tLoss: 0.017344\n",
      "Train Epoch: 2 [10880/11051 (98%)]\tLoss: 0.029461\n",
      "Train Epoch: 2 [10960/11051 (99%)]\tLoss: 0.019786\n",
      "Train Epoch: 2 [11040/11051 (100%)]\tLoss: 0.037568\n",
      "====> Epoch: 2 Average loss: 0.0280\n",
      "\n",
      "Started training epoch no. 4\n",
      "Train Epoch: 3 [0/11051 (0%)]\tLoss: 0.022607\n",
      "Train Epoch: 3 [80/11051 (1%)]\tLoss: 0.023858\n",
      "Train Epoch: 3 [160/11051 (1%)]\tLoss: 0.031588\n",
      "Train Epoch: 3 [240/11051 (2%)]\tLoss: 0.019185\n",
      "Train Epoch: 3 [320/11051 (3%)]\tLoss: 0.036738\n",
      "Train Epoch: 3 [400/11051 (4%)]\tLoss: 0.038968\n",
      "Train Epoch: 3 [480/11051 (4%)]\tLoss: 0.017473\n",
      "Train Epoch: 3 [560/11051 (5%)]\tLoss: 0.030373\n",
      "Train Epoch: 3 [640/11051 (6%)]\tLoss: 0.023048\n",
      "Train Epoch: 3 [720/11051 (7%)]\tLoss: 0.025442\n",
      "Train Epoch: 3 [800/11051 (7%)]\tLoss: 0.017853\n",
      "Train Epoch: 3 [880/11051 (8%)]\tLoss: 0.014647\n",
      "Train Epoch: 3 [960/11051 (9%)]\tLoss: 0.017712\n",
      "Train Epoch: 3 [1040/11051 (9%)]\tLoss: 0.045303\n",
      "Train Epoch: 3 [1120/11051 (10%)]\tLoss: 0.013585\n",
      "Train Epoch: 3 [1200/11051 (11%)]\tLoss: 0.010968\n",
      "Train Epoch: 3 [1280/11051 (12%)]\tLoss: 0.023485\n",
      "Train Epoch: 3 [1360/11051 (12%)]\tLoss: 0.026936\n",
      "Train Epoch: 3 [1440/11051 (13%)]\tLoss: 0.017809\n",
      "Train Epoch: 3 [1520/11051 (14%)]\tLoss: 0.023292\n",
      "Train Epoch: 3 [1600/11051 (14%)]\tLoss: 0.034875\n",
      "Train Epoch: 3 [1680/11051 (15%)]\tLoss: 0.019842\n",
      "Train Epoch: 3 [1760/11051 (16%)]\tLoss: 0.017751\n",
      "Train Epoch: 3 [1840/11051 (17%)]\tLoss: 0.020610\n",
      "Train Epoch: 3 [1920/11051 (17%)]\tLoss: 0.017578\n",
      "Train Epoch: 3 [2000/11051 (18%)]\tLoss: 0.041046\n",
      "Train Epoch: 3 [2080/11051 (19%)]\tLoss: 0.048968\n",
      "Train Epoch: 3 [2160/11051 (20%)]\tLoss: 0.030950\n",
      "Train Epoch: 3 [2240/11051 (20%)]\tLoss: 0.012907\n",
      "Train Epoch: 3 [2320/11051 (21%)]\tLoss: 0.007951\n",
      "Train Epoch: 3 [2400/11051 (22%)]\tLoss: 0.028493\n",
      "Train Epoch: 3 [2480/11051 (22%)]\tLoss: 0.014006\n",
      "Train Epoch: 3 [2560/11051 (23%)]\tLoss: 0.024488\n",
      "Train Epoch: 3 [2640/11051 (24%)]\tLoss: 0.025518\n",
      "Train Epoch: 3 [2720/11051 (25%)]\tLoss: 0.021960\n",
      "Train Epoch: 3 [2800/11051 (25%)]\tLoss: 0.032405\n",
      "Train Epoch: 3 [2880/11051 (26%)]\tLoss: 0.009768\n",
      "Train Epoch: 3 [2960/11051 (27%)]\tLoss: 0.024103\n",
      "Train Epoch: 3 [3040/11051 (27%)]\tLoss: 0.018016\n",
      "Train Epoch: 3 [3120/11051 (28%)]\tLoss: 0.026555\n",
      "Train Epoch: 3 [3200/11051 (29%)]\tLoss: 0.017617\n",
      "Train Epoch: 3 [3280/11051 (30%)]\tLoss: 0.030019\n",
      "Train Epoch: 3 [3360/11051 (30%)]\tLoss: 0.019255\n",
      "Train Epoch: 3 [3440/11051 (31%)]\tLoss: 0.016662\n",
      "Train Epoch: 3 [3520/11051 (32%)]\tLoss: 0.026001\n",
      "Train Epoch: 3 [3600/11051 (33%)]\tLoss: 0.016527\n",
      "Train Epoch: 3 [3680/11051 (33%)]\tLoss: 0.009116\n",
      "Train Epoch: 3 [3760/11051 (34%)]\tLoss: 0.028661\n",
      "Train Epoch: 3 [3840/11051 (35%)]\tLoss: 0.022204\n",
      "Train Epoch: 3 [3920/11051 (35%)]\tLoss: 0.035837\n",
      "Train Epoch: 3 [4000/11051 (36%)]\tLoss: 0.037087\n",
      "Train Epoch: 3 [4080/11051 (37%)]\tLoss: 0.065184\n",
      "Train Epoch: 3 [4160/11051 (38%)]\tLoss: 0.012149\n",
      "Train Epoch: 3 [4240/11051 (38%)]\tLoss: 0.019600\n",
      "Train Epoch: 3 [4320/11051 (39%)]\tLoss: 0.013994\n",
      "Train Epoch: 3 [4400/11051 (40%)]\tLoss: 0.018300\n",
      "Train Epoch: 3 [4480/11051 (41%)]\tLoss: 0.038994\n",
      "Train Epoch: 3 [4560/11051 (41%)]\tLoss: 0.042634\n",
      "Train Epoch: 3 [4640/11051 (42%)]\tLoss: 0.032450\n",
      "Train Epoch: 3 [4720/11051 (43%)]\tLoss: 0.012885\n",
      "Train Epoch: 3 [4800/11051 (43%)]\tLoss: 0.019380\n",
      "Train Epoch: 3 [4880/11051 (44%)]\tLoss: 0.028811\n",
      "Train Epoch: 3 [4960/11051 (45%)]\tLoss: 0.024187\n",
      "Train Epoch: 3 [5040/11051 (46%)]\tLoss: 0.025099\n",
      "Train Epoch: 3 [5120/11051 (46%)]\tLoss: 0.056201\n",
      "Train Epoch: 3 [5200/11051 (47%)]\tLoss: 0.030748\n",
      "Train Epoch: 3 [5280/11051 (48%)]\tLoss: 0.020015\n",
      "Train Epoch: 3 [5360/11051 (48%)]\tLoss: 0.035472\n",
      "Train Epoch: 3 [5440/11051 (49%)]\tLoss: 0.025545\n",
      "Train Epoch: 3 [5520/11051 (50%)]\tLoss: 0.026364\n",
      "Train Epoch: 3 [5600/11051 (51%)]\tLoss: 0.018308\n",
      "Train Epoch: 3 [5680/11051 (51%)]\tLoss: 0.024684\n",
      "Train Epoch: 3 [5760/11051 (52%)]\tLoss: 0.031177\n",
      "Train Epoch: 3 [5840/11051 (53%)]\tLoss: 0.013044\n",
      "Train Epoch: 3 [5920/11051 (54%)]\tLoss: 0.047607\n",
      "Train Epoch: 3 [6000/11051 (54%)]\tLoss: 0.035801\n",
      "Train Epoch: 3 [6080/11051 (55%)]\tLoss: 0.031124\n",
      "Train Epoch: 3 [6160/11051 (56%)]\tLoss: 0.035052\n",
      "Train Epoch: 3 [6240/11051 (56%)]\tLoss: 0.026585\n",
      "Train Epoch: 3 [6320/11051 (57%)]\tLoss: 0.021354\n",
      "Train Epoch: 3 [6400/11051 (58%)]\tLoss: 0.024197\n",
      "Train Epoch: 3 [6480/11051 (59%)]\tLoss: 0.034322\n",
      "Train Epoch: 3 [6560/11051 (59%)]\tLoss: 0.004580\n",
      "Train Epoch: 3 [6640/11051 (60%)]\tLoss: 0.029784\n",
      "Train Epoch: 3 [6720/11051 (61%)]\tLoss: 0.027892\n",
      "Train Epoch: 3 [6800/11051 (62%)]\tLoss: 0.016122\n",
      "Train Epoch: 3 [6880/11051 (62%)]\tLoss: 0.041871\n",
      "Train Epoch: 3 [6960/11051 (63%)]\tLoss: 0.051017\n",
      "Train Epoch: 3 [7040/11051 (64%)]\tLoss: 0.021167\n",
      "Train Epoch: 3 [7120/11051 (64%)]\tLoss: 0.036218\n",
      "Train Epoch: 3 [7200/11051 (65%)]\tLoss: 0.039843\n",
      "Train Epoch: 3 [7280/11051 (66%)]\tLoss: 0.037379\n",
      "Train Epoch: 3 [7360/11051 (67%)]\tLoss: 0.032405\n",
      "Train Epoch: 3 [7440/11051 (67%)]\tLoss: 0.119354\n",
      "Train Epoch: 3 [7520/11051 (68%)]\tLoss: 0.015459\n",
      "Train Epoch: 3 [7600/11051 (69%)]\tLoss: 0.006748\n",
      "Train Epoch: 3 [7680/11051 (69%)]\tLoss: 0.018316\n",
      "Train Epoch: 3 [7760/11051 (70%)]\tLoss: 0.026674\n",
      "Train Epoch: 3 [7840/11051 (71%)]\tLoss: 0.010228\n",
      "Train Epoch: 3 [7920/11051 (72%)]\tLoss: 0.010897\n",
      "Train Epoch: 3 [8000/11051 (72%)]\tLoss: 0.014411\n",
      "Train Epoch: 3 [8080/11051 (73%)]\tLoss: 0.016684\n",
      "Train Epoch: 3 [8160/11051 (74%)]\tLoss: 0.027453\n",
      "Train Epoch: 3 [8240/11051 (75%)]\tLoss: 0.020428\n",
      "Train Epoch: 3 [8320/11051 (75%)]\tLoss: 0.023016\n",
      "Train Epoch: 3 [8400/11051 (76%)]\tLoss: 0.013918\n",
      "Train Epoch: 3 [8480/11051 (77%)]\tLoss: 0.020467\n",
      "Train Epoch: 3 [8560/11051 (77%)]\tLoss: 0.031500\n",
      "Train Epoch: 3 [8640/11051 (78%)]\tLoss: 0.022211\n",
      "Train Epoch: 3 [8720/11051 (79%)]\tLoss: 0.020303\n",
      "Train Epoch: 3 [8800/11051 (80%)]\tLoss: 0.008393\n",
      "Train Epoch: 3 [8880/11051 (80%)]\tLoss: 0.019025\n",
      "Train Epoch: 3 [8960/11051 (81%)]\tLoss: 0.035391\n",
      "Train Epoch: 3 [9040/11051 (82%)]\tLoss: 0.034093\n",
      "Train Epoch: 3 [9120/11051 (82%)]\tLoss: 0.009871\n",
      "Train Epoch: 3 [9200/11051 (83%)]\tLoss: 0.015692\n",
      "Train Epoch: 3 [9280/11051 (84%)]\tLoss: 0.019831\n",
      "Train Epoch: 3 [9360/11051 (85%)]\tLoss: 0.042854\n",
      "Train Epoch: 3 [9440/11051 (85%)]\tLoss: 0.047709\n",
      "Train Epoch: 3 [9520/11051 (86%)]\tLoss: 0.014021\n",
      "Train Epoch: 3 [9600/11051 (87%)]\tLoss: 0.023693\n",
      "Train Epoch: 3 [9680/11051 (88%)]\tLoss: 0.023216\n",
      "Train Epoch: 3 [9760/11051 (88%)]\tLoss: 0.041898\n",
      "Train Epoch: 3 [9840/11051 (89%)]\tLoss: 0.014523\n",
      "Train Epoch: 3 [9920/11051 (90%)]\tLoss: 0.027118\n",
      "Train Epoch: 3 [10000/11051 (90%)]\tLoss: 0.015602\n",
      "Train Epoch: 3 [10080/11051 (91%)]\tLoss: 0.015685\n",
      "Train Epoch: 3 [10160/11051 (92%)]\tLoss: 0.039410\n",
      "Train Epoch: 3 [10240/11051 (93%)]\tLoss: 0.016920\n",
      "Train Epoch: 3 [10320/11051 (93%)]\tLoss: 0.024967\n",
      "Train Epoch: 3 [10400/11051 (94%)]\tLoss: 0.015981\n",
      "Train Epoch: 3 [10480/11051 (95%)]\tLoss: 0.007414\n",
      "Train Epoch: 3 [10560/11051 (96%)]\tLoss: 0.037905\n",
      "Train Epoch: 3 [10640/11051 (96%)]\tLoss: 0.025416\n",
      "Train Epoch: 3 [10720/11051 (97%)]\tLoss: 0.036466\n",
      "Train Epoch: 3 [10800/11051 (98%)]\tLoss: 0.025330\n",
      "Train Epoch: 3 [10880/11051 (98%)]\tLoss: 0.010063\n",
      "Train Epoch: 3 [10960/11051 (99%)]\tLoss: 0.011126\n",
      "Train Epoch: 3 [11040/11051 (100%)]\tLoss: 0.012376\n",
      "====> Epoch: 3 Average loss: 0.0255\n",
      "\n",
      "Started training epoch no. 5\n",
      "Train Epoch: 4 [0/11051 (0%)]\tLoss: 0.017489\n",
      "Train Epoch: 4 [80/11051 (1%)]\tLoss: 0.033041\n",
      "Train Epoch: 4 [160/11051 (1%)]\tLoss: 0.019390\n",
      "Train Epoch: 4 [240/11051 (2%)]\tLoss: 0.030098\n",
      "Train Epoch: 4 [320/11051 (3%)]\tLoss: 0.006515\n",
      "Train Epoch: 4 [400/11051 (4%)]\tLoss: 0.033036\n",
      "Train Epoch: 4 [480/11051 (4%)]\tLoss: 0.034684\n",
      "Train Epoch: 4 [560/11051 (5%)]\tLoss: 0.013321\n",
      "Train Epoch: 4 [640/11051 (6%)]\tLoss: 0.030564\n",
      "Train Epoch: 4 [720/11051 (7%)]\tLoss: 0.029305\n",
      "Train Epoch: 4 [800/11051 (7%)]\tLoss: 0.005909\n",
      "Train Epoch: 4 [880/11051 (8%)]\tLoss: 0.022129\n",
      "Train Epoch: 4 [960/11051 (9%)]\tLoss: 0.020692\n",
      "Train Epoch: 4 [1040/11051 (9%)]\tLoss: 0.046449\n",
      "Train Epoch: 4 [1120/11051 (10%)]\tLoss: 0.031807\n",
      "Train Epoch: 4 [1200/11051 (11%)]\tLoss: 0.039490\n",
      "Train Epoch: 4 [1280/11051 (12%)]\tLoss: 0.029497\n",
      "Train Epoch: 4 [1360/11051 (12%)]\tLoss: 0.025476\n",
      "Train Epoch: 4 [1440/11051 (13%)]\tLoss: 0.038099\n",
      "Train Epoch: 4 [1520/11051 (14%)]\tLoss: 0.012371\n",
      "Train Epoch: 4 [1600/11051 (14%)]\tLoss: 0.012465\n",
      "Train Epoch: 4 [1680/11051 (15%)]\tLoss: 0.027760\n",
      "Train Epoch: 4 [1760/11051 (16%)]\tLoss: 0.027094\n",
      "Train Epoch: 4 [1840/11051 (17%)]\tLoss: 0.028230\n",
      "Train Epoch: 4 [1920/11051 (17%)]\tLoss: 0.028658\n",
      "Train Epoch: 4 [2000/11051 (18%)]\tLoss: 0.055577\n",
      "Train Epoch: 4 [2080/11051 (19%)]\tLoss: 0.025199\n",
      "Train Epoch: 4 [2160/11051 (20%)]\tLoss: 0.050665\n",
      "Train Epoch: 4 [2240/11051 (20%)]\tLoss: 0.028239\n",
      "Train Epoch: 4 [2320/11051 (21%)]\tLoss: 0.026898\n",
      "Train Epoch: 4 [2400/11051 (22%)]\tLoss: 0.025095\n",
      "Train Epoch: 4 [2480/11051 (22%)]\tLoss: 0.015987\n",
      "Train Epoch: 4 [2560/11051 (23%)]\tLoss: 0.013277\n",
      "Train Epoch: 4 [2640/11051 (24%)]\tLoss: 0.018572\n",
      "Train Epoch: 4 [2720/11051 (25%)]\tLoss: 0.016249\n",
      "Train Epoch: 4 [2800/11051 (25%)]\tLoss: 0.034774\n",
      "Train Epoch: 4 [2880/11051 (26%)]\tLoss: 0.016897\n",
      "Train Epoch: 4 [2960/11051 (27%)]\tLoss: 0.014652\n",
      "Train Epoch: 4 [3040/11051 (27%)]\tLoss: 0.049266\n",
      "Train Epoch: 4 [3120/11051 (28%)]\tLoss: 0.025012\n",
      "Train Epoch: 4 [3200/11051 (29%)]\tLoss: 0.029104\n",
      "Train Epoch: 4 [3280/11051 (30%)]\tLoss: 0.021307\n",
      "Train Epoch: 4 [3360/11051 (30%)]\tLoss: 0.048960\n",
      "Train Epoch: 4 [3440/11051 (31%)]\tLoss: 0.010271\n",
      "Train Epoch: 4 [3520/11051 (32%)]\tLoss: 0.020919\n",
      "Train Epoch: 4 [3600/11051 (33%)]\tLoss: 0.047290\n",
      "Train Epoch: 4 [3680/11051 (33%)]\tLoss: 0.035052\n",
      "Train Epoch: 4 [3760/11051 (34%)]\tLoss: 0.051290\n",
      "Train Epoch: 4 [3840/11051 (35%)]\tLoss: 0.030490\n",
      "Train Epoch: 4 [3920/11051 (35%)]\tLoss: 0.012885\n",
      "Train Epoch: 4 [4000/11051 (36%)]\tLoss: 0.030496\n",
      "Train Epoch: 4 [4080/11051 (37%)]\tLoss: 0.027553\n",
      "Train Epoch: 4 [4160/11051 (38%)]\tLoss: 0.018422\n",
      "Train Epoch: 4 [4240/11051 (38%)]\tLoss: 0.011455\n",
      "Train Epoch: 4 [4320/11051 (39%)]\tLoss: 0.035022\n",
      "Train Epoch: 4 [4400/11051 (40%)]\tLoss: 0.037196\n",
      "Train Epoch: 4 [4480/11051 (41%)]\tLoss: 0.030575\n",
      "Train Epoch: 4 [4560/11051 (41%)]\tLoss: 0.011981\n",
      "Train Epoch: 4 [4640/11051 (42%)]\tLoss: 0.014005\n",
      "Train Epoch: 4 [4720/11051 (43%)]\tLoss: 0.034122\n",
      "Train Epoch: 4 [4800/11051 (43%)]\tLoss: 0.018208\n",
      "Train Epoch: 4 [4880/11051 (44%)]\tLoss: 0.024432\n",
      "Train Epoch: 4 [4960/11051 (45%)]\tLoss: 0.027486\n",
      "Train Epoch: 4 [5040/11051 (46%)]\tLoss: 0.020496\n",
      "Train Epoch: 4 [5120/11051 (46%)]\tLoss: 0.057377\n",
      "Train Epoch: 4 [5200/11051 (47%)]\tLoss: 0.020426\n",
      "Train Epoch: 4 [5280/11051 (48%)]\tLoss: 0.024337\n",
      "Train Epoch: 4 [5360/11051 (48%)]\tLoss: 0.044722\n",
      "Train Epoch: 4 [5440/11051 (49%)]\tLoss: 0.021879\n",
      "Train Epoch: 4 [5520/11051 (50%)]\tLoss: 0.024783\n",
      "Train Epoch: 4 [5600/11051 (51%)]\tLoss: 0.039090\n",
      "Train Epoch: 4 [5680/11051 (51%)]\tLoss: 0.008753\n",
      "Train Epoch: 4 [5760/11051 (52%)]\tLoss: 0.010320\n",
      "Train Epoch: 4 [5840/11051 (53%)]\tLoss: 0.028435\n",
      "Train Epoch: 4 [5920/11051 (54%)]\tLoss: 0.031010\n",
      "Train Epoch: 4 [6000/11051 (54%)]\tLoss: 0.015971\n",
      "Train Epoch: 4 [6080/11051 (55%)]\tLoss: 0.014490\n",
      "Train Epoch: 4 [6160/11051 (56%)]\tLoss: 0.018692\n",
      "Train Epoch: 4 [6240/11051 (56%)]\tLoss: 0.025277\n",
      "Train Epoch: 4 [6320/11051 (57%)]\tLoss: 0.036636\n",
      "Train Epoch: 4 [6400/11051 (58%)]\tLoss: 0.028865\n",
      "Train Epoch: 4 [6480/11051 (59%)]\tLoss: 0.019685\n",
      "Train Epoch: 4 [6560/11051 (59%)]\tLoss: 0.031104\n",
      "Train Epoch: 4 [6640/11051 (60%)]\tLoss: 0.031946\n",
      "Train Epoch: 4 [6720/11051 (61%)]\tLoss: 0.016741\n",
      "Train Epoch: 4 [6800/11051 (62%)]\tLoss: 0.021843\n",
      "Train Epoch: 4 [6880/11051 (62%)]\tLoss: 0.027649\n",
      "Train Epoch: 4 [6960/11051 (63%)]\tLoss: 0.036505\n",
      "Train Epoch: 4 [7040/11051 (64%)]\tLoss: 0.016536\n",
      "Train Epoch: 4 [7120/11051 (64%)]\tLoss: 0.028828\n",
      "Train Epoch: 4 [7200/11051 (65%)]\tLoss: 0.050874\n",
      "Train Epoch: 4 [7280/11051 (66%)]\tLoss: 0.004763\n",
      "Train Epoch: 4 [7360/11051 (67%)]\tLoss: 0.046716\n",
      "Train Epoch: 4 [7440/11051 (67%)]\tLoss: 0.036306\n",
      "Train Epoch: 4 [7520/11051 (68%)]\tLoss: 0.041521\n",
      "Train Epoch: 4 [7600/11051 (69%)]\tLoss: 0.037406\n",
      "Train Epoch: 4 [7680/11051 (69%)]\tLoss: 0.030297\n",
      "Train Epoch: 4 [7760/11051 (70%)]\tLoss: 0.021330\n",
      "Train Epoch: 4 [7840/11051 (71%)]\tLoss: 0.042072\n",
      "Train Epoch: 4 [7920/11051 (72%)]\tLoss: 0.013849\n",
      "Train Epoch: 4 [8000/11051 (72%)]\tLoss: 0.033634\n",
      "Train Epoch: 4 [8080/11051 (73%)]\tLoss: 0.036915\n",
      "Train Epoch: 4 [8160/11051 (74%)]\tLoss: 0.022112\n",
      "Train Epoch: 4 [8240/11051 (75%)]\tLoss: 0.023312\n",
      "Train Epoch: 4 [8320/11051 (75%)]\tLoss: 0.015654\n",
      "Train Epoch: 4 [8400/11051 (76%)]\tLoss: 0.025635\n",
      "Train Epoch: 4 [8480/11051 (77%)]\tLoss: 0.032959\n",
      "Train Epoch: 4 [8560/11051 (77%)]\tLoss: 0.038403\n",
      "Train Epoch: 4 [8640/11051 (78%)]\tLoss: 0.018222\n",
      "Train Epoch: 4 [8720/11051 (79%)]\tLoss: 0.021117\n",
      "Train Epoch: 4 [8800/11051 (80%)]\tLoss: 0.032583\n",
      "Train Epoch: 4 [8880/11051 (80%)]\tLoss: 0.040349\n",
      "Train Epoch: 4 [8960/11051 (81%)]\tLoss: 0.014147\n",
      "Train Epoch: 4 [9040/11051 (82%)]\tLoss: 0.023971\n",
      "Train Epoch: 4 [9120/11051 (82%)]\tLoss: 0.029958\n",
      "Train Epoch: 4 [9200/11051 (83%)]\tLoss: 0.022204\n",
      "Train Epoch: 4 [9280/11051 (84%)]\tLoss: 0.051645\n",
      "Train Epoch: 4 [9360/11051 (85%)]\tLoss: 0.023164\n",
      "Train Epoch: 4 [9440/11051 (85%)]\tLoss: 0.057738\n",
      "Train Epoch: 4 [9520/11051 (86%)]\tLoss: 0.020567\n",
      "Train Epoch: 4 [9600/11051 (87%)]\tLoss: 0.007838\n",
      "Train Epoch: 4 [9680/11051 (88%)]\tLoss: 0.034916\n",
      "Train Epoch: 4 [9760/11051 (88%)]\tLoss: 0.008646\n",
      "Train Epoch: 4 [9840/11051 (89%)]\tLoss: 0.019049\n",
      "Train Epoch: 4 [9920/11051 (90%)]\tLoss: 0.018380\n",
      "Train Epoch: 4 [10000/11051 (90%)]\tLoss: 0.015893\n",
      "Train Epoch: 4 [10080/11051 (91%)]\tLoss: 0.027893\n",
      "Train Epoch: 4 [10160/11051 (92%)]\tLoss: 0.027621\n",
      "Train Epoch: 4 [10240/11051 (93%)]\tLoss: 0.010011\n",
      "Train Epoch: 4 [10320/11051 (93%)]\tLoss: 0.017068\n",
      "Train Epoch: 4 [10400/11051 (94%)]\tLoss: 0.018595\n",
      "Train Epoch: 4 [10480/11051 (95%)]\tLoss: 0.043621\n",
      "Train Epoch: 4 [10560/11051 (96%)]\tLoss: 0.039798\n",
      "Train Epoch: 4 [10640/11051 (96%)]\tLoss: 0.019750\n",
      "Train Epoch: 4 [10720/11051 (97%)]\tLoss: 0.021550\n",
      "Train Epoch: 4 [10800/11051 (98%)]\tLoss: 0.018605\n",
      "Train Epoch: 4 [10880/11051 (98%)]\tLoss: 0.012425\n",
      "Train Epoch: 4 [10960/11051 (99%)]\tLoss: 0.030365\n",
      "Train Epoch: 4 [11040/11051 (100%)]\tLoss: 0.054118\n",
      "====> Epoch: 4 Average loss: 0.0246\n",
      "\n",
      "Started training epoch no. 6\n",
      "Train Epoch: 5 [0/11051 (0%)]\tLoss: 0.016447\n",
      "Train Epoch: 5 [80/11051 (1%)]\tLoss: 0.022771\n",
      "Train Epoch: 5 [160/11051 (1%)]\tLoss: 0.022548\n",
      "Train Epoch: 5 [240/11051 (2%)]\tLoss: 0.022693\n",
      "Train Epoch: 5 [320/11051 (3%)]\tLoss: 0.011330\n",
      "Train Epoch: 5 [400/11051 (4%)]\tLoss: 0.029469\n",
      "Train Epoch: 5 [480/11051 (4%)]\tLoss: 0.022059\n",
      "Train Epoch: 5 [560/11051 (5%)]\tLoss: 0.031499\n",
      "Train Epoch: 5 [640/11051 (6%)]\tLoss: 0.015675\n",
      "Train Epoch: 5 [720/11051 (7%)]\tLoss: 0.034000\n",
      "Train Epoch: 5 [800/11051 (7%)]\tLoss: 0.030282\n",
      "Train Epoch: 5 [880/11051 (8%)]\tLoss: 0.015938\n",
      "Train Epoch: 5 [960/11051 (9%)]\tLoss: 0.012556\n",
      "Train Epoch: 5 [1040/11051 (9%)]\tLoss: 0.015908\n",
      "Train Epoch: 5 [1120/11051 (10%)]\tLoss: 0.021425\n",
      "Train Epoch: 5 [1200/11051 (11%)]\tLoss: 0.022394\n",
      "Train Epoch: 5 [1280/11051 (12%)]\tLoss: 0.021114\n",
      "Train Epoch: 5 [1360/11051 (12%)]\tLoss: 0.021726\n",
      "Train Epoch: 5 [1440/11051 (13%)]\tLoss: 0.022113\n",
      "Train Epoch: 5 [1520/11051 (14%)]\tLoss: 0.031913\n",
      "Train Epoch: 5 [1600/11051 (14%)]\tLoss: 0.029332\n",
      "Train Epoch: 5 [1680/11051 (15%)]\tLoss: 0.014891\n",
      "Train Epoch: 5 [1760/11051 (16%)]\tLoss: 0.030609\n",
      "Train Epoch: 5 [1840/11051 (17%)]\tLoss: 0.021398\n",
      "Train Epoch: 5 [1920/11051 (17%)]\tLoss: 0.019780\n",
      "Train Epoch: 5 [2000/11051 (18%)]\tLoss: 0.010830\n",
      "Train Epoch: 5 [2080/11051 (19%)]\tLoss: 0.033406\n",
      "Train Epoch: 5 [2160/11051 (20%)]\tLoss: 0.018940\n",
      "Train Epoch: 5 [2240/11051 (20%)]\tLoss: 0.036446\n",
      "Train Epoch: 5 [2320/11051 (21%)]\tLoss: 0.018944\n",
      "Train Epoch: 5 [2400/11051 (22%)]\tLoss: 0.012432\n",
      "Train Epoch: 5 [2480/11051 (22%)]\tLoss: 0.039828\n",
      "Train Epoch: 5 [2560/11051 (23%)]\tLoss: 0.014397\n",
      "Train Epoch: 5 [2640/11051 (24%)]\tLoss: 0.015338\n",
      "Train Epoch: 5 [2720/11051 (25%)]\tLoss: 0.009389\n",
      "Train Epoch: 5 [2800/11051 (25%)]\tLoss: 0.029903\n",
      "Train Epoch: 5 [2880/11051 (26%)]\tLoss: 0.013820\n",
      "Train Epoch: 5 [2960/11051 (27%)]\tLoss: 0.037805\n",
      "Train Epoch: 5 [3040/11051 (27%)]\tLoss: 0.017169\n",
      "Train Epoch: 5 [3120/11051 (28%)]\tLoss: 0.026322\n",
      "Train Epoch: 5 [3200/11051 (29%)]\tLoss: 0.025978\n",
      "Train Epoch: 5 [3280/11051 (30%)]\tLoss: 0.024473\n",
      "Train Epoch: 5 [3360/11051 (30%)]\tLoss: 0.010358\n",
      "Train Epoch: 5 [3440/11051 (31%)]\tLoss: 0.032495\n",
      "Train Epoch: 5 [3520/11051 (32%)]\tLoss: 0.018795\n",
      "Train Epoch: 5 [3600/11051 (33%)]\tLoss: 0.027430\n",
      "Train Epoch: 5 [3680/11051 (33%)]\tLoss: 0.052667\n",
      "Train Epoch: 5 [3760/11051 (34%)]\tLoss: 0.020131\n",
      "Train Epoch: 5 [3840/11051 (35%)]\tLoss: 0.042685\n",
      "Train Epoch: 5 [3920/11051 (35%)]\tLoss: 0.026440\n",
      "Train Epoch: 5 [4000/11051 (36%)]\tLoss: 0.027317\n",
      "Train Epoch: 5 [4080/11051 (37%)]\tLoss: 0.026910\n",
      "Train Epoch: 5 [4160/11051 (38%)]\tLoss: 0.014898\n",
      "Train Epoch: 5 [4240/11051 (38%)]\tLoss: 0.034869\n",
      "Train Epoch: 5 [4320/11051 (39%)]\tLoss: 0.013259\n",
      "Train Epoch: 5 [4400/11051 (40%)]\tLoss: 0.018734\n",
      "Train Epoch: 5 [4480/11051 (41%)]\tLoss: 0.018504\n",
      "Train Epoch: 5 [4560/11051 (41%)]\tLoss: 0.014368\n",
      "Train Epoch: 5 [4640/11051 (42%)]\tLoss: 0.029938\n",
      "Train Epoch: 5 [4720/11051 (43%)]\tLoss: 0.022957\n",
      "Train Epoch: 5 [4800/11051 (43%)]\tLoss: 0.013828\n",
      "Train Epoch: 5 [4880/11051 (44%)]\tLoss: 0.011393\n",
      "Train Epoch: 5 [4960/11051 (45%)]\tLoss: 0.016262\n",
      "Train Epoch: 5 [5040/11051 (46%)]\tLoss: 0.052954\n",
      "Train Epoch: 5 [5120/11051 (46%)]\tLoss: 0.014122\n",
      "Train Epoch: 5 [5200/11051 (47%)]\tLoss: 0.014645\n",
      "Train Epoch: 5 [5280/11051 (48%)]\tLoss: 0.010664\n",
      "Train Epoch: 5 [5360/11051 (48%)]\tLoss: 0.020571\n",
      "Train Epoch: 5 [5440/11051 (49%)]\tLoss: 0.043197\n",
      "Train Epoch: 5 [5520/11051 (50%)]\tLoss: 0.028413\n",
      "Train Epoch: 5 [5600/11051 (51%)]\tLoss: 0.025710\n",
      "Train Epoch: 5 [5680/11051 (51%)]\tLoss: 0.019671\n",
      "Train Epoch: 5 [5760/11051 (52%)]\tLoss: 0.022918\n",
      "Train Epoch: 5 [5840/11051 (53%)]\tLoss: 0.023078\n",
      "Train Epoch: 5 [5920/11051 (54%)]\tLoss: 0.025872\n",
      "Train Epoch: 5 [6000/11051 (54%)]\tLoss: 0.051847\n",
      "Train Epoch: 5 [6080/11051 (55%)]\tLoss: 0.017890\n",
      "Train Epoch: 5 [6160/11051 (56%)]\tLoss: 0.029667\n",
      "Train Epoch: 5 [6240/11051 (56%)]\tLoss: 0.016462\n",
      "Train Epoch: 5 [6320/11051 (57%)]\tLoss: 0.035002\n",
      "Train Epoch: 5 [6400/11051 (58%)]\tLoss: 0.010701\n",
      "Train Epoch: 5 [6480/11051 (59%)]\tLoss: 0.031927\n",
      "Train Epoch: 5 [6560/11051 (59%)]\tLoss: 0.024633\n",
      "Train Epoch: 5 [6640/11051 (60%)]\tLoss: 0.021165\n",
      "Train Epoch: 5 [6720/11051 (61%)]\tLoss: 0.030081\n",
      "Train Epoch: 5 [6800/11051 (62%)]\tLoss: 0.022036\n",
      "Train Epoch: 5 [6880/11051 (62%)]\tLoss: 0.042276\n",
      "Train Epoch: 5 [6960/11051 (63%)]\tLoss: 0.035442\n",
      "Train Epoch: 5 [7040/11051 (64%)]\tLoss: 0.016705\n",
      "Train Epoch: 5 [7120/11051 (64%)]\tLoss: 0.014958\n",
      "Train Epoch: 5 [7200/11051 (65%)]\tLoss: 0.025512\n",
      "Train Epoch: 5 [7280/11051 (66%)]\tLoss: 0.023235\n",
      "Train Epoch: 5 [7360/11051 (67%)]\tLoss: 0.055573\n",
      "Train Epoch: 5 [7440/11051 (67%)]\tLoss: 0.025103\n",
      "Train Epoch: 5 [7520/11051 (68%)]\tLoss: 0.006574\n",
      "Train Epoch: 5 [7600/11051 (69%)]\tLoss: 0.025929\n",
      "Train Epoch: 5 [7680/11051 (69%)]\tLoss: 0.029122\n",
      "Train Epoch: 5 [7760/11051 (70%)]\tLoss: 0.016136\n",
      "Train Epoch: 5 [7840/11051 (71%)]\tLoss: 0.016670\n",
      "Train Epoch: 5 [7920/11051 (72%)]\tLoss: 0.011235\n",
      "Train Epoch: 5 [8000/11051 (72%)]\tLoss: 0.011975\n",
      "Train Epoch: 5 [8080/11051 (73%)]\tLoss: 0.016396\n",
      "Train Epoch: 5 [8160/11051 (74%)]\tLoss: 0.056013\n",
      "Train Epoch: 5 [8240/11051 (75%)]\tLoss: 0.036451\n",
      "Train Epoch: 5 [8320/11051 (75%)]\tLoss: 0.012559\n",
      "Train Epoch: 5 [8400/11051 (76%)]\tLoss: 0.024857\n",
      "Train Epoch: 5 [8480/11051 (77%)]\tLoss: 0.017568\n",
      "Train Epoch: 5 [8560/11051 (77%)]\tLoss: 0.012611\n",
      "Train Epoch: 5 [8640/11051 (78%)]\tLoss: 0.023792\n",
      "Train Epoch: 5 [8720/11051 (79%)]\tLoss: 0.022671\n",
      "Train Epoch: 5 [8800/11051 (80%)]\tLoss: 0.029501\n",
      "Train Epoch: 5 [8880/11051 (80%)]\tLoss: 0.018651\n",
      "Train Epoch: 5 [8960/11051 (81%)]\tLoss: 0.028647\n",
      "Train Epoch: 5 [9040/11051 (82%)]\tLoss: 0.028883\n",
      "Train Epoch: 5 [9120/11051 (82%)]\tLoss: 0.023417\n",
      "Train Epoch: 5 [9200/11051 (83%)]\tLoss: 0.029874\n",
      "Train Epoch: 5 [9280/11051 (84%)]\tLoss: 0.018005\n",
      "Train Epoch: 5 [9360/11051 (85%)]\tLoss: 0.018549\n",
      "Train Epoch: 5 [9440/11051 (85%)]\tLoss: 0.026917\n",
      "Train Epoch: 5 [9520/11051 (86%)]\tLoss: 0.040201\n",
      "Train Epoch: 5 [9600/11051 (87%)]\tLoss: 0.012222\n",
      "Train Epoch: 5 [9680/11051 (88%)]\tLoss: 0.027773\n",
      "Train Epoch: 5 [9760/11051 (88%)]\tLoss: 0.026441\n",
      "Train Epoch: 5 [9840/11051 (89%)]\tLoss: 0.023834\n",
      "Train Epoch: 5 [9920/11051 (90%)]\tLoss: 0.030830\n",
      "Train Epoch: 5 [10000/11051 (90%)]\tLoss: 0.009091\n",
      "Train Epoch: 5 [10080/11051 (91%)]\tLoss: 0.021376\n",
      "Train Epoch: 5 [10160/11051 (92%)]\tLoss: 0.009504\n",
      "Train Epoch: 5 [10240/11051 (93%)]\tLoss: 0.026852\n",
      "Train Epoch: 5 [10320/11051 (93%)]\tLoss: 0.040818\n",
      "Train Epoch: 5 [10400/11051 (94%)]\tLoss: 0.008126\n",
      "Train Epoch: 5 [10480/11051 (95%)]\tLoss: 0.015273\n",
      "Train Epoch: 5 [10560/11051 (96%)]\tLoss: 0.016728\n",
      "Train Epoch: 5 [10640/11051 (96%)]\tLoss: 0.029011\n",
      "Train Epoch: 5 [10720/11051 (97%)]\tLoss: 0.026925\n",
      "Train Epoch: 5 [10800/11051 (98%)]\tLoss: 0.013671\n",
      "Train Epoch: 5 [10880/11051 (98%)]\tLoss: 0.020538\n",
      "Train Epoch: 5 [10960/11051 (99%)]\tLoss: 0.049452\n",
      "Train Epoch: 5 [11040/11051 (100%)]\tLoss: 0.029916\n",
      "====> Epoch: 5 Average loss: 0.0241\n",
      "\n",
      "Started training epoch no. 7\n",
      "Train Epoch: 6 [0/11051 (0%)]\tLoss: 0.020618\n",
      "Train Epoch: 6 [80/11051 (1%)]\tLoss: 0.014512\n",
      "Train Epoch: 6 [160/11051 (1%)]\tLoss: 0.018135\n",
      "Train Epoch: 6 [240/11051 (2%)]\tLoss: 0.015842\n",
      "Train Epoch: 6 [320/11051 (3%)]\tLoss: 0.024951\n",
      "Train Epoch: 6 [400/11051 (4%)]\tLoss: 0.027830\n",
      "Train Epoch: 6 [480/11051 (4%)]\tLoss: 0.020596\n",
      "Train Epoch: 6 [560/11051 (5%)]\tLoss: 0.017095\n",
      "Train Epoch: 6 [640/11051 (6%)]\tLoss: 0.033333\n",
      "Train Epoch: 6 [720/11051 (7%)]\tLoss: 0.006398\n",
      "Train Epoch: 6 [800/11051 (7%)]\tLoss: 0.011335\n",
      "Train Epoch: 6 [880/11051 (8%)]\tLoss: 0.049570\n",
      "Train Epoch: 6 [960/11051 (9%)]\tLoss: 0.046160\n",
      "Train Epoch: 6 [1040/11051 (9%)]\tLoss: 0.038399\n",
      "Train Epoch: 6 [1120/11051 (10%)]\tLoss: 0.017494\n",
      "Train Epoch: 6 [1200/11051 (11%)]\tLoss: 0.011371\n",
      "Train Epoch: 6 [1280/11051 (12%)]\tLoss: 0.027222\n",
      "Train Epoch: 6 [1360/11051 (12%)]\tLoss: 0.026429\n",
      "Train Epoch: 6 [1440/11051 (13%)]\tLoss: 0.017341\n",
      "Train Epoch: 6 [1520/11051 (14%)]\tLoss: 0.015586\n",
      "Train Epoch: 6 [1600/11051 (14%)]\tLoss: 0.029855\n",
      "Train Epoch: 6 [1680/11051 (15%)]\tLoss: 0.025725\n",
      "Train Epoch: 6 [1760/11051 (16%)]\tLoss: 0.013111\n",
      "Train Epoch: 6 [1840/11051 (17%)]\tLoss: 0.014980\n",
      "Train Epoch: 6 [1920/11051 (17%)]\tLoss: 0.027857\n",
      "Train Epoch: 6 [2000/11051 (18%)]\tLoss: 0.024191\n",
      "Train Epoch: 6 [2080/11051 (19%)]\tLoss: 0.027844\n",
      "Train Epoch: 6 [2160/11051 (20%)]\tLoss: 0.023327\n",
      "Train Epoch: 6 [2240/11051 (20%)]\tLoss: 0.027906\n",
      "Train Epoch: 6 [2320/11051 (21%)]\tLoss: 0.033470\n",
      "Train Epoch: 6 [2400/11051 (22%)]\tLoss: 0.047708\n",
      "Train Epoch: 6 [2480/11051 (22%)]\tLoss: 0.011608\n",
      "Train Epoch: 6 [2560/11051 (23%)]\tLoss: 0.019631\n",
      "Train Epoch: 6 [2640/11051 (24%)]\tLoss: 0.015176\n",
      "Train Epoch: 6 [2720/11051 (25%)]\tLoss: 0.018365\n",
      "Train Epoch: 6 [2800/11051 (25%)]\tLoss: 0.029221\n",
      "Train Epoch: 6 [2880/11051 (26%)]\tLoss: 0.019207\n",
      "Train Epoch: 6 [2960/11051 (27%)]\tLoss: 0.018543\n",
      "Train Epoch: 6 [3040/11051 (27%)]\tLoss: 0.029817\n",
      "Train Epoch: 6 [3120/11051 (28%)]\tLoss: 0.043691\n",
      "Train Epoch: 6 [3200/11051 (29%)]\tLoss: 0.017405\n",
      "Train Epoch: 6 [3280/11051 (30%)]\tLoss: 0.009881\n",
      "Train Epoch: 6 [3360/11051 (30%)]\tLoss: 0.007140\n",
      "Train Epoch: 6 [3440/11051 (31%)]\tLoss: 0.022035\n",
      "Train Epoch: 6 [3520/11051 (32%)]\tLoss: 0.019230\n",
      "Train Epoch: 6 [3600/11051 (33%)]\tLoss: 0.002573\n",
      "Train Epoch: 6 [3680/11051 (33%)]\tLoss: 0.013408\n",
      "Train Epoch: 6 [3760/11051 (34%)]\tLoss: 0.031014\n",
      "Train Epoch: 6 [3840/11051 (35%)]\tLoss: 0.010082\n",
      "Train Epoch: 6 [3920/11051 (35%)]\tLoss: 0.024243\n",
      "Train Epoch: 6 [4000/11051 (36%)]\tLoss: 0.023535\n",
      "Train Epoch: 6 [4080/11051 (37%)]\tLoss: 0.017806\n",
      "Train Epoch: 6 [4160/11051 (38%)]\tLoss: 0.009377\n",
      "Train Epoch: 6 [4240/11051 (38%)]\tLoss: 0.021466\n",
      "Train Epoch: 6 [4320/11051 (39%)]\tLoss: 0.034838\n",
      "Train Epoch: 6 [4400/11051 (40%)]\tLoss: 0.014778\n",
      "Train Epoch: 6 [4480/11051 (41%)]\tLoss: 0.026552\n",
      "Train Epoch: 6 [4560/11051 (41%)]\tLoss: 0.026058\n",
      "Train Epoch: 6 [4640/11051 (42%)]\tLoss: 0.013128\n",
      "Train Epoch: 6 [4720/11051 (43%)]\tLoss: 0.016771\n",
      "Train Epoch: 6 [4800/11051 (43%)]\tLoss: 0.011349\n",
      "Train Epoch: 6 [4880/11051 (44%)]\tLoss: 0.012541\n",
      "Train Epoch: 6 [4960/11051 (45%)]\tLoss: 0.055690\n",
      "Train Epoch: 6 [5040/11051 (46%)]\tLoss: 0.029086\n",
      "Train Epoch: 6 [5120/11051 (46%)]\tLoss: 0.017048\n",
      "Train Epoch: 6 [5200/11051 (47%)]\tLoss: 0.013169\n",
      "Train Epoch: 6 [5280/11051 (48%)]\tLoss: 0.011182\n",
      "Train Epoch: 6 [5360/11051 (48%)]\tLoss: 0.017452\n",
      "Train Epoch: 6 [5440/11051 (49%)]\tLoss: 0.026286\n",
      "Train Epoch: 6 [5520/11051 (50%)]\tLoss: 0.027493\n",
      "Train Epoch: 6 [5600/11051 (51%)]\tLoss: 0.030474\n",
      "Train Epoch: 6 [5680/11051 (51%)]\tLoss: 0.028686\n",
      "Train Epoch: 6 [5760/11051 (52%)]\tLoss: 0.023300\n",
      "Train Epoch: 6 [5840/11051 (53%)]\tLoss: 0.013533\n",
      "Train Epoch: 6 [5920/11051 (54%)]\tLoss: 0.034454\n",
      "Train Epoch: 6 [6000/11051 (54%)]\tLoss: 0.011757\n",
      "Train Epoch: 6 [6080/11051 (55%)]\tLoss: 0.013894\n",
      "Train Epoch: 6 [6160/11051 (56%)]\tLoss: 0.018991\n",
      "Train Epoch: 6 [6240/11051 (56%)]\tLoss: 0.024197\n",
      "Train Epoch: 6 [6320/11051 (57%)]\tLoss: 0.013871\n",
      "Train Epoch: 6 [6400/11051 (58%)]\tLoss: 0.034773\n",
      "Train Epoch: 6 [6480/11051 (59%)]\tLoss: 0.018781\n",
      "Train Epoch: 6 [6560/11051 (59%)]\tLoss: 0.023540\n",
      "Train Epoch: 6 [6640/11051 (60%)]\tLoss: 0.030988\n",
      "Train Epoch: 6 [6720/11051 (61%)]\tLoss: 0.029215\n",
      "Train Epoch: 6 [6800/11051 (62%)]\tLoss: 0.010241\n",
      "Train Epoch: 6 [6880/11051 (62%)]\tLoss: 0.023829\n",
      "Train Epoch: 6 [6960/11051 (63%)]\tLoss: 0.026071\n",
      "Train Epoch: 6 [7040/11051 (64%)]\tLoss: 0.058572\n",
      "Train Epoch: 6 [7120/11051 (64%)]\tLoss: 0.031960\n",
      "Train Epoch: 6 [7200/11051 (65%)]\tLoss: 0.013674\n",
      "Train Epoch: 6 [7280/11051 (66%)]\tLoss: 0.031023\n",
      "Train Epoch: 6 [7360/11051 (67%)]\tLoss: 0.041084\n",
      "Train Epoch: 6 [7440/11051 (67%)]\tLoss: 0.013230\n",
      "Train Epoch: 6 [7520/11051 (68%)]\tLoss: 0.010208\n",
      "Train Epoch: 6 [7600/11051 (69%)]\tLoss: 0.035122\n",
      "Train Epoch: 6 [7680/11051 (69%)]\tLoss: 0.035506\n",
      "Train Epoch: 6 [7760/11051 (70%)]\tLoss: 0.022327\n",
      "Train Epoch: 6 [7840/11051 (71%)]\tLoss: 0.012300\n",
      "Train Epoch: 6 [7920/11051 (72%)]\tLoss: 0.022350\n",
      "Train Epoch: 6 [8000/11051 (72%)]\tLoss: 0.015657\n",
      "Train Epoch: 6 [8080/11051 (73%)]\tLoss: 0.017909\n",
      "Train Epoch: 6 [8160/11051 (74%)]\tLoss: 0.022220\n",
      "Train Epoch: 6 [8240/11051 (75%)]\tLoss: 0.027516\n",
      "Train Epoch: 6 [8320/11051 (75%)]\tLoss: 0.021450\n",
      "Train Epoch: 6 [8400/11051 (76%)]\tLoss: 0.018610\n",
      "Train Epoch: 6 [8480/11051 (77%)]\tLoss: 0.033893\n",
      "Train Epoch: 6 [8560/11051 (77%)]\tLoss: 0.033420\n",
      "Train Epoch: 6 [8640/11051 (78%)]\tLoss: 0.036361\n",
      "Train Epoch: 6 [8720/11051 (79%)]\tLoss: 0.013991\n",
      "Train Epoch: 6 [8800/11051 (80%)]\tLoss: 0.023946\n",
      "Train Epoch: 6 [8880/11051 (80%)]\tLoss: 0.034989\n",
      "Train Epoch: 6 [8960/11051 (81%)]\tLoss: 0.038864\n",
      "Train Epoch: 6 [9040/11051 (82%)]\tLoss: 0.013814\n",
      "Train Epoch: 6 [9120/11051 (82%)]\tLoss: 0.023414\n",
      "Train Epoch: 6 [9200/11051 (83%)]\tLoss: 0.017441\n",
      "Train Epoch: 6 [9280/11051 (84%)]\tLoss: 0.031535\n",
      "Train Epoch: 6 [9360/11051 (85%)]\tLoss: 0.016595\n",
      "Train Epoch: 6 [9440/11051 (85%)]\tLoss: 0.009035\n",
      "Train Epoch: 6 [9520/11051 (86%)]\tLoss: 0.022580\n",
      "Train Epoch: 6 [9600/11051 (87%)]\tLoss: 0.018264\n",
      "Train Epoch: 6 [9680/11051 (88%)]\tLoss: 0.025579\n",
      "Train Epoch: 6 [9760/11051 (88%)]\tLoss: 0.028857\n",
      "Train Epoch: 6 [9840/11051 (89%)]\tLoss: 0.020467\n",
      "Train Epoch: 6 [9920/11051 (90%)]\tLoss: 0.033877\n",
      "Train Epoch: 6 [10000/11051 (90%)]\tLoss: 0.022631\n",
      "Train Epoch: 6 [10080/11051 (91%)]\tLoss: 0.048680\n",
      "Train Epoch: 6 [10160/11051 (92%)]\tLoss: 0.014958\n",
      "Train Epoch: 6 [10240/11051 (93%)]\tLoss: 0.010589\n",
      "Train Epoch: 6 [10320/11051 (93%)]\tLoss: 0.041345\n",
      "Train Epoch: 6 [10400/11051 (94%)]\tLoss: 0.013797\n",
      "Train Epoch: 6 [10480/11051 (95%)]\tLoss: 0.025656\n",
      "Train Epoch: 6 [10560/11051 (96%)]\tLoss: 0.025049\n",
      "Train Epoch: 6 [10640/11051 (96%)]\tLoss: 0.031482\n",
      "Train Epoch: 6 [10720/11051 (97%)]\tLoss: 0.030922\n",
      "Train Epoch: 6 [10800/11051 (98%)]\tLoss: 0.019177\n",
      "Train Epoch: 6 [10880/11051 (98%)]\tLoss: 0.030873\n",
      "Train Epoch: 6 [10960/11051 (99%)]\tLoss: 0.037400\n",
      "Train Epoch: 6 [11040/11051 (100%)]\tLoss: 0.007475\n",
      "====> Epoch: 6 Average loss: 0.0238\n",
      "\n",
      "Started training epoch no. 8\n",
      "Train Epoch: 7 [0/11051 (0%)]\tLoss: 0.023989\n",
      "Train Epoch: 7 [80/11051 (1%)]\tLoss: 0.022608\n",
      "Train Epoch: 7 [160/11051 (1%)]\tLoss: 0.025711\n",
      "Train Epoch: 7 [240/11051 (2%)]\tLoss: 0.016877\n",
      "Train Epoch: 7 [320/11051 (3%)]\tLoss: 0.042014\n",
      "Train Epoch: 7 [400/11051 (4%)]\tLoss: 0.023485\n",
      "Train Epoch: 7 [480/11051 (4%)]\tLoss: 0.037575\n",
      "Train Epoch: 7 [560/11051 (5%)]\tLoss: 0.040051\n",
      "Train Epoch: 7 [640/11051 (6%)]\tLoss: 0.018463\n",
      "Train Epoch: 7 [720/11051 (7%)]\tLoss: 0.052099\n",
      "Train Epoch: 7 [800/11051 (7%)]\tLoss: 0.018897\n",
      "Train Epoch: 7 [880/11051 (8%)]\tLoss: 0.021078\n",
      "Train Epoch: 7 [960/11051 (9%)]\tLoss: 0.027463\n",
      "Train Epoch: 7 [1040/11051 (9%)]\tLoss: 0.013796\n",
      "Train Epoch: 7 [1120/11051 (10%)]\tLoss: 0.006819\n",
      "Train Epoch: 7 [1200/11051 (11%)]\tLoss: 0.022121\n",
      "Train Epoch: 7 [1280/11051 (12%)]\tLoss: 0.015255\n",
      "Train Epoch: 7 [1360/11051 (12%)]\tLoss: 0.010431\n",
      "Train Epoch: 7 [1440/11051 (13%)]\tLoss: 0.026841\n",
      "Train Epoch: 7 [1520/11051 (14%)]\tLoss: 0.030162\n",
      "Train Epoch: 7 [1600/11051 (14%)]\tLoss: 0.017468\n",
      "Train Epoch: 7 [1680/11051 (15%)]\tLoss: 0.024448\n",
      "Train Epoch: 7 [1760/11051 (16%)]\tLoss: 0.012634\n",
      "Train Epoch: 7 [1840/11051 (17%)]\tLoss: 0.023757\n",
      "Train Epoch: 7 [1920/11051 (17%)]\tLoss: 0.036407\n",
      "Train Epoch: 7 [2000/11051 (18%)]\tLoss: 0.030010\n",
      "Train Epoch: 7 [2080/11051 (19%)]\tLoss: 0.039198\n",
      "Train Epoch: 7 [2160/11051 (20%)]\tLoss: 0.038394\n",
      "Train Epoch: 7 [2240/11051 (20%)]\tLoss: 0.023869\n",
      "Train Epoch: 7 [2320/11051 (21%)]\tLoss: 0.022000\n",
      "Train Epoch: 7 [2400/11051 (22%)]\tLoss: 0.033739\n",
      "Train Epoch: 7 [2480/11051 (22%)]\tLoss: 0.032709\n",
      "Train Epoch: 7 [2560/11051 (23%)]\tLoss: 0.009788\n",
      "Train Epoch: 7 [2640/11051 (24%)]\tLoss: 0.037844\n",
      "Train Epoch: 7 [2720/11051 (25%)]\tLoss: 0.014956\n",
      "Train Epoch: 7 [2800/11051 (25%)]\tLoss: 0.036077\n",
      "Train Epoch: 7 [2880/11051 (26%)]\tLoss: 0.024017\n",
      "Train Epoch: 7 [2960/11051 (27%)]\tLoss: 0.022833\n",
      "Train Epoch: 7 [3040/11051 (27%)]\tLoss: 0.019525\n",
      "Train Epoch: 7 [3120/11051 (28%)]\tLoss: 0.021164\n",
      "Train Epoch: 7 [3200/11051 (29%)]\tLoss: 0.021246\n",
      "Train Epoch: 7 [3280/11051 (30%)]\tLoss: 0.026138\n",
      "Train Epoch: 7 [3360/11051 (30%)]\tLoss: 0.020251\n",
      "Train Epoch: 7 [3440/11051 (31%)]\tLoss: 0.025898\n",
      "Train Epoch: 7 [3520/11051 (32%)]\tLoss: 0.033284\n",
      "Train Epoch: 7 [3600/11051 (33%)]\tLoss: 0.043436\n",
      "Train Epoch: 7 [3680/11051 (33%)]\tLoss: 0.022719\n",
      "Train Epoch: 7 [3760/11051 (34%)]\tLoss: 0.020740\n",
      "Train Epoch: 7 [3840/11051 (35%)]\tLoss: 0.025217\n",
      "Train Epoch: 7 [3920/11051 (35%)]\tLoss: 0.039884\n",
      "Train Epoch: 7 [4000/11051 (36%)]\tLoss: 0.021900\n",
      "Train Epoch: 7 [4080/11051 (37%)]\tLoss: 0.025086\n",
      "Train Epoch: 7 [4160/11051 (38%)]\tLoss: 0.035693\n",
      "Train Epoch: 7 [4240/11051 (38%)]\tLoss: 0.018369\n",
      "Train Epoch: 7 [4320/11051 (39%)]\tLoss: 0.009977\n",
      "Train Epoch: 7 [4400/11051 (40%)]\tLoss: 0.026733\n",
      "Train Epoch: 7 [4480/11051 (41%)]\tLoss: 0.015918\n",
      "Train Epoch: 7 [4560/11051 (41%)]\tLoss: 0.014354\n",
      "Train Epoch: 7 [4640/11051 (42%)]\tLoss: 0.024530\n",
      "Train Epoch: 7 [4720/11051 (43%)]\tLoss: 0.015966\n",
      "Train Epoch: 7 [4800/11051 (43%)]\tLoss: 0.025258\n",
      "Train Epoch: 7 [4880/11051 (44%)]\tLoss: 0.015467\n",
      "Train Epoch: 7 [4960/11051 (45%)]\tLoss: 0.022637\n",
      "Train Epoch: 7 [5040/11051 (46%)]\tLoss: 0.010566\n",
      "Train Epoch: 7 [5120/11051 (46%)]\tLoss: 0.013036\n",
      "Train Epoch: 7 [5200/11051 (47%)]\tLoss: 0.037771\n",
      "Train Epoch: 7 [5280/11051 (48%)]\tLoss: 0.022774\n",
      "Train Epoch: 7 [5360/11051 (48%)]\tLoss: 0.034776\n",
      "Train Epoch: 7 [5440/11051 (49%)]\tLoss: 0.042726\n",
      "Train Epoch: 7 [5520/11051 (50%)]\tLoss: 0.022579\n",
      "Train Epoch: 7 [5600/11051 (51%)]\tLoss: 0.015475\n",
      "Train Epoch: 7 [5680/11051 (51%)]\tLoss: 0.022234\n",
      "Train Epoch: 7 [5760/11051 (52%)]\tLoss: 0.022237\n",
      "Train Epoch: 7 [5840/11051 (53%)]\tLoss: 0.029036\n",
      "Train Epoch: 7 [5920/11051 (54%)]\tLoss: 0.045126\n",
      "Train Epoch: 7 [6000/11051 (54%)]\tLoss: 0.025804\n",
      "Train Epoch: 7 [6080/11051 (55%)]\tLoss: 0.028102\n",
      "Train Epoch: 7 [6160/11051 (56%)]\tLoss: 0.021222\n",
      "Train Epoch: 7 [6240/11051 (56%)]\tLoss: 0.026482\n",
      "Train Epoch: 7 [6320/11051 (57%)]\tLoss: 0.028655\n",
      "Train Epoch: 7 [6400/11051 (58%)]\tLoss: 0.029111\n",
      "Train Epoch: 7 [6480/11051 (59%)]\tLoss: 0.005785\n",
      "Train Epoch: 7 [6560/11051 (59%)]\tLoss: 0.039448\n",
      "Train Epoch: 7 [6640/11051 (60%)]\tLoss: 0.030261\n",
      "Train Epoch: 7 [6720/11051 (61%)]\tLoss: 0.051374\n",
      "Train Epoch: 7 [6800/11051 (62%)]\tLoss: 0.021263\n",
      "Train Epoch: 7 [6880/11051 (62%)]\tLoss: 0.014153\n",
      "Train Epoch: 7 [6960/11051 (63%)]\tLoss: 0.024685\n",
      "Train Epoch: 7 [7040/11051 (64%)]\tLoss: 0.034705\n",
      "Train Epoch: 7 [7120/11051 (64%)]\tLoss: 0.015314\n",
      "Train Epoch: 7 [7200/11051 (65%)]\tLoss: 0.013142\n",
      "Train Epoch: 7 [7280/11051 (66%)]\tLoss: 0.025113\n",
      "Train Epoch: 7 [7360/11051 (67%)]\tLoss: 0.015570\n",
      "Train Epoch: 7 [7440/11051 (67%)]\tLoss: 0.022903\n",
      "Train Epoch: 7 [7520/11051 (68%)]\tLoss: 0.026725\n",
      "Train Epoch: 7 [7600/11051 (69%)]\tLoss: 0.015620\n",
      "Train Epoch: 7 [7680/11051 (69%)]\tLoss: 0.013707\n",
      "Train Epoch: 7 [7760/11051 (70%)]\tLoss: 0.087771\n",
      "Train Epoch: 7 [7840/11051 (71%)]\tLoss: 0.024021\n",
      "Train Epoch: 7 [7920/11051 (72%)]\tLoss: 0.032096\n",
      "Train Epoch: 7 [8000/11051 (72%)]\tLoss: 0.024760\n",
      "Train Epoch: 7 [8080/11051 (73%)]\tLoss: 0.014945\n",
      "Train Epoch: 7 [8160/11051 (74%)]\tLoss: 0.025375\n",
      "Train Epoch: 7 [8240/11051 (75%)]\tLoss: 0.026341\n",
      "Train Epoch: 7 [8320/11051 (75%)]\tLoss: 0.036619\n",
      "Train Epoch: 7 [8400/11051 (76%)]\tLoss: 0.016471\n",
      "Train Epoch: 7 [8480/11051 (77%)]\tLoss: 0.019470\n",
      "Train Epoch: 7 [8560/11051 (77%)]\tLoss: 0.026194\n",
      "Train Epoch: 7 [8640/11051 (78%)]\tLoss: 0.014021\n",
      "Train Epoch: 7 [8720/11051 (79%)]\tLoss: 0.035434\n",
      "Train Epoch: 7 [8800/11051 (80%)]\tLoss: 0.030364\n",
      "Train Epoch: 7 [8880/11051 (80%)]\tLoss: 0.012219\n",
      "Train Epoch: 7 [8960/11051 (81%)]\tLoss: 0.015991\n",
      "Train Epoch: 7 [9040/11051 (82%)]\tLoss: 0.015686\n",
      "Train Epoch: 7 [9120/11051 (82%)]\tLoss: 0.018992\n",
      "Train Epoch: 7 [9200/11051 (83%)]\tLoss: 0.017026\n",
      "Train Epoch: 7 [9280/11051 (84%)]\tLoss: 0.017278\n",
      "Train Epoch: 7 [9360/11051 (85%)]\tLoss: 0.009371\n",
      "Train Epoch: 7 [9440/11051 (85%)]\tLoss: 0.016851\n",
      "Train Epoch: 7 [9520/11051 (86%)]\tLoss: 0.028472\n",
      "Train Epoch: 7 [9600/11051 (87%)]\tLoss: 0.019817\n",
      "Train Epoch: 7 [9680/11051 (88%)]\tLoss: 0.027414\n",
      "Train Epoch: 7 [9760/11051 (88%)]\tLoss: 0.007254\n",
      "Train Epoch: 7 [9840/11051 (89%)]\tLoss: 0.027464\n",
      "Train Epoch: 7 [9920/11051 (90%)]\tLoss: 0.021581\n",
      "Train Epoch: 7 [10000/11051 (90%)]\tLoss: 0.027573\n",
      "Train Epoch: 7 [10080/11051 (91%)]\tLoss: 0.016139\n",
      "Train Epoch: 7 [10160/11051 (92%)]\tLoss: 0.015131\n",
      "Train Epoch: 7 [10240/11051 (93%)]\tLoss: 0.014394\n",
      "Train Epoch: 7 [10320/11051 (93%)]\tLoss: 0.019867\n",
      "Train Epoch: 7 [10400/11051 (94%)]\tLoss: 0.060790\n",
      "Train Epoch: 7 [10480/11051 (95%)]\tLoss: 0.033156\n",
      "Train Epoch: 7 [10560/11051 (96%)]\tLoss: 0.044323\n",
      "Train Epoch: 7 [10640/11051 (96%)]\tLoss: 0.029261\n",
      "Train Epoch: 7 [10720/11051 (97%)]\tLoss: 0.031972\n",
      "Train Epoch: 7 [10800/11051 (98%)]\tLoss: 0.028375\n",
      "Train Epoch: 7 [10880/11051 (98%)]\tLoss: 0.028277\n",
      "Train Epoch: 7 [10960/11051 (99%)]\tLoss: 0.029762\n",
      "Train Epoch: 7 [11040/11051 (100%)]\tLoss: 0.021881\n",
      "====> Epoch: 7 Average loss: 0.0235\n",
      "\n",
      "Started training epoch no. 9\n",
      "Train Epoch: 8 [0/11051 (0%)]\tLoss: 0.009453\n",
      "Train Epoch: 8 [80/11051 (1%)]\tLoss: 0.043290\n",
      "Train Epoch: 8 [160/11051 (1%)]\tLoss: 0.011521\n",
      "Train Epoch: 8 [240/11051 (2%)]\tLoss: 0.010278\n",
      "Train Epoch: 8 [320/11051 (3%)]\tLoss: 0.021583\n",
      "Train Epoch: 8 [400/11051 (4%)]\tLoss: 0.012637\n",
      "Train Epoch: 8 [480/11051 (4%)]\tLoss: 0.020016\n",
      "Train Epoch: 8 [560/11051 (5%)]\tLoss: 0.030004\n",
      "Train Epoch: 8 [640/11051 (6%)]\tLoss: 0.016636\n",
      "Train Epoch: 8 [720/11051 (7%)]\tLoss: 0.021416\n",
      "Train Epoch: 8 [800/11051 (7%)]\tLoss: 0.025006\n",
      "Train Epoch: 8 [880/11051 (8%)]\tLoss: 0.027172\n",
      "Train Epoch: 8 [960/11051 (9%)]\tLoss: 0.015793\n",
      "Train Epoch: 8 [1040/11051 (9%)]\tLoss: 0.023186\n",
      "Train Epoch: 8 [1120/11051 (10%)]\tLoss: 0.030345\n",
      "Train Epoch: 8 [1200/11051 (11%)]\tLoss: 0.014069\n",
      "Train Epoch: 8 [1280/11051 (12%)]\tLoss: 0.026038\n",
      "Train Epoch: 8 [1360/11051 (12%)]\tLoss: 0.016408\n",
      "Train Epoch: 8 [1440/11051 (13%)]\tLoss: 0.019601\n",
      "Train Epoch: 8 [1520/11051 (14%)]\tLoss: 0.050683\n",
      "Train Epoch: 8 [1600/11051 (14%)]\tLoss: 0.018409\n",
      "Train Epoch: 8 [1680/11051 (15%)]\tLoss: 0.023447\n",
      "Train Epoch: 8 [1760/11051 (16%)]\tLoss: 0.040874\n",
      "Train Epoch: 8 [1840/11051 (17%)]\tLoss: 0.013822\n",
      "Train Epoch: 8 [1920/11051 (17%)]\tLoss: 0.010292\n",
      "Train Epoch: 8 [2000/11051 (18%)]\tLoss: 0.013626\n",
      "Train Epoch: 8 [2080/11051 (19%)]\tLoss: 0.015786\n",
      "Train Epoch: 8 [2160/11051 (20%)]\tLoss: 0.024793\n",
      "Train Epoch: 8 [2240/11051 (20%)]\tLoss: 0.046690\n",
      "Train Epoch: 8 [2320/11051 (21%)]\tLoss: 0.025178\n",
      "Train Epoch: 8 [2400/11051 (22%)]\tLoss: 0.014124\n",
      "Train Epoch: 8 [2480/11051 (22%)]\tLoss: 0.016125\n",
      "Train Epoch: 8 [2560/11051 (23%)]\tLoss: 0.035803\n",
      "Train Epoch: 8 [2640/11051 (24%)]\tLoss: 0.015900\n",
      "Train Epoch: 8 [2720/11051 (25%)]\tLoss: 0.024562\n",
      "Train Epoch: 8 [2800/11051 (25%)]\tLoss: 0.036001\n",
      "Train Epoch: 8 [2880/11051 (26%)]\tLoss: 0.026355\n",
      "Train Epoch: 8 [2960/11051 (27%)]\tLoss: 0.023541\n",
      "Train Epoch: 8 [3040/11051 (27%)]\tLoss: 0.013176\n",
      "Train Epoch: 8 [3120/11051 (28%)]\tLoss: 0.011448\n",
      "Train Epoch: 8 [3200/11051 (29%)]\tLoss: 0.010978\n",
      "Train Epoch: 8 [3280/11051 (30%)]\tLoss: 0.028849\n",
      "Train Epoch: 8 [3360/11051 (30%)]\tLoss: 0.023585\n",
      "Train Epoch: 8 [3440/11051 (31%)]\tLoss: 0.045193\n",
      "Train Epoch: 8 [3520/11051 (32%)]\tLoss: 0.030300\n",
      "Train Epoch: 8 [3600/11051 (33%)]\tLoss: 0.017619\n",
      "Train Epoch: 8 [3680/11051 (33%)]\tLoss: 0.014234\n",
      "Train Epoch: 8 [3760/11051 (34%)]\tLoss: 0.020840\n",
      "Train Epoch: 8 [3840/11051 (35%)]\tLoss: 0.009675\n",
      "Train Epoch: 8 [3920/11051 (35%)]\tLoss: 0.009975\n",
      "Train Epoch: 8 [4000/11051 (36%)]\tLoss: 0.011148\n",
      "Train Epoch: 8 [4080/11051 (37%)]\tLoss: 0.013708\n",
      "Train Epoch: 8 [4160/11051 (38%)]\tLoss: 0.026180\n",
      "Train Epoch: 8 [4240/11051 (38%)]\tLoss: 0.011477\n",
      "Train Epoch: 8 [4320/11051 (39%)]\tLoss: 0.035825\n",
      "Train Epoch: 8 [4400/11051 (40%)]\tLoss: 0.011089\n",
      "Train Epoch: 8 [4480/11051 (41%)]\tLoss: 0.029280\n",
      "Train Epoch: 8 [4560/11051 (41%)]\tLoss: 0.047640\n",
      "Train Epoch: 8 [4640/11051 (42%)]\tLoss: 0.031614\n",
      "Train Epoch: 8 [4720/11051 (43%)]\tLoss: 0.015986\n",
      "Train Epoch: 8 [4800/11051 (43%)]\tLoss: 0.031656\n",
      "Train Epoch: 8 [4880/11051 (44%)]\tLoss: 0.021357\n",
      "Train Epoch: 8 [4960/11051 (45%)]\tLoss: 0.023526\n",
      "Train Epoch: 8 [5040/11051 (46%)]\tLoss: 0.023722\n",
      "Train Epoch: 8 [5120/11051 (46%)]\tLoss: 0.014414\n",
      "Train Epoch: 8 [5200/11051 (47%)]\tLoss: 0.018468\n",
      "Train Epoch: 8 [5280/11051 (48%)]\tLoss: 0.020332\n",
      "Train Epoch: 8 [5360/11051 (48%)]\tLoss: 0.019615\n",
      "Train Epoch: 8 [5440/11051 (49%)]\tLoss: 0.028521\n",
      "Train Epoch: 8 [5520/11051 (50%)]\tLoss: 0.039479\n",
      "Train Epoch: 8 [5600/11051 (51%)]\tLoss: 0.025833\n",
      "Train Epoch: 8 [5680/11051 (51%)]\tLoss: 0.025457\n",
      "Train Epoch: 8 [5760/11051 (52%)]\tLoss: 0.036939\n",
      "Train Epoch: 8 [5840/11051 (53%)]\tLoss: 0.016981\n",
      "Train Epoch: 8 [5920/11051 (54%)]\tLoss: 0.039102\n",
      "Train Epoch: 8 [6000/11051 (54%)]\tLoss: 0.020640\n",
      "Train Epoch: 8 [6080/11051 (55%)]\tLoss: 0.045195\n",
      "Train Epoch: 8 [6160/11051 (56%)]\tLoss: 0.022063\n",
      "Train Epoch: 8 [6240/11051 (56%)]\tLoss: 0.003177\n",
      "Train Epoch: 8 [6320/11051 (57%)]\tLoss: 0.016362\n",
      "Train Epoch: 8 [6400/11051 (58%)]\tLoss: 0.024942\n",
      "Train Epoch: 8 [6480/11051 (59%)]\tLoss: 0.051662\n",
      "Train Epoch: 8 [6560/11051 (59%)]\tLoss: 0.024094\n",
      "Train Epoch: 8 [6640/11051 (60%)]\tLoss: 0.019925\n",
      "Train Epoch: 8 [6720/11051 (61%)]\tLoss: 0.026373\n",
      "Train Epoch: 8 [6800/11051 (62%)]\tLoss: 0.024629\n",
      "Train Epoch: 8 [6880/11051 (62%)]\tLoss: 0.013854\n",
      "Train Epoch: 8 [6960/11051 (63%)]\tLoss: 0.015856\n",
      "Train Epoch: 8 [7040/11051 (64%)]\tLoss: 0.039483\n",
      "Train Epoch: 8 [7120/11051 (64%)]\tLoss: 0.014984\n",
      "Train Epoch: 8 [7200/11051 (65%)]\tLoss: 0.017253\n",
      "Train Epoch: 8 [7280/11051 (66%)]\tLoss: 0.015147\n",
      "Train Epoch: 8 [7360/11051 (67%)]\tLoss: 0.043236\n",
      "Train Epoch: 8 [7440/11051 (67%)]\tLoss: 0.034369\n",
      "Train Epoch: 8 [7520/11051 (68%)]\tLoss: 0.021879\n",
      "Train Epoch: 8 [7600/11051 (69%)]\tLoss: 0.034826\n",
      "Train Epoch: 8 [7680/11051 (69%)]\tLoss: 0.017741\n",
      "Train Epoch: 8 [7760/11051 (70%)]\tLoss: 0.008671\n",
      "Train Epoch: 8 [7840/11051 (71%)]\tLoss: 0.009671\n",
      "Train Epoch: 8 [7920/11051 (72%)]\tLoss: 0.023940\n",
      "Train Epoch: 8 [8000/11051 (72%)]\tLoss: 0.015856\n",
      "Train Epoch: 8 [8080/11051 (73%)]\tLoss: 0.020467\n",
      "Train Epoch: 8 [8160/11051 (74%)]\tLoss: 0.032936\n",
      "Train Epoch: 8 [8240/11051 (75%)]\tLoss: 0.027308\n",
      "Train Epoch: 8 [8320/11051 (75%)]\tLoss: 0.024106\n",
      "Train Epoch: 8 [8400/11051 (76%)]\tLoss: 0.019149\n",
      "Train Epoch: 8 [8480/11051 (77%)]\tLoss: 0.021844\n",
      "Train Epoch: 8 [8560/11051 (77%)]\tLoss: 0.009109\n",
      "Train Epoch: 8 [8640/11051 (78%)]\tLoss: 0.019684\n",
      "Train Epoch: 8 [8720/11051 (79%)]\tLoss: 0.013539\n",
      "Train Epoch: 8 [8800/11051 (80%)]\tLoss: 0.030452\n",
      "Train Epoch: 8 [8880/11051 (80%)]\tLoss: 0.019523\n",
      "Train Epoch: 8 [8960/11051 (81%)]\tLoss: 0.017658\n",
      "Train Epoch: 8 [9040/11051 (82%)]\tLoss: 0.019519\n",
      "Train Epoch: 8 [9120/11051 (82%)]\tLoss: 0.009030\n",
      "Train Epoch: 8 [9200/11051 (83%)]\tLoss: 0.019257\n",
      "Train Epoch: 8 [9280/11051 (84%)]\tLoss: 0.024445\n",
      "Train Epoch: 8 [9360/11051 (85%)]\tLoss: 0.037244\n",
      "Train Epoch: 8 [9440/11051 (85%)]\tLoss: 0.019870\n",
      "Train Epoch: 8 [9520/11051 (86%)]\tLoss: 0.014631\n",
      "Train Epoch: 8 [9600/11051 (87%)]\tLoss: 0.010312\n",
      "Train Epoch: 8 [9680/11051 (88%)]\tLoss: 0.019243\n",
      "Train Epoch: 8 [9760/11051 (88%)]\tLoss: 0.029992\n",
      "Train Epoch: 8 [9840/11051 (89%)]\tLoss: 0.023706\n",
      "Train Epoch: 8 [9920/11051 (90%)]\tLoss: 0.054813\n",
      "Train Epoch: 8 [10000/11051 (90%)]\tLoss: 0.015793\n",
      "Train Epoch: 8 [10080/11051 (91%)]\tLoss: 0.032023\n",
      "Train Epoch: 8 [10160/11051 (92%)]\tLoss: 0.014543\n",
      "Train Epoch: 8 [10240/11051 (93%)]\tLoss: 0.015639\n",
      "Train Epoch: 8 [10320/11051 (93%)]\tLoss: 0.021926\n",
      "Train Epoch: 8 [10400/11051 (94%)]\tLoss: 0.027925\n",
      "Train Epoch: 8 [10480/11051 (95%)]\tLoss: 0.015694\n",
      "Train Epoch: 8 [10560/11051 (96%)]\tLoss: 0.012027\n",
      "Train Epoch: 8 [10640/11051 (96%)]\tLoss: 0.014025\n",
      "Train Epoch: 8 [10720/11051 (97%)]\tLoss: 0.030735\n",
      "Train Epoch: 8 [10800/11051 (98%)]\tLoss: 0.027614\n",
      "Train Epoch: 8 [10880/11051 (98%)]\tLoss: 0.033708\n",
      "Train Epoch: 8 [10960/11051 (99%)]\tLoss: 0.019119\n",
      "Train Epoch: 8 [11040/11051 (100%)]\tLoss: 0.016100\n",
      "====> Epoch: 8 Average loss: 0.0233\n",
      "\n",
      "Started training epoch no. 10\n",
      "Train Epoch: 9 [0/11051 (0%)]\tLoss: 0.015208\n",
      "Train Epoch: 9 [80/11051 (1%)]\tLoss: 0.050597\n",
      "Train Epoch: 9 [160/11051 (1%)]\tLoss: 0.021903\n",
      "Train Epoch: 9 [240/11051 (2%)]\tLoss: 0.023131\n",
      "Train Epoch: 9 [320/11051 (3%)]\tLoss: 0.026532\n",
      "Train Epoch: 9 [400/11051 (4%)]\tLoss: 0.022249\n",
      "Train Epoch: 9 [480/11051 (4%)]\tLoss: 0.033568\n",
      "Train Epoch: 9 [560/11051 (5%)]\tLoss: 0.023805\n",
      "Train Epoch: 9 [640/11051 (6%)]\tLoss: 0.024230\n",
      "Train Epoch: 9 [720/11051 (7%)]\tLoss: 0.013972\n",
      "Train Epoch: 9 [800/11051 (7%)]\tLoss: 0.031971\n",
      "Train Epoch: 9 [880/11051 (8%)]\tLoss: 0.027276\n",
      "Train Epoch: 9 [960/11051 (9%)]\tLoss: 0.038371\n",
      "Train Epoch: 9 [1040/11051 (9%)]\tLoss: 0.039064\n",
      "Train Epoch: 9 [1120/11051 (10%)]\tLoss: 0.015110\n",
      "Train Epoch: 9 [1200/11051 (11%)]\tLoss: 0.015290\n",
      "Train Epoch: 9 [1280/11051 (12%)]\tLoss: 0.008813\n",
      "Train Epoch: 9 [1360/11051 (12%)]\tLoss: 0.030590\n",
      "Train Epoch: 9 [1440/11051 (13%)]\tLoss: 0.007966\n",
      "Train Epoch: 9 [1520/11051 (14%)]\tLoss: 0.032227\n",
      "Train Epoch: 9 [1600/11051 (14%)]\tLoss: 0.005799\n",
      "Train Epoch: 9 [1680/11051 (15%)]\tLoss: 0.023381\n",
      "Train Epoch: 9 [1760/11051 (16%)]\tLoss: 0.032793\n",
      "Train Epoch: 9 [1840/11051 (17%)]\tLoss: 0.022447\n",
      "Train Epoch: 9 [1920/11051 (17%)]\tLoss: 0.022907\n",
      "Train Epoch: 9 [2000/11051 (18%)]\tLoss: 0.012246\n",
      "Train Epoch: 9 [2080/11051 (19%)]\tLoss: 0.029967\n",
      "Train Epoch: 9 [2160/11051 (20%)]\tLoss: 0.010418\n",
      "Train Epoch: 9 [2240/11051 (20%)]\tLoss: 0.026161\n",
      "Train Epoch: 9 [2320/11051 (21%)]\tLoss: 0.029953\n",
      "Train Epoch: 9 [2400/11051 (22%)]\tLoss: 0.034465\n",
      "Train Epoch: 9 [2480/11051 (22%)]\tLoss: 0.032597\n",
      "Train Epoch: 9 [2560/11051 (23%)]\tLoss: 0.021064\n",
      "Train Epoch: 9 [2640/11051 (24%)]\tLoss: 0.028929\n",
      "Train Epoch: 9 [2720/11051 (25%)]\tLoss: 0.019240\n",
      "Train Epoch: 9 [2800/11051 (25%)]\tLoss: 0.022908\n",
      "Train Epoch: 9 [2880/11051 (26%)]\tLoss: 0.011510\n",
      "Train Epoch: 9 [2960/11051 (27%)]\tLoss: 0.019430\n",
      "Train Epoch: 9 [3040/11051 (27%)]\tLoss: 0.027869\n",
      "Train Epoch: 9 [3120/11051 (28%)]\tLoss: 0.029177\n",
      "Train Epoch: 9 [3200/11051 (29%)]\tLoss: 0.067668\n",
      "Train Epoch: 9 [3280/11051 (30%)]\tLoss: 0.005906\n",
      "Train Epoch: 9 [3360/11051 (30%)]\tLoss: 0.029789\n",
      "Train Epoch: 9 [3440/11051 (31%)]\tLoss: 0.018398\n",
      "Train Epoch: 9 [3520/11051 (32%)]\tLoss: 0.021826\n",
      "Train Epoch: 9 [3600/11051 (33%)]\tLoss: 0.045199\n",
      "Train Epoch: 9 [3680/11051 (33%)]\tLoss: 0.017269\n",
      "Train Epoch: 9 [3760/11051 (34%)]\tLoss: 0.014106\n",
      "Train Epoch: 9 [3840/11051 (35%)]\tLoss: 0.024448\n",
      "Train Epoch: 9 [3920/11051 (35%)]\tLoss: 0.032293\n",
      "Train Epoch: 9 [4000/11051 (36%)]\tLoss: 0.023868\n",
      "Train Epoch: 9 [4080/11051 (37%)]\tLoss: 0.023946\n",
      "Train Epoch: 9 [4160/11051 (38%)]\tLoss: 0.013852\n",
      "Train Epoch: 9 [4240/11051 (38%)]\tLoss: 0.014709\n",
      "Train Epoch: 9 [4320/11051 (39%)]\tLoss: 0.030190\n",
      "Train Epoch: 9 [4400/11051 (40%)]\tLoss: 0.056449\n",
      "Train Epoch: 9 [4480/11051 (41%)]\tLoss: 0.013637\n",
      "Train Epoch: 9 [4560/11051 (41%)]\tLoss: 0.020375\n",
      "Train Epoch: 9 [4640/11051 (42%)]\tLoss: 0.020436\n",
      "Train Epoch: 9 [4720/11051 (43%)]\tLoss: 0.010964\n",
      "Train Epoch: 9 [4800/11051 (43%)]\tLoss: 0.027641\n",
      "Train Epoch: 9 [4880/11051 (44%)]\tLoss: 0.011868\n",
      "Train Epoch: 9 [4960/11051 (45%)]\tLoss: 0.019570\n",
      "Train Epoch: 9 [5040/11051 (46%)]\tLoss: 0.032365\n",
      "Train Epoch: 9 [5120/11051 (46%)]\tLoss: 0.020175\n",
      "Train Epoch: 9 [5200/11051 (47%)]\tLoss: 0.028576\n",
      "Train Epoch: 9 [5280/11051 (48%)]\tLoss: 0.028989\n",
      "Train Epoch: 9 [5360/11051 (48%)]\tLoss: 0.036881\n",
      "Train Epoch: 9 [5440/11051 (49%)]\tLoss: 0.028617\n",
      "Train Epoch: 9 [5520/11051 (50%)]\tLoss: 0.021705\n",
      "Train Epoch: 9 [5600/11051 (51%)]\tLoss: 0.019157\n",
      "Train Epoch: 9 [5680/11051 (51%)]\tLoss: 0.024981\n",
      "Train Epoch: 9 [5760/11051 (52%)]\tLoss: 0.032951\n",
      "Train Epoch: 9 [5840/11051 (53%)]\tLoss: 0.022846\n",
      "Train Epoch: 9 [5920/11051 (54%)]\tLoss: 0.056773\n",
      "Train Epoch: 9 [6000/11051 (54%)]\tLoss: 0.004834\n",
      "Train Epoch: 9 [6080/11051 (55%)]\tLoss: 0.015381\n",
      "Train Epoch: 9 [6160/11051 (56%)]\tLoss: 0.014264\n",
      "Train Epoch: 9 [6240/11051 (56%)]\tLoss: 0.035759\n",
      "Train Epoch: 9 [6320/11051 (57%)]\tLoss: 0.011819\n",
      "Train Epoch: 9 [6400/11051 (58%)]\tLoss: 0.023398\n",
      "Train Epoch: 9 [6480/11051 (59%)]\tLoss: 0.030514\n",
      "Train Epoch: 9 [6560/11051 (59%)]\tLoss: 0.027615\n",
      "Train Epoch: 9 [6640/11051 (60%)]\tLoss: 0.027103\n",
      "Train Epoch: 9 [6720/11051 (61%)]\tLoss: 0.022887\n",
      "Train Epoch: 9 [6800/11051 (62%)]\tLoss: 0.027907\n",
      "Train Epoch: 9 [6880/11051 (62%)]\tLoss: 0.019216\n",
      "Train Epoch: 9 [6960/11051 (63%)]\tLoss: 0.046392\n",
      "Train Epoch: 9 [7040/11051 (64%)]\tLoss: 0.028671\n",
      "Train Epoch: 9 [7120/11051 (64%)]\tLoss: 0.024354\n",
      "Train Epoch: 9 [7200/11051 (65%)]\tLoss: 0.038059\n",
      "Train Epoch: 9 [7280/11051 (66%)]\tLoss: 0.019543\n",
      "Train Epoch: 9 [7360/11051 (67%)]\tLoss: 0.016254\n",
      "Train Epoch: 9 [7440/11051 (67%)]\tLoss: 0.016738\n",
      "Train Epoch: 9 [7520/11051 (68%)]\tLoss: 0.014842\n",
      "Train Epoch: 9 [7600/11051 (69%)]\tLoss: 0.012153\n",
      "Train Epoch: 9 [7680/11051 (69%)]\tLoss: 0.012054\n",
      "Train Epoch: 9 [7760/11051 (70%)]\tLoss: 0.022114\n",
      "Train Epoch: 9 [7840/11051 (71%)]\tLoss: 0.003262\n",
      "Train Epoch: 9 [7920/11051 (72%)]\tLoss: 0.028154\n",
      "Train Epoch: 9 [8000/11051 (72%)]\tLoss: 0.027345\n",
      "Train Epoch: 9 [8080/11051 (73%)]\tLoss: 0.031974\n",
      "Train Epoch: 9 [8160/11051 (74%)]\tLoss: 0.011407\n",
      "Train Epoch: 9 [8240/11051 (75%)]\tLoss: 0.025338\n",
      "Train Epoch: 9 [8320/11051 (75%)]\tLoss: 0.009169\n",
      "Train Epoch: 9 [8400/11051 (76%)]\tLoss: 0.021713\n",
      "Train Epoch: 9 [8480/11051 (77%)]\tLoss: 0.018582\n",
      "Train Epoch: 9 [8560/11051 (77%)]\tLoss: 0.023273\n",
      "Train Epoch: 9 [8640/11051 (78%)]\tLoss: 0.074359\n",
      "Train Epoch: 9 [8720/11051 (79%)]\tLoss: 0.024928\n",
      "Train Epoch: 9 [8800/11051 (80%)]\tLoss: 0.024569\n",
      "Train Epoch: 9 [8880/11051 (80%)]\tLoss: 0.030895\n",
      "Train Epoch: 9 [8960/11051 (81%)]\tLoss: 0.042837\n",
      "Train Epoch: 9 [9040/11051 (82%)]\tLoss: 0.028723\n",
      "Train Epoch: 9 [9120/11051 (82%)]\tLoss: 0.026393\n",
      "Train Epoch: 9 [9200/11051 (83%)]\tLoss: 0.012381\n",
      "Train Epoch: 9 [9280/11051 (84%)]\tLoss: 0.039243\n",
      "Train Epoch: 9 [9360/11051 (85%)]\tLoss: 0.050091\n",
      "Train Epoch: 9 [9440/11051 (85%)]\tLoss: 0.023666\n",
      "Train Epoch: 9 [9520/11051 (86%)]\tLoss: 0.010931\n",
      "Train Epoch: 9 [9600/11051 (87%)]\tLoss: 0.021381\n",
      "Train Epoch: 9 [9680/11051 (88%)]\tLoss: 0.011008\n",
      "Train Epoch: 9 [9760/11051 (88%)]\tLoss: 0.019072\n",
      "Train Epoch: 9 [9840/11051 (89%)]\tLoss: 0.012344\n",
      "Train Epoch: 9 [9920/11051 (90%)]\tLoss: 0.018339\n",
      "Train Epoch: 9 [10000/11051 (90%)]\tLoss: 0.022951\n",
      "Train Epoch: 9 [10080/11051 (91%)]\tLoss: 0.045862\n",
      "Train Epoch: 9 [10160/11051 (92%)]\tLoss: 0.018740\n",
      "Train Epoch: 9 [10240/11051 (93%)]\tLoss: 0.004531\n",
      "Train Epoch: 9 [10320/11051 (93%)]\tLoss: 0.024169\n",
      "Train Epoch: 9 [10400/11051 (94%)]\tLoss: 0.022412\n",
      "Train Epoch: 9 [10480/11051 (95%)]\tLoss: 0.019730\n",
      "Train Epoch: 9 [10560/11051 (96%)]\tLoss: 0.019039\n",
      "Train Epoch: 9 [10640/11051 (96%)]\tLoss: 0.024177\n",
      "Train Epoch: 9 [10720/11051 (97%)]\tLoss: 0.033024\n",
      "Train Epoch: 9 [10800/11051 (98%)]\tLoss: 0.013876\n",
      "Train Epoch: 9 [10880/11051 (98%)]\tLoss: 0.026454\n",
      "Train Epoch: 9 [10960/11051 (99%)]\tLoss: 0.021833\n",
      "Train Epoch: 9 [11040/11051 (100%)]\tLoss: 0.034684\n",
      "====> Epoch: 9 Average loss: 0.0231\n",
      "\n",
      "Started training epoch no. 11\n",
      "Train Epoch: 10 [0/11051 (0%)]\tLoss: 0.032341\n",
      "Train Epoch: 10 [80/11051 (1%)]\tLoss: 0.027690\n",
      "Train Epoch: 10 [160/11051 (1%)]\tLoss: 0.019229\n",
      "Train Epoch: 10 [240/11051 (2%)]\tLoss: 0.023687\n",
      "Train Epoch: 10 [320/11051 (3%)]\tLoss: 0.032650\n",
      "Train Epoch: 10 [400/11051 (4%)]\tLoss: 0.023200\n",
      "Train Epoch: 10 [480/11051 (4%)]\tLoss: 0.023046\n",
      "Train Epoch: 10 [560/11051 (5%)]\tLoss: 0.007914\n",
      "Train Epoch: 10 [640/11051 (6%)]\tLoss: 0.016926\n",
      "Train Epoch: 10 [720/11051 (7%)]\tLoss: 0.029380\n",
      "Train Epoch: 10 [800/11051 (7%)]\tLoss: 0.022031\n",
      "Train Epoch: 10 [880/11051 (8%)]\tLoss: 0.029586\n",
      "Train Epoch: 10 [960/11051 (9%)]\tLoss: 0.040082\n",
      "Train Epoch: 10 [1040/11051 (9%)]\tLoss: 0.022808\n",
      "Train Epoch: 10 [1120/11051 (10%)]\tLoss: 0.025131\n",
      "Train Epoch: 10 [1200/11051 (11%)]\tLoss: 0.013732\n",
      "Train Epoch: 10 [1280/11051 (12%)]\tLoss: 0.010641\n",
      "Train Epoch: 10 [1360/11051 (12%)]\tLoss: 0.019465\n",
      "Train Epoch: 10 [1440/11051 (13%)]\tLoss: 0.022092\n",
      "Train Epoch: 10 [1520/11051 (14%)]\tLoss: 0.013562\n",
      "Train Epoch: 10 [1600/11051 (14%)]\tLoss: 0.028355\n",
      "Train Epoch: 10 [1680/11051 (15%)]\tLoss: 0.042688\n",
      "Train Epoch: 10 [1760/11051 (16%)]\tLoss: 0.026166\n",
      "Train Epoch: 10 [1840/11051 (17%)]\tLoss: 0.023764\n",
      "Train Epoch: 10 [1920/11051 (17%)]\tLoss: 0.016373\n",
      "Train Epoch: 10 [2000/11051 (18%)]\tLoss: 0.021371\n",
      "Train Epoch: 10 [2080/11051 (19%)]\tLoss: 0.029591\n",
      "Train Epoch: 10 [2160/11051 (20%)]\tLoss: 0.021901\n",
      "Train Epoch: 10 [2240/11051 (20%)]\tLoss: 0.038695\n",
      "Train Epoch: 10 [2320/11051 (21%)]\tLoss: 0.023360\n",
      "Train Epoch: 10 [2400/11051 (22%)]\tLoss: 0.050416\n",
      "Train Epoch: 10 [2480/11051 (22%)]\tLoss: 0.012991\n",
      "Train Epoch: 10 [2560/11051 (23%)]\tLoss: 0.024046\n",
      "Train Epoch: 10 [2640/11051 (24%)]\tLoss: 0.020966\n",
      "Train Epoch: 10 [2720/11051 (25%)]\tLoss: 0.012180\n",
      "Train Epoch: 10 [2800/11051 (25%)]\tLoss: 0.026702\n",
      "Train Epoch: 10 [2880/11051 (26%)]\tLoss: 0.019019\n",
      "Train Epoch: 10 [2960/11051 (27%)]\tLoss: 0.008461\n",
      "Train Epoch: 10 [3040/11051 (27%)]\tLoss: 0.021048\n",
      "Train Epoch: 10 [3120/11051 (28%)]\tLoss: 0.013476\n",
      "Train Epoch: 10 [3200/11051 (29%)]\tLoss: 0.016867\n",
      "Train Epoch: 10 [3280/11051 (30%)]\tLoss: 0.027975\n",
      "Train Epoch: 10 [3360/11051 (30%)]\tLoss: 0.019587\n",
      "Train Epoch: 10 [3440/11051 (31%)]\tLoss: 0.013712\n",
      "Train Epoch: 10 [3520/11051 (32%)]\tLoss: 0.031961\n",
      "Train Epoch: 10 [3600/11051 (33%)]\tLoss: 0.024666\n",
      "Train Epoch: 10 [3680/11051 (33%)]\tLoss: 0.030183\n",
      "Train Epoch: 10 [3760/11051 (34%)]\tLoss: 0.021124\n",
      "Train Epoch: 10 [3840/11051 (35%)]\tLoss: 0.027550\n",
      "Train Epoch: 10 [3920/11051 (35%)]\tLoss: 0.016941\n",
      "Train Epoch: 10 [4000/11051 (36%)]\tLoss: 0.022189\n",
      "Train Epoch: 10 [4080/11051 (37%)]\tLoss: 0.012946\n",
      "Train Epoch: 10 [4160/11051 (38%)]\tLoss: 0.031655\n",
      "Train Epoch: 10 [4240/11051 (38%)]\tLoss: 0.015970\n",
      "Train Epoch: 10 [4320/11051 (39%)]\tLoss: 0.008952\n",
      "Train Epoch: 10 [4400/11051 (40%)]\tLoss: 0.010660\n",
      "Train Epoch: 10 [4480/11051 (41%)]\tLoss: 0.018073\n",
      "Train Epoch: 10 [4560/11051 (41%)]\tLoss: 0.031476\n",
      "Train Epoch: 10 [4640/11051 (42%)]\tLoss: 0.019217\n",
      "Train Epoch: 10 [4720/11051 (43%)]\tLoss: 0.036538\n",
      "Train Epoch: 10 [4800/11051 (43%)]\tLoss: 0.023148\n",
      "Train Epoch: 10 [4880/11051 (44%)]\tLoss: 0.059020\n",
      "Train Epoch: 10 [4960/11051 (45%)]\tLoss: 0.018373\n",
      "Train Epoch: 10 [5040/11051 (46%)]\tLoss: 0.035432\n",
      "Train Epoch: 10 [5120/11051 (46%)]\tLoss: 0.041105\n",
      "Train Epoch: 10 [5200/11051 (47%)]\tLoss: 0.018032\n",
      "Train Epoch: 10 [5280/11051 (48%)]\tLoss: 0.030285\n",
      "Train Epoch: 10 [5360/11051 (48%)]\tLoss: 0.017859\n",
      "Train Epoch: 10 [5440/11051 (49%)]\tLoss: 0.027378\n",
      "Train Epoch: 10 [5520/11051 (50%)]\tLoss: 0.031763\n",
      "Train Epoch: 10 [5600/11051 (51%)]\tLoss: 0.023119\n",
      "Train Epoch: 10 [5680/11051 (51%)]\tLoss: 0.018881\n",
      "Train Epoch: 10 [5760/11051 (52%)]\tLoss: 0.035174\n",
      "Train Epoch: 10 [5840/11051 (53%)]\tLoss: 0.037332\n",
      "Train Epoch: 10 [5920/11051 (54%)]\tLoss: 0.018452\n",
      "Train Epoch: 10 [6000/11051 (54%)]\tLoss: 0.014986\n",
      "Train Epoch: 10 [6080/11051 (55%)]\tLoss: 0.017271\n",
      "Train Epoch: 10 [6160/11051 (56%)]\tLoss: 0.032634\n",
      "Train Epoch: 10 [6240/11051 (56%)]\tLoss: 0.018773\n",
      "Train Epoch: 10 [6320/11051 (57%)]\tLoss: 0.017547\n",
      "Train Epoch: 10 [6400/11051 (58%)]\tLoss: 0.014631\n",
      "Train Epoch: 10 [6480/11051 (59%)]\tLoss: 0.014661\n",
      "Train Epoch: 10 [6560/11051 (59%)]\tLoss: 0.021750\n",
      "Train Epoch: 10 [6640/11051 (60%)]\tLoss: 0.024423\n",
      "Train Epoch: 10 [6720/11051 (61%)]\tLoss: 0.030062\n",
      "Train Epoch: 10 [6800/11051 (62%)]\tLoss: 0.015062\n",
      "Train Epoch: 10 [6880/11051 (62%)]\tLoss: 0.013814\n",
      "Train Epoch: 10 [6960/11051 (63%)]\tLoss: 0.020758\n",
      "Train Epoch: 10 [7040/11051 (64%)]\tLoss: 0.023150\n",
      "Train Epoch: 10 [7120/11051 (64%)]\tLoss: 0.012308\n",
      "Train Epoch: 10 [7200/11051 (65%)]\tLoss: 0.032681\n",
      "Train Epoch: 10 [7280/11051 (66%)]\tLoss: 0.036720\n",
      "Train Epoch: 10 [7360/11051 (67%)]\tLoss: 0.012602\n",
      "Train Epoch: 10 [7440/11051 (67%)]\tLoss: 0.015161\n",
      "Train Epoch: 10 [7520/11051 (68%)]\tLoss: 0.018925\n",
      "Train Epoch: 10 [7600/11051 (69%)]\tLoss: 0.033015\n",
      "Train Epoch: 10 [7680/11051 (69%)]\tLoss: 0.027841\n",
      "Train Epoch: 10 [7760/11051 (70%)]\tLoss: 0.019780\n",
      "Train Epoch: 10 [7840/11051 (71%)]\tLoss: 0.018035\n",
      "Train Epoch: 10 [7920/11051 (72%)]\tLoss: 0.021713\n",
      "Train Epoch: 10 [8000/11051 (72%)]\tLoss: 0.015221\n",
      "Train Epoch: 10 [8080/11051 (73%)]\tLoss: 0.025797\n",
      "Train Epoch: 10 [8160/11051 (74%)]\tLoss: 0.022898\n",
      "Train Epoch: 10 [8240/11051 (75%)]\tLoss: 0.026023\n",
      "Train Epoch: 10 [8320/11051 (75%)]\tLoss: 0.037088\n",
      "Train Epoch: 10 [8400/11051 (76%)]\tLoss: 0.019353\n",
      "Train Epoch: 10 [8480/11051 (77%)]\tLoss: 0.019058\n",
      "Train Epoch: 10 [8560/11051 (77%)]\tLoss: 0.030281\n",
      "Train Epoch: 10 [8640/11051 (78%)]\tLoss: 0.018166\n",
      "Train Epoch: 10 [8720/11051 (79%)]\tLoss: 0.015664\n",
      "Train Epoch: 10 [8800/11051 (80%)]\tLoss: 0.019817\n",
      "Train Epoch: 10 [8880/11051 (80%)]\tLoss: 0.014651\n",
      "Train Epoch: 10 [8960/11051 (81%)]\tLoss: 0.016675\n",
      "Train Epoch: 10 [9040/11051 (82%)]\tLoss: 0.017511\n",
      "Train Epoch: 10 [9120/11051 (82%)]\tLoss: 0.023783\n",
      "Train Epoch: 10 [9200/11051 (83%)]\tLoss: 0.013822\n",
      "Train Epoch: 10 [9280/11051 (84%)]\tLoss: 0.020687\n",
      "Train Epoch: 10 [9360/11051 (85%)]\tLoss: 0.017141\n",
      "Train Epoch: 10 [9440/11051 (85%)]\tLoss: 0.019030\n",
      "Train Epoch: 10 [9520/11051 (86%)]\tLoss: 0.019751\n",
      "Train Epoch: 10 [9600/11051 (87%)]\tLoss: 0.015980\n",
      "Train Epoch: 10 [9680/11051 (88%)]\tLoss: 0.025495\n",
      "Train Epoch: 10 [9760/11051 (88%)]\tLoss: 0.013043\n",
      "Train Epoch: 10 [9840/11051 (89%)]\tLoss: 0.030894\n",
      "Train Epoch: 10 [9920/11051 (90%)]\tLoss: 0.013276\n",
      "Train Epoch: 10 [10000/11051 (90%)]\tLoss: 0.024982\n",
      "Train Epoch: 10 [10080/11051 (91%)]\tLoss: 0.027976\n",
      "Train Epoch: 10 [10160/11051 (92%)]\tLoss: 0.010938\n",
      "Train Epoch: 10 [10240/11051 (93%)]\tLoss: 0.043400\n",
      "Train Epoch: 10 [10320/11051 (93%)]\tLoss: 0.015595\n",
      "Train Epoch: 10 [10400/11051 (94%)]\tLoss: 0.014919\n",
      "Train Epoch: 10 [10480/11051 (95%)]\tLoss: 0.037804\n",
      "Train Epoch: 10 [10560/11051 (96%)]\tLoss: 0.022529\n",
      "Train Epoch: 10 [10640/11051 (96%)]\tLoss: 0.012685\n",
      "Train Epoch: 10 [10720/11051 (97%)]\tLoss: 0.019762\n",
      "Train Epoch: 10 [10800/11051 (98%)]\tLoss: 0.031194\n",
      "Train Epoch: 10 [10880/11051 (98%)]\tLoss: 0.008335\n",
      "Train Epoch: 10 [10960/11051 (99%)]\tLoss: 0.027509\n",
      "Train Epoch: 10 [11040/11051 (100%)]\tLoss: 0.023456\n",
      "====> Epoch: 10 Average loss: 0.0229\n",
      "\n",
      "Started training epoch no. 12\n",
      "Train Epoch: 11 [0/11051 (0%)]\tLoss: 0.018223\n",
      "Train Epoch: 11 [80/11051 (1%)]\tLoss: 0.034820\n",
      "Train Epoch: 11 [160/11051 (1%)]\tLoss: 0.006587\n",
      "Train Epoch: 11 [240/11051 (2%)]\tLoss: 0.017793\n",
      "Train Epoch: 11 [320/11051 (3%)]\tLoss: 0.031308\n",
      "Train Epoch: 11 [400/11051 (4%)]\tLoss: 0.015647\n",
      "Train Epoch: 11 [480/11051 (4%)]\tLoss: 0.036525\n",
      "Train Epoch: 11 [560/11051 (5%)]\tLoss: 0.045127\n",
      "Train Epoch: 11 [640/11051 (6%)]\tLoss: 0.014141\n",
      "Train Epoch: 11 [720/11051 (7%)]\tLoss: 0.027264\n",
      "Train Epoch: 11 [800/11051 (7%)]\tLoss: 0.012650\n",
      "Train Epoch: 11 [880/11051 (8%)]\tLoss: 0.028790\n",
      "Train Epoch: 11 [960/11051 (9%)]\tLoss: 0.013997\n",
      "Train Epoch: 11 [1040/11051 (9%)]\tLoss: 0.016709\n",
      "Train Epoch: 11 [1120/11051 (10%)]\tLoss: 0.017342\n",
      "Train Epoch: 11 [1200/11051 (11%)]\tLoss: 0.020875\n",
      "Train Epoch: 11 [1280/11051 (12%)]\tLoss: 0.014731\n",
      "Train Epoch: 11 [1360/11051 (12%)]\tLoss: 0.023175\n",
      "Train Epoch: 11 [1440/11051 (13%)]\tLoss: 0.009465\n",
      "Train Epoch: 11 [1520/11051 (14%)]\tLoss: 0.022920\n",
      "Train Epoch: 11 [1600/11051 (14%)]\tLoss: 0.009663\n",
      "Train Epoch: 11 [1680/11051 (15%)]\tLoss: 0.027784\n",
      "Train Epoch: 11 [1760/11051 (16%)]\tLoss: 0.033080\n",
      "Train Epoch: 11 [1840/11051 (17%)]\tLoss: 0.016239\n",
      "Train Epoch: 11 [1920/11051 (17%)]\tLoss: 0.030894\n",
      "Train Epoch: 11 [2000/11051 (18%)]\tLoss: 0.023984\n",
      "Train Epoch: 11 [2080/11051 (19%)]\tLoss: 0.033152\n",
      "Train Epoch: 11 [2160/11051 (20%)]\tLoss: 0.014460\n",
      "Train Epoch: 11 [2240/11051 (20%)]\tLoss: 0.015709\n",
      "Train Epoch: 11 [2320/11051 (21%)]\tLoss: 0.015351\n",
      "Train Epoch: 11 [2400/11051 (22%)]\tLoss: 0.009288\n",
      "Train Epoch: 11 [2480/11051 (22%)]\tLoss: 0.021261\n",
      "Train Epoch: 11 [2560/11051 (23%)]\tLoss: 0.022061\n",
      "Train Epoch: 11 [2640/11051 (24%)]\tLoss: 0.035042\n",
      "Train Epoch: 11 [2720/11051 (25%)]\tLoss: 0.020740\n",
      "Train Epoch: 11 [2800/11051 (25%)]\tLoss: 0.022226\n",
      "Train Epoch: 11 [2880/11051 (26%)]\tLoss: 0.016467\n",
      "Train Epoch: 11 [2960/11051 (27%)]\tLoss: 0.025322\n",
      "Train Epoch: 11 [3040/11051 (27%)]\tLoss: 0.021041\n",
      "Train Epoch: 11 [3120/11051 (28%)]\tLoss: 0.011720\n",
      "Train Epoch: 11 [3200/11051 (29%)]\tLoss: 0.018097\n",
      "Train Epoch: 11 [3280/11051 (30%)]\tLoss: 0.017807\n",
      "Train Epoch: 11 [3360/11051 (30%)]\tLoss: 0.031688\n",
      "Train Epoch: 11 [3440/11051 (31%)]\tLoss: 0.025227\n",
      "Train Epoch: 11 [3520/11051 (32%)]\tLoss: 0.014465\n",
      "Train Epoch: 11 [3600/11051 (33%)]\tLoss: 0.021852\n",
      "Train Epoch: 11 [3680/11051 (33%)]\tLoss: 0.030738\n",
      "Train Epoch: 11 [3760/11051 (34%)]\tLoss: 0.014767\n",
      "Train Epoch: 11 [3840/11051 (35%)]\tLoss: 0.022530\n",
      "Train Epoch: 11 [3920/11051 (35%)]\tLoss: 0.012173\n",
      "Train Epoch: 11 [4000/11051 (36%)]\tLoss: 0.031834\n",
      "Train Epoch: 11 [4080/11051 (37%)]\tLoss: 0.038973\n",
      "Train Epoch: 11 [4160/11051 (38%)]\tLoss: 0.017091\n",
      "Train Epoch: 11 [4240/11051 (38%)]\tLoss: 0.047274\n",
      "Train Epoch: 11 [4320/11051 (39%)]\tLoss: 0.034897\n",
      "Train Epoch: 11 [4400/11051 (40%)]\tLoss: 0.029061\n",
      "Train Epoch: 11 [4480/11051 (41%)]\tLoss: 0.010610\n",
      "Train Epoch: 11 [4560/11051 (41%)]\tLoss: 0.029925\n",
      "Train Epoch: 11 [4640/11051 (42%)]\tLoss: 0.020479\n",
      "Train Epoch: 11 [4720/11051 (43%)]\tLoss: 0.024185\n",
      "Train Epoch: 11 [4800/11051 (43%)]\tLoss: 0.031982\n",
      "Train Epoch: 11 [4880/11051 (44%)]\tLoss: 0.089919\n",
      "Train Epoch: 11 [4960/11051 (45%)]\tLoss: 0.018913\n",
      "Train Epoch: 11 [5040/11051 (46%)]\tLoss: 0.016371\n",
      "Train Epoch: 11 [5120/11051 (46%)]\tLoss: 0.019179\n",
      "Train Epoch: 11 [5200/11051 (47%)]\tLoss: 0.005793\n",
      "Train Epoch: 11 [5280/11051 (48%)]\tLoss: 0.016997\n",
      "Train Epoch: 11 [5360/11051 (48%)]\tLoss: 0.022427\n",
      "Train Epoch: 11 [5440/11051 (49%)]\tLoss: 0.021138\n",
      "Train Epoch: 11 [5520/11051 (50%)]\tLoss: 0.030861\n",
      "Train Epoch: 11 [5600/11051 (51%)]\tLoss: 0.028580\n",
      "Train Epoch: 11 [5680/11051 (51%)]\tLoss: 0.027201\n",
      "Train Epoch: 11 [5760/11051 (52%)]\tLoss: 0.009553\n",
      "Train Epoch: 11 [5840/11051 (53%)]\tLoss: 0.018824\n",
      "Train Epoch: 11 [5920/11051 (54%)]\tLoss: 0.022469\n",
      "Train Epoch: 11 [6000/11051 (54%)]\tLoss: 0.018124\n",
      "Train Epoch: 11 [6080/11051 (55%)]\tLoss: 0.017064\n",
      "Train Epoch: 11 [6160/11051 (56%)]\tLoss: 0.024963\n",
      "Train Epoch: 11 [6240/11051 (56%)]\tLoss: 0.013425\n",
      "Train Epoch: 11 [6320/11051 (57%)]\tLoss: 0.023923\n",
      "Train Epoch: 11 [6400/11051 (58%)]\tLoss: 0.028323\n",
      "Train Epoch: 11 [6480/11051 (59%)]\tLoss: 0.009507\n",
      "Train Epoch: 11 [6560/11051 (59%)]\tLoss: 0.017005\n",
      "Train Epoch: 11 [6640/11051 (60%)]\tLoss: 0.023482\n",
      "Train Epoch: 11 [6720/11051 (61%)]\tLoss: 0.031795\n",
      "Train Epoch: 11 [6800/11051 (62%)]\tLoss: 0.025569\n",
      "Train Epoch: 11 [6880/11051 (62%)]\tLoss: 0.027534\n",
      "Train Epoch: 11 [6960/11051 (63%)]\tLoss: 0.017930\n",
      "Train Epoch: 11 [7040/11051 (64%)]\tLoss: 0.036131\n",
      "Train Epoch: 11 [7120/11051 (64%)]\tLoss: 0.016394\n",
      "Train Epoch: 11 [7200/11051 (65%)]\tLoss: 0.008341\n",
      "Train Epoch: 11 [7280/11051 (66%)]\tLoss: 0.029311\n",
      "Train Epoch: 11 [7360/11051 (67%)]\tLoss: 0.045333\n",
      "Train Epoch: 11 [7440/11051 (67%)]\tLoss: 0.017803\n",
      "Train Epoch: 11 [7520/11051 (68%)]\tLoss: 0.030061\n",
      "Train Epoch: 11 [7600/11051 (69%)]\tLoss: 0.016231\n",
      "Train Epoch: 11 [7680/11051 (69%)]\tLoss: 0.022011\n",
      "Train Epoch: 11 [7760/11051 (70%)]\tLoss: 0.021766\n",
      "Train Epoch: 11 [7840/11051 (71%)]\tLoss: 0.013240\n",
      "Train Epoch: 11 [7920/11051 (72%)]\tLoss: 0.011140\n",
      "Train Epoch: 11 [8000/11051 (72%)]\tLoss: 0.027453\n",
      "Train Epoch: 11 [8080/11051 (73%)]\tLoss: 0.012159\n",
      "Train Epoch: 11 [8160/11051 (74%)]\tLoss: 0.035393\n",
      "Train Epoch: 11 [8240/11051 (75%)]\tLoss: 0.014882\n",
      "Train Epoch: 11 [8320/11051 (75%)]\tLoss: 0.030856\n",
      "Train Epoch: 11 [8400/11051 (76%)]\tLoss: 0.032212\n",
      "Train Epoch: 11 [8480/11051 (77%)]\tLoss: 0.025951\n",
      "Train Epoch: 11 [8560/11051 (77%)]\tLoss: 0.008964\n",
      "Train Epoch: 11 [8640/11051 (78%)]\tLoss: 0.026373\n",
      "Train Epoch: 11 [8720/11051 (79%)]\tLoss: 0.019766\n",
      "Train Epoch: 11 [8800/11051 (80%)]\tLoss: 0.018695\n",
      "Train Epoch: 11 [8880/11051 (80%)]\tLoss: 0.044637\n",
      "Train Epoch: 11 [8960/11051 (81%)]\tLoss: 0.017519\n",
      "Train Epoch: 11 [9040/11051 (82%)]\tLoss: 0.020008\n",
      "Train Epoch: 11 [9120/11051 (82%)]\tLoss: 0.015894\n",
      "Train Epoch: 11 [9200/11051 (83%)]\tLoss: 0.023576\n",
      "Train Epoch: 11 [9280/11051 (84%)]\tLoss: 0.014486\n",
      "Train Epoch: 11 [9360/11051 (85%)]\tLoss: 0.038060\n",
      "Train Epoch: 11 [9440/11051 (85%)]\tLoss: 0.014745\n",
      "Train Epoch: 11 [9520/11051 (86%)]\tLoss: 0.049229\n",
      "Train Epoch: 11 [9600/11051 (87%)]\tLoss: 0.024072\n",
      "Train Epoch: 11 [9680/11051 (88%)]\tLoss: 0.034057\n",
      "Train Epoch: 11 [9760/11051 (88%)]\tLoss: 0.020648\n",
      "Train Epoch: 11 [9840/11051 (89%)]\tLoss: 0.060944\n",
      "Train Epoch: 11 [9920/11051 (90%)]\tLoss: 0.042708\n",
      "Train Epoch: 11 [10000/11051 (90%)]\tLoss: 0.019134\n",
      "Train Epoch: 11 [10080/11051 (91%)]\tLoss: 0.035991\n",
      "Train Epoch: 11 [10160/11051 (92%)]\tLoss: 0.013064\n",
      "Train Epoch: 11 [10240/11051 (93%)]\tLoss: 0.025914\n",
      "Train Epoch: 11 [10320/11051 (93%)]\tLoss: 0.021842\n",
      "Train Epoch: 11 [10400/11051 (94%)]\tLoss: 0.010030\n",
      "Train Epoch: 11 [10480/11051 (95%)]\tLoss: 0.032456\n",
      "Train Epoch: 11 [10560/11051 (96%)]\tLoss: 0.016560\n",
      "Train Epoch: 11 [10640/11051 (96%)]\tLoss: 0.017905\n",
      "Train Epoch: 11 [10720/11051 (97%)]\tLoss: 0.029375\n",
      "Train Epoch: 11 [10800/11051 (98%)]\tLoss: 0.029395\n",
      "Train Epoch: 11 [10880/11051 (98%)]\tLoss: 0.014291\n",
      "Train Epoch: 11 [10960/11051 (99%)]\tLoss: 0.044323\n",
      "Train Epoch: 11 [11040/11051 (100%)]\tLoss: 0.037976\n",
      "====> Epoch: 11 Average loss: 0.0228\n",
      "\n",
      "Started training epoch no. 13\n",
      "Train Epoch: 12 [0/11051 (0%)]\tLoss: 0.020213\n",
      "Train Epoch: 12 [80/11051 (1%)]\tLoss: 0.016072\n",
      "Train Epoch: 12 [160/11051 (1%)]\tLoss: 0.031116\n",
      "Train Epoch: 12 [240/11051 (2%)]\tLoss: 0.011611\n",
      "Train Epoch: 12 [320/11051 (3%)]\tLoss: 0.014170\n",
      "Train Epoch: 12 [400/11051 (4%)]\tLoss: 0.029966\n",
      "Train Epoch: 12 [480/11051 (4%)]\tLoss: 0.026943\n",
      "Train Epoch: 12 [560/11051 (5%)]\tLoss: 0.024127\n",
      "Train Epoch: 12 [640/11051 (6%)]\tLoss: 0.022695\n",
      "Train Epoch: 12 [720/11051 (7%)]\tLoss: 0.014062\n",
      "Train Epoch: 12 [800/11051 (7%)]\tLoss: 0.021466\n",
      "Train Epoch: 12 [880/11051 (8%)]\tLoss: 0.039279\n",
      "Train Epoch: 12 [960/11051 (9%)]\tLoss: 0.026140\n",
      "Train Epoch: 12 [1040/11051 (9%)]\tLoss: 0.016712\n",
      "Train Epoch: 12 [1120/11051 (10%)]\tLoss: 0.031120\n",
      "Train Epoch: 12 [1200/11051 (11%)]\tLoss: 0.017922\n",
      "Train Epoch: 12 [1280/11051 (12%)]\tLoss: 0.010039\n",
      "Train Epoch: 12 [1360/11051 (12%)]\tLoss: 0.028733\n",
      "Train Epoch: 12 [1440/11051 (13%)]\tLoss: 0.007920\n",
      "Train Epoch: 12 [1520/11051 (14%)]\tLoss: 0.020125\n",
      "Train Epoch: 12 [1600/11051 (14%)]\tLoss: 0.020405\n",
      "Train Epoch: 12 [1680/11051 (15%)]\tLoss: 0.025984\n",
      "Train Epoch: 12 [1760/11051 (16%)]\tLoss: 0.032420\n",
      "Train Epoch: 12 [1840/11051 (17%)]\tLoss: 0.014705\n",
      "Train Epoch: 12 [1920/11051 (17%)]\tLoss: 0.009243\n",
      "Train Epoch: 12 [2000/11051 (18%)]\tLoss: 0.024007\n",
      "Train Epoch: 12 [2080/11051 (19%)]\tLoss: 0.020208\n",
      "Train Epoch: 12 [2160/11051 (20%)]\tLoss: 0.029264\n",
      "Train Epoch: 12 [2240/11051 (20%)]\tLoss: 0.013170\n",
      "Train Epoch: 12 [2320/11051 (21%)]\tLoss: 0.023284\n",
      "Train Epoch: 12 [2400/11051 (22%)]\tLoss: 0.008776\n",
      "Train Epoch: 12 [2480/11051 (22%)]\tLoss: 0.009221\n",
      "Train Epoch: 12 [2560/11051 (23%)]\tLoss: 0.014539\n",
      "Train Epoch: 12 [2640/11051 (24%)]\tLoss: 0.022653\n",
      "Train Epoch: 12 [2720/11051 (25%)]\tLoss: 0.027289\n",
      "Train Epoch: 12 [2800/11051 (25%)]\tLoss: 0.019277\n",
      "Train Epoch: 12 [2880/11051 (26%)]\tLoss: 0.015300\n",
      "Train Epoch: 12 [2960/11051 (27%)]\tLoss: 0.018709\n",
      "Train Epoch: 12 [3040/11051 (27%)]\tLoss: 0.036835\n",
      "Train Epoch: 12 [3120/11051 (28%)]\tLoss: 0.031083\n",
      "Train Epoch: 12 [3200/11051 (29%)]\tLoss: 0.023524\n",
      "Train Epoch: 12 [3280/11051 (30%)]\tLoss: 0.018605\n",
      "Train Epoch: 12 [3360/11051 (30%)]\tLoss: 0.018254\n",
      "Train Epoch: 12 [3440/11051 (31%)]\tLoss: 0.016751\n",
      "Train Epoch: 12 [3520/11051 (32%)]\tLoss: 0.014817\n",
      "Train Epoch: 12 [3600/11051 (33%)]\tLoss: 0.024124\n",
      "Train Epoch: 12 [3680/11051 (33%)]\tLoss: 0.031055\n",
      "Train Epoch: 12 [3760/11051 (34%)]\tLoss: 0.025526\n",
      "Train Epoch: 12 [3840/11051 (35%)]\tLoss: 0.019301\n",
      "Train Epoch: 12 [3920/11051 (35%)]\tLoss: 0.014792\n",
      "Train Epoch: 12 [4000/11051 (36%)]\tLoss: 0.024559\n",
      "Train Epoch: 12 [4080/11051 (37%)]\tLoss: 0.018450\n",
      "Train Epoch: 12 [4160/11051 (38%)]\tLoss: 0.021404\n",
      "Train Epoch: 12 [4240/11051 (38%)]\tLoss: 0.026516\n",
      "Train Epoch: 12 [4320/11051 (39%)]\tLoss: 0.009648\n",
      "Train Epoch: 12 [4400/11051 (40%)]\tLoss: 0.027793\n",
      "Train Epoch: 12 [4480/11051 (41%)]\tLoss: 0.022736\n",
      "Train Epoch: 12 [4560/11051 (41%)]\tLoss: 0.030366\n",
      "Train Epoch: 12 [4640/11051 (42%)]\tLoss: 0.022061\n",
      "Train Epoch: 12 [4720/11051 (43%)]\tLoss: 0.008697\n",
      "Train Epoch: 12 [4800/11051 (43%)]\tLoss: 0.033844\n",
      "Train Epoch: 12 [4880/11051 (44%)]\tLoss: 0.013302\n",
      "Train Epoch: 12 [4960/11051 (45%)]\tLoss: 0.029712\n",
      "Train Epoch: 12 [5040/11051 (46%)]\tLoss: 0.023178\n",
      "Train Epoch: 12 [5120/11051 (46%)]\tLoss: 0.029815\n",
      "Train Epoch: 12 [5200/11051 (47%)]\tLoss: 0.019850\n",
      "Train Epoch: 12 [5280/11051 (48%)]\tLoss: 0.026286\n",
      "Train Epoch: 12 [5360/11051 (48%)]\tLoss: 0.032542\n",
      "Train Epoch: 12 [5440/11051 (49%)]\tLoss: 0.016575\n",
      "Train Epoch: 12 [5520/11051 (50%)]\tLoss: 0.035409\n",
      "Train Epoch: 12 [5600/11051 (51%)]\tLoss: 0.029599\n",
      "Train Epoch: 12 [5680/11051 (51%)]\tLoss: 0.013161\n",
      "Train Epoch: 12 [5760/11051 (52%)]\tLoss: 0.015063\n",
      "Train Epoch: 12 [5840/11051 (53%)]\tLoss: 0.022166\n",
      "Train Epoch: 12 [5920/11051 (54%)]\tLoss: 0.032236\n",
      "Train Epoch: 12 [6000/11051 (54%)]\tLoss: 0.025108\n",
      "Train Epoch: 12 [6080/11051 (55%)]\tLoss: 0.018579\n",
      "Train Epoch: 12 [6160/11051 (56%)]\tLoss: 0.026958\n",
      "Train Epoch: 12 [6240/11051 (56%)]\tLoss: 0.020473\n",
      "Train Epoch: 12 [6320/11051 (57%)]\tLoss: 0.022606\n",
      "Train Epoch: 12 [6400/11051 (58%)]\tLoss: 0.027791\n",
      "Train Epoch: 12 [6480/11051 (59%)]\tLoss: 0.037949\n",
      "Train Epoch: 12 [6560/11051 (59%)]\tLoss: 0.017681\n",
      "Train Epoch: 12 [6640/11051 (60%)]\tLoss: 0.017892\n",
      "Train Epoch: 12 [6720/11051 (61%)]\tLoss: 0.016964\n",
      "Train Epoch: 12 [6800/11051 (62%)]\tLoss: 0.007099\n",
      "Train Epoch: 12 [6880/11051 (62%)]\tLoss: 0.014232\n",
      "Train Epoch: 12 [6960/11051 (63%)]\tLoss: 0.018020\n",
      "Train Epoch: 12 [7040/11051 (64%)]\tLoss: 0.025423\n",
      "Train Epoch: 12 [7120/11051 (64%)]\tLoss: 0.018058\n",
      "Train Epoch: 12 [7200/11051 (65%)]\tLoss: 0.016941\n",
      "Train Epoch: 12 [7280/11051 (66%)]\tLoss: 0.035549\n",
      "Train Epoch: 12 [7360/11051 (67%)]\tLoss: 0.022182\n",
      "Train Epoch: 12 [7440/11051 (67%)]\tLoss: 0.029272\n",
      "Train Epoch: 12 [7520/11051 (68%)]\tLoss: 0.024746\n",
      "Train Epoch: 12 [7600/11051 (69%)]\tLoss: 0.028785\n",
      "Train Epoch: 12 [7680/11051 (69%)]\tLoss: 0.020031\n",
      "Train Epoch: 12 [7760/11051 (70%)]\tLoss: 0.031315\n",
      "Train Epoch: 12 [7840/11051 (71%)]\tLoss: 0.032066\n",
      "Train Epoch: 12 [7920/11051 (72%)]\tLoss: 0.026592\n",
      "Train Epoch: 12 [8000/11051 (72%)]\tLoss: 0.038318\n",
      "Train Epoch: 12 [8080/11051 (73%)]\tLoss: 0.017169\n",
      "Train Epoch: 12 [8160/11051 (74%)]\tLoss: 0.024804\n",
      "Train Epoch: 12 [8240/11051 (75%)]\tLoss: 0.013455\n",
      "Train Epoch: 12 [8320/11051 (75%)]\tLoss: 0.022491\n",
      "Train Epoch: 12 [8400/11051 (76%)]\tLoss: 0.017748\n",
      "Train Epoch: 12 [8480/11051 (77%)]\tLoss: 0.030135\n",
      "Train Epoch: 12 [8560/11051 (77%)]\tLoss: 0.037690\n",
      "Train Epoch: 12 [8640/11051 (78%)]\tLoss: 0.016034\n",
      "Train Epoch: 12 [8720/11051 (79%)]\tLoss: 0.020657\n",
      "Train Epoch: 12 [8800/11051 (80%)]\tLoss: 0.018440\n",
      "Train Epoch: 12 [8880/11051 (80%)]\tLoss: 0.027689\n",
      "Train Epoch: 12 [8960/11051 (81%)]\tLoss: 0.014546\n",
      "Train Epoch: 12 [9040/11051 (82%)]\tLoss: 0.020694\n",
      "Train Epoch: 12 [9120/11051 (82%)]\tLoss: 0.033549\n",
      "Train Epoch: 12 [9200/11051 (83%)]\tLoss: 0.014834\n",
      "Train Epoch: 12 [9280/11051 (84%)]\tLoss: 0.019921\n",
      "Train Epoch: 12 [9360/11051 (85%)]\tLoss: 0.010570\n",
      "Train Epoch: 12 [9440/11051 (85%)]\tLoss: 0.016556\n",
      "Train Epoch: 12 [9520/11051 (86%)]\tLoss: 0.048972\n",
      "Train Epoch: 12 [9600/11051 (87%)]\tLoss: 0.014577\n",
      "Train Epoch: 12 [9680/11051 (88%)]\tLoss: 0.038766\n",
      "Train Epoch: 12 [9760/11051 (88%)]\tLoss: 0.014543\n",
      "Train Epoch: 12 [9840/11051 (89%)]\tLoss: 0.032534\n",
      "Train Epoch: 12 [9920/11051 (90%)]\tLoss: 0.034621\n",
      "Train Epoch: 12 [10000/11051 (90%)]\tLoss: 0.022155\n",
      "Train Epoch: 12 [10080/11051 (91%)]\tLoss: 0.038140\n",
      "Train Epoch: 12 [10160/11051 (92%)]\tLoss: 0.012428\n",
      "Train Epoch: 12 [10240/11051 (93%)]\tLoss: 0.023687\n",
      "Train Epoch: 12 [10320/11051 (93%)]\tLoss: 0.018316\n",
      "Train Epoch: 12 [10400/11051 (94%)]\tLoss: 0.021542\n",
      "Train Epoch: 12 [10480/11051 (95%)]\tLoss: 0.027253\n",
      "Train Epoch: 12 [10560/11051 (96%)]\tLoss: 0.019640\n",
      "Train Epoch: 12 [10640/11051 (96%)]\tLoss: 0.022135\n",
      "Train Epoch: 12 [10720/11051 (97%)]\tLoss: 0.005698\n",
      "Train Epoch: 12 [10800/11051 (98%)]\tLoss: 0.022382\n",
      "Train Epoch: 12 [10880/11051 (98%)]\tLoss: 0.023672\n",
      "Train Epoch: 12 [10960/11051 (99%)]\tLoss: 0.012161\n",
      "Train Epoch: 12 [11040/11051 (100%)]\tLoss: 0.018180\n",
      "====> Epoch: 12 Average loss: 0.0226\n",
      "\n",
      "Started training epoch no. 14\n",
      "Train Epoch: 13 [0/11051 (0%)]\tLoss: 0.041882\n",
      "Train Epoch: 13 [80/11051 (1%)]\tLoss: 0.028186\n",
      "Train Epoch: 13 [160/11051 (1%)]\tLoss: 0.017639\n",
      "Train Epoch: 13 [240/11051 (2%)]\tLoss: 0.018994\n",
      "Train Epoch: 13 [320/11051 (3%)]\tLoss: 0.020620\n",
      "Train Epoch: 13 [400/11051 (4%)]\tLoss: 0.033461\n",
      "Train Epoch: 13 [480/11051 (4%)]\tLoss: 0.015462\n",
      "Train Epoch: 13 [560/11051 (5%)]\tLoss: 0.028832\n",
      "Train Epoch: 13 [640/11051 (6%)]\tLoss: 0.036158\n",
      "Train Epoch: 13 [720/11051 (7%)]\tLoss: 0.029998\n",
      "Train Epoch: 13 [800/11051 (7%)]\tLoss: 0.016444\n",
      "Train Epoch: 13 [880/11051 (8%)]\tLoss: 0.011770\n",
      "Train Epoch: 13 [960/11051 (9%)]\tLoss: 0.015685\n",
      "Train Epoch: 13 [1040/11051 (9%)]\tLoss: 0.044859\n",
      "Train Epoch: 13 [1120/11051 (10%)]\tLoss: 0.018394\n",
      "Train Epoch: 13 [1200/11051 (11%)]\tLoss: 0.014438\n",
      "Train Epoch: 13 [1280/11051 (12%)]\tLoss: 0.057918\n",
      "Train Epoch: 13 [1360/11051 (12%)]\tLoss: 0.023664\n",
      "Train Epoch: 13 [1440/11051 (13%)]\tLoss: 0.020520\n",
      "Train Epoch: 13 [1520/11051 (14%)]\tLoss: 0.021124\n",
      "Train Epoch: 13 [1600/11051 (14%)]\tLoss: 0.017341\n",
      "Train Epoch: 13 [1680/11051 (15%)]\tLoss: 0.016667\n",
      "Train Epoch: 13 [1760/11051 (16%)]\tLoss: 0.021211\n",
      "Train Epoch: 13 [1840/11051 (17%)]\tLoss: 0.014556\n",
      "Train Epoch: 13 [1920/11051 (17%)]\tLoss: 0.019633\n",
      "Train Epoch: 13 [2000/11051 (18%)]\tLoss: 0.016234\n",
      "Train Epoch: 13 [2080/11051 (19%)]\tLoss: 0.023363\n",
      "Train Epoch: 13 [2160/11051 (20%)]\tLoss: 0.042127\n",
      "Train Epoch: 13 [2240/11051 (20%)]\tLoss: 0.018526\n",
      "Train Epoch: 13 [2320/11051 (21%)]\tLoss: 0.027159\n",
      "Train Epoch: 13 [2400/11051 (22%)]\tLoss: 0.013793\n",
      "Train Epoch: 13 [2480/11051 (22%)]\tLoss: 0.027276\n",
      "Train Epoch: 13 [2560/11051 (23%)]\tLoss: 0.035899\n",
      "Train Epoch: 13 [2640/11051 (24%)]\tLoss: 0.039456\n",
      "Train Epoch: 13 [2720/11051 (25%)]\tLoss: 0.048455\n",
      "Train Epoch: 13 [2800/11051 (25%)]\tLoss: 0.025113\n",
      "Train Epoch: 13 [2880/11051 (26%)]\tLoss: 0.017043\n",
      "Train Epoch: 13 [2960/11051 (27%)]\tLoss: 0.016711\n",
      "Train Epoch: 13 [3040/11051 (27%)]\tLoss: 0.017562\n",
      "Train Epoch: 13 [3120/11051 (28%)]\tLoss: 0.026094\n",
      "Train Epoch: 13 [3200/11051 (29%)]\tLoss: 0.011873\n",
      "Train Epoch: 13 [3280/11051 (30%)]\tLoss: 0.024165\n",
      "Train Epoch: 13 [3360/11051 (30%)]\tLoss: 0.017381\n",
      "Train Epoch: 13 [3440/11051 (31%)]\tLoss: 0.018152\n",
      "Train Epoch: 13 [3520/11051 (32%)]\tLoss: 0.028329\n",
      "Train Epoch: 13 [3600/11051 (33%)]\tLoss: 0.009099\n",
      "Train Epoch: 13 [3680/11051 (33%)]\tLoss: 0.033187\n",
      "Train Epoch: 13 [3760/11051 (34%)]\tLoss: 0.013631\n",
      "Train Epoch: 13 [3840/11051 (35%)]\tLoss: 0.026406\n",
      "Train Epoch: 13 [3920/11051 (35%)]\tLoss: 0.014011\n",
      "Train Epoch: 13 [4000/11051 (36%)]\tLoss: 0.025603\n",
      "Train Epoch: 13 [4080/11051 (37%)]\tLoss: 0.016630\n",
      "Train Epoch: 13 [4160/11051 (38%)]\tLoss: 0.028402\n",
      "Train Epoch: 13 [4240/11051 (38%)]\tLoss: 0.015791\n",
      "Train Epoch: 13 [4320/11051 (39%)]\tLoss: 0.019146\n",
      "Train Epoch: 13 [4400/11051 (40%)]\tLoss: 0.008943\n",
      "Train Epoch: 13 [4480/11051 (41%)]\tLoss: 0.059899\n",
      "Train Epoch: 13 [4560/11051 (41%)]\tLoss: 0.013043\n",
      "Train Epoch: 13 [4640/11051 (42%)]\tLoss: 0.016366\n",
      "Train Epoch: 13 [4720/11051 (43%)]\tLoss: 0.026314\n",
      "Train Epoch: 13 [4800/11051 (43%)]\tLoss: 0.013274\n",
      "Train Epoch: 13 [4880/11051 (44%)]\tLoss: 0.012844\n",
      "Train Epoch: 13 [4960/11051 (45%)]\tLoss: 0.032892\n",
      "Train Epoch: 13 [5040/11051 (46%)]\tLoss: 0.018690\n",
      "Train Epoch: 13 [5120/11051 (46%)]\tLoss: 0.021098\n",
      "Train Epoch: 13 [5200/11051 (47%)]\tLoss: 0.029952\n",
      "Train Epoch: 13 [5280/11051 (48%)]\tLoss: 0.021304\n",
      "Train Epoch: 13 [5360/11051 (48%)]\tLoss: 0.017436\n",
      "Train Epoch: 13 [5440/11051 (49%)]\tLoss: 0.007686\n",
      "Train Epoch: 13 [5520/11051 (50%)]\tLoss: 0.018783\n",
      "Train Epoch: 13 [5600/11051 (51%)]\tLoss: 0.019686\n",
      "Train Epoch: 13 [5680/11051 (51%)]\tLoss: 0.040203\n",
      "Train Epoch: 13 [5760/11051 (52%)]\tLoss: 0.023266\n",
      "Train Epoch: 13 [5840/11051 (53%)]\tLoss: 0.014940\n",
      "Train Epoch: 13 [5920/11051 (54%)]\tLoss: 0.026901\n",
      "Train Epoch: 13 [6000/11051 (54%)]\tLoss: 0.028228\n",
      "Train Epoch: 13 [6080/11051 (55%)]\tLoss: 0.020344\n",
      "Train Epoch: 13 [6160/11051 (56%)]\tLoss: 0.010110\n",
      "Train Epoch: 13 [6240/11051 (56%)]\tLoss: 0.017421\n",
      "Train Epoch: 13 [6320/11051 (57%)]\tLoss: 0.014915\n",
      "Train Epoch: 13 [6400/11051 (58%)]\tLoss: 0.026770\n",
      "Train Epoch: 13 [6480/11051 (59%)]\tLoss: 0.028619\n",
      "Train Epoch: 13 [6560/11051 (59%)]\tLoss: 0.019549\n",
      "Train Epoch: 13 [6640/11051 (60%)]\tLoss: 0.021667\n",
      "Train Epoch: 13 [6720/11051 (61%)]\tLoss: 0.016036\n",
      "Train Epoch: 13 [6800/11051 (62%)]\tLoss: 0.014774\n",
      "Train Epoch: 13 [6880/11051 (62%)]\tLoss: 0.010750\n",
      "Train Epoch: 13 [6960/11051 (63%)]\tLoss: 0.007865\n",
      "Train Epoch: 13 [7040/11051 (64%)]\tLoss: 0.010955\n",
      "Train Epoch: 13 [7120/11051 (64%)]\tLoss: 0.015142\n",
      "Train Epoch: 13 [7200/11051 (65%)]\tLoss: 0.030949\n",
      "Train Epoch: 13 [7280/11051 (66%)]\tLoss: 0.068117\n",
      "Train Epoch: 13 [7360/11051 (67%)]\tLoss: 0.035572\n",
      "Train Epoch: 13 [7440/11051 (67%)]\tLoss: 0.022731\n",
      "Train Epoch: 13 [7520/11051 (68%)]\tLoss: 0.016267\n",
      "Train Epoch: 13 [7600/11051 (69%)]\tLoss: 0.015891\n",
      "Train Epoch: 13 [7680/11051 (69%)]\tLoss: 0.027624\n",
      "Train Epoch: 13 [7760/11051 (70%)]\tLoss: 0.012863\n",
      "Train Epoch: 13 [7840/11051 (71%)]\tLoss: 0.026159\n",
      "Train Epoch: 13 [7920/11051 (72%)]\tLoss: 0.029665\n",
      "Train Epoch: 13 [8000/11051 (72%)]\tLoss: 0.017911\n",
      "Train Epoch: 13 [8080/11051 (73%)]\tLoss: 0.024642\n",
      "Train Epoch: 13 [8160/11051 (74%)]\tLoss: 0.010464\n",
      "Train Epoch: 13 [8240/11051 (75%)]\tLoss: 0.019448\n",
      "Train Epoch: 13 [8320/11051 (75%)]\tLoss: 0.024295\n",
      "Train Epoch: 13 [8400/11051 (76%)]\tLoss: 0.011133\n",
      "Train Epoch: 13 [8480/11051 (77%)]\tLoss: 0.073210\n",
      "Train Epoch: 13 [8560/11051 (77%)]\tLoss: 0.023795\n",
      "Train Epoch: 13 [8640/11051 (78%)]\tLoss: 0.040147\n",
      "Train Epoch: 13 [8720/11051 (79%)]\tLoss: 0.021564\n",
      "Train Epoch: 13 [8800/11051 (80%)]\tLoss: 0.031880\n",
      "Train Epoch: 13 [8880/11051 (80%)]\tLoss: 0.024999\n",
      "Train Epoch: 13 [8960/11051 (81%)]\tLoss: 0.021192\n",
      "Train Epoch: 13 [9040/11051 (82%)]\tLoss: 0.016946\n",
      "Train Epoch: 13 [9120/11051 (82%)]\tLoss: 0.022168\n",
      "Train Epoch: 13 [9200/11051 (83%)]\tLoss: 0.020618\n",
      "Train Epoch: 13 [9280/11051 (84%)]\tLoss: 0.011198\n",
      "Train Epoch: 13 [9360/11051 (85%)]\tLoss: 0.034092\n",
      "Train Epoch: 13 [9440/11051 (85%)]\tLoss: 0.042188\n",
      "Train Epoch: 13 [9520/11051 (86%)]\tLoss: 0.017670\n",
      "Train Epoch: 13 [9600/11051 (87%)]\tLoss: 0.023351\n",
      "Train Epoch: 13 [9680/11051 (88%)]\tLoss: 0.016641\n",
      "Train Epoch: 13 [9760/11051 (88%)]\tLoss: 0.019999\n",
      "Train Epoch: 13 [9840/11051 (89%)]\tLoss: 0.020166\n",
      "Train Epoch: 13 [9920/11051 (90%)]\tLoss: 0.011961\n",
      "Train Epoch: 13 [10000/11051 (90%)]\tLoss: 0.016975\n",
      "Train Epoch: 13 [10080/11051 (91%)]\tLoss: 0.023150\n",
      "Train Epoch: 13 [10160/11051 (92%)]\tLoss: 0.024131\n",
      "Train Epoch: 13 [10240/11051 (93%)]\tLoss: 0.017944\n",
      "Train Epoch: 13 [10320/11051 (93%)]\tLoss: 0.034229\n",
      "Train Epoch: 13 [10400/11051 (94%)]\tLoss: 0.018516\n",
      "Train Epoch: 13 [10480/11051 (95%)]\tLoss: 0.013998\n",
      "Train Epoch: 13 [10560/11051 (96%)]\tLoss: 0.025511\n",
      "Train Epoch: 13 [10640/11051 (96%)]\tLoss: 0.018678\n",
      "Train Epoch: 13 [10720/11051 (97%)]\tLoss: 0.025285\n",
      "Train Epoch: 13 [10800/11051 (98%)]\tLoss: 0.016045\n",
      "Train Epoch: 13 [10880/11051 (98%)]\tLoss: 0.027993\n",
      "Train Epoch: 13 [10960/11051 (99%)]\tLoss: 0.043382\n",
      "Train Epoch: 13 [11040/11051 (100%)]\tLoss: 0.022561\n",
      "====> Epoch: 13 Average loss: 0.0225\n",
      "\n",
      "Started training epoch no. 15\n",
      "Train Epoch: 14 [0/11051 (0%)]\tLoss: 0.018969\n",
      "Train Epoch: 14 [80/11051 (1%)]\tLoss: 0.041114\n",
      "Train Epoch: 14 [160/11051 (1%)]\tLoss: 0.024090\n",
      "Train Epoch: 14 [240/11051 (2%)]\tLoss: 0.021937\n",
      "Train Epoch: 14 [320/11051 (3%)]\tLoss: 0.023678\n",
      "Train Epoch: 14 [400/11051 (4%)]\tLoss: 0.014079\n",
      "Train Epoch: 14 [480/11051 (4%)]\tLoss: 0.010500\n",
      "Train Epoch: 14 [560/11051 (5%)]\tLoss: 0.020860\n",
      "Train Epoch: 14 [640/11051 (6%)]\tLoss: 0.021088\n",
      "Train Epoch: 14 [720/11051 (7%)]\tLoss: 0.043686\n",
      "Train Epoch: 14 [800/11051 (7%)]\tLoss: 0.016610\n",
      "Train Epoch: 14 [880/11051 (8%)]\tLoss: 0.025267\n",
      "Train Epoch: 14 [960/11051 (9%)]\tLoss: 0.045004\n",
      "Train Epoch: 14 [1040/11051 (9%)]\tLoss: 0.023916\n",
      "Train Epoch: 14 [1120/11051 (10%)]\tLoss: 0.028017\n",
      "Train Epoch: 14 [1200/11051 (11%)]\tLoss: 0.032210\n",
      "Train Epoch: 14 [1280/11051 (12%)]\tLoss: 0.021946\n",
      "Train Epoch: 14 [1360/11051 (12%)]\tLoss: 0.019982\n",
      "Train Epoch: 14 [1440/11051 (13%)]\tLoss: 0.008711\n",
      "Train Epoch: 14 [1520/11051 (14%)]\tLoss: 0.016288\n",
      "Train Epoch: 14 [1600/11051 (14%)]\tLoss: 0.036412\n",
      "Train Epoch: 14 [1680/11051 (15%)]\tLoss: 0.014881\n",
      "Train Epoch: 14 [1760/11051 (16%)]\tLoss: 0.020816\n",
      "Train Epoch: 14 [1840/11051 (17%)]\tLoss: 0.017979\n",
      "Train Epoch: 14 [1920/11051 (17%)]\tLoss: 0.032392\n",
      "Train Epoch: 14 [2000/11051 (18%)]\tLoss: 0.007375\n",
      "Train Epoch: 14 [2080/11051 (19%)]\tLoss: 0.017687\n",
      "Train Epoch: 14 [2160/11051 (20%)]\tLoss: 0.019167\n",
      "Train Epoch: 14 [2240/11051 (20%)]\tLoss: 0.015889\n",
      "Train Epoch: 14 [2320/11051 (21%)]\tLoss: 0.021301\n",
      "Train Epoch: 14 [2400/11051 (22%)]\tLoss: 0.020437\n",
      "Train Epoch: 14 [2480/11051 (22%)]\tLoss: 0.023714\n",
      "Train Epoch: 14 [2560/11051 (23%)]\tLoss: 0.032218\n",
      "Train Epoch: 14 [2640/11051 (24%)]\tLoss: 0.026410\n",
      "Train Epoch: 14 [2720/11051 (25%)]\tLoss: 0.019720\n",
      "Train Epoch: 14 [2800/11051 (25%)]\tLoss: 0.014569\n",
      "Train Epoch: 14 [2880/11051 (26%)]\tLoss: 0.017154\n",
      "Train Epoch: 14 [2960/11051 (27%)]\tLoss: 0.016167\n",
      "Train Epoch: 14 [3040/11051 (27%)]\tLoss: 0.023781\n",
      "Train Epoch: 14 [3120/11051 (28%)]\tLoss: 0.008963\n",
      "Train Epoch: 14 [3200/11051 (29%)]\tLoss: 0.019815\n",
      "Train Epoch: 14 [3280/11051 (30%)]\tLoss: 0.026558\n",
      "Train Epoch: 14 [3360/11051 (30%)]\tLoss: 0.024399\n",
      "Train Epoch: 14 [3440/11051 (31%)]\tLoss: 0.039289\n",
      "Train Epoch: 14 [3520/11051 (32%)]\tLoss: 0.016479\n",
      "Train Epoch: 14 [3600/11051 (33%)]\tLoss: 0.022571\n",
      "Train Epoch: 14 [3680/11051 (33%)]\tLoss: 0.017438\n",
      "Train Epoch: 14 [3760/11051 (34%)]\tLoss: 0.009449\n",
      "Train Epoch: 14 [3840/11051 (35%)]\tLoss: 0.027798\n",
      "Train Epoch: 14 [3920/11051 (35%)]\tLoss: 0.024288\n",
      "Train Epoch: 14 [4000/11051 (36%)]\tLoss: 0.014520\n",
      "Train Epoch: 14 [4080/11051 (37%)]\tLoss: 0.023130\n",
      "Train Epoch: 14 [4160/11051 (38%)]\tLoss: 0.024650\n",
      "Train Epoch: 14 [4240/11051 (38%)]\tLoss: 0.038239\n",
      "Train Epoch: 14 [4320/11051 (39%)]\tLoss: 0.023730\n",
      "Train Epoch: 14 [4400/11051 (40%)]\tLoss: 0.016087\n",
      "Train Epoch: 14 [4480/11051 (41%)]\tLoss: 0.008025\n",
      "Train Epoch: 14 [4560/11051 (41%)]\tLoss: 0.018973\n",
      "Train Epoch: 14 [4640/11051 (42%)]\tLoss: 0.025133\n",
      "Train Epoch: 14 [4720/11051 (43%)]\tLoss: 0.012685\n",
      "Train Epoch: 14 [4800/11051 (43%)]\tLoss: 0.013135\n",
      "Train Epoch: 14 [4880/11051 (44%)]\tLoss: 0.021832\n",
      "Train Epoch: 14 [4960/11051 (45%)]\tLoss: 0.074919\n",
      "Train Epoch: 14 [5040/11051 (46%)]\tLoss: 0.010584\n",
      "Train Epoch: 14 [5120/11051 (46%)]\tLoss: 0.014638\n",
      "Train Epoch: 14 [5200/11051 (47%)]\tLoss: 0.026697\n",
      "Train Epoch: 14 [5280/11051 (48%)]\tLoss: 0.024738\n",
      "Train Epoch: 14 [5360/11051 (48%)]\tLoss: 0.029227\n",
      "Train Epoch: 14 [5440/11051 (49%)]\tLoss: 0.025321\n",
      "Train Epoch: 14 [5520/11051 (50%)]\tLoss: 0.016181\n",
      "Train Epoch: 14 [5600/11051 (51%)]\tLoss: 0.022833\n",
      "Train Epoch: 14 [5680/11051 (51%)]\tLoss: 0.027741\n",
      "Train Epoch: 14 [5760/11051 (52%)]\tLoss: 0.014417\n",
      "Train Epoch: 14 [5840/11051 (53%)]\tLoss: 0.011249\n",
      "Train Epoch: 14 [5920/11051 (54%)]\tLoss: 0.030316\n",
      "Train Epoch: 14 [6000/11051 (54%)]\tLoss: 0.024987\n",
      "Train Epoch: 14 [6080/11051 (55%)]\tLoss: 0.024454\n",
      "Train Epoch: 14 [6160/11051 (56%)]\tLoss: 0.028654\n",
      "Train Epoch: 14 [6240/11051 (56%)]\tLoss: 0.013769\n",
      "Train Epoch: 14 [6320/11051 (57%)]\tLoss: 0.020842\n",
      "Train Epoch: 14 [6400/11051 (58%)]\tLoss: 0.009308\n",
      "Train Epoch: 14 [6480/11051 (59%)]\tLoss: 0.017647\n",
      "Train Epoch: 14 [6560/11051 (59%)]\tLoss: 0.011992\n",
      "Train Epoch: 14 [6640/11051 (60%)]\tLoss: 0.015659\n",
      "Train Epoch: 14 [6720/11051 (61%)]\tLoss: 0.018092\n",
      "Train Epoch: 14 [6800/11051 (62%)]\tLoss: 0.029928\n",
      "Train Epoch: 14 [6880/11051 (62%)]\tLoss: 0.016574\n",
      "Train Epoch: 14 [6960/11051 (63%)]\tLoss: 0.019846\n",
      "Train Epoch: 14 [7040/11051 (64%)]\tLoss: 0.018525\n",
      "Train Epoch: 14 [7120/11051 (64%)]\tLoss: 0.013526\n",
      "Train Epoch: 14 [7200/11051 (65%)]\tLoss: 0.030150\n",
      "Train Epoch: 14 [7280/11051 (66%)]\tLoss: 0.030800\n",
      "Train Epoch: 14 [7360/11051 (67%)]\tLoss: 0.030527\n",
      "Train Epoch: 14 [7440/11051 (67%)]\tLoss: 0.021773\n",
      "Train Epoch: 14 [7520/11051 (68%)]\tLoss: 0.010028\n",
      "Train Epoch: 14 [7600/11051 (69%)]\tLoss: 0.021997\n",
      "Train Epoch: 14 [7680/11051 (69%)]\tLoss: 0.015075\n",
      "Train Epoch: 14 [7760/11051 (70%)]\tLoss: 0.018066\n",
      "Train Epoch: 14 [7840/11051 (71%)]\tLoss: 0.036051\n",
      "Train Epoch: 14 [7920/11051 (72%)]\tLoss: 0.019549\n",
      "Train Epoch: 14 [8000/11051 (72%)]\tLoss: 0.050848\n",
      "Train Epoch: 14 [8080/11051 (73%)]\tLoss: 0.016237\n",
      "Train Epoch: 14 [8160/11051 (74%)]\tLoss: 0.018202\n",
      "Train Epoch: 14 [8240/11051 (75%)]\tLoss: 0.027551\n",
      "Train Epoch: 14 [8320/11051 (75%)]\tLoss: 0.023401\n",
      "Train Epoch: 14 [8400/11051 (76%)]\tLoss: 0.018731\n",
      "Train Epoch: 14 [8480/11051 (77%)]\tLoss: 0.012789\n",
      "Train Epoch: 14 [8560/11051 (77%)]\tLoss: 0.015173\n",
      "Train Epoch: 14 [8640/11051 (78%)]\tLoss: 0.020164\n",
      "Train Epoch: 14 [8720/11051 (79%)]\tLoss: 0.020293\n",
      "Train Epoch: 14 [8800/11051 (80%)]\tLoss: 0.015681\n",
      "Train Epoch: 14 [8880/11051 (80%)]\tLoss: 0.019529\n",
      "Train Epoch: 14 [8960/11051 (81%)]\tLoss: 0.014022\n",
      "Train Epoch: 14 [9040/11051 (82%)]\tLoss: 0.009390\n",
      "Train Epoch: 14 [9120/11051 (82%)]\tLoss: 0.018210\n",
      "Train Epoch: 14 [9200/11051 (83%)]\tLoss: 0.022338\n",
      "Train Epoch: 14 [9280/11051 (84%)]\tLoss: 0.025404\n",
      "Train Epoch: 14 [9360/11051 (85%)]\tLoss: 0.016942\n",
      "Train Epoch: 14 [9440/11051 (85%)]\tLoss: 0.013770\n",
      "Train Epoch: 14 [9520/11051 (86%)]\tLoss: 0.035533\n",
      "Train Epoch: 14 [9600/11051 (87%)]\tLoss: 0.021490\n",
      "Train Epoch: 14 [9680/11051 (88%)]\tLoss: 0.010479\n",
      "Train Epoch: 14 [9760/11051 (88%)]\tLoss: 0.011875\n",
      "Train Epoch: 14 [9840/11051 (89%)]\tLoss: 0.013820\n",
      "Train Epoch: 14 [9920/11051 (90%)]\tLoss: 0.012876\n",
      "Train Epoch: 14 [10000/11051 (90%)]\tLoss: 0.008673\n",
      "Train Epoch: 14 [10080/11051 (91%)]\tLoss: 0.047250\n",
      "Train Epoch: 14 [10160/11051 (92%)]\tLoss: 0.041471\n",
      "Train Epoch: 14 [10240/11051 (93%)]\tLoss: 0.031758\n",
      "Train Epoch: 14 [10320/11051 (93%)]\tLoss: 0.029094\n",
      "Train Epoch: 14 [10400/11051 (94%)]\tLoss: 0.028916\n",
      "Train Epoch: 14 [10480/11051 (95%)]\tLoss: 0.072684\n",
      "Train Epoch: 14 [10560/11051 (96%)]\tLoss: 0.023508\n",
      "Train Epoch: 14 [10640/11051 (96%)]\tLoss: 0.029874\n",
      "Train Epoch: 14 [10720/11051 (97%)]\tLoss: 0.032046\n",
      "Train Epoch: 14 [10800/11051 (98%)]\tLoss: 0.058683\n",
      "Train Epoch: 14 [10880/11051 (98%)]\tLoss: 0.016369\n",
      "Train Epoch: 14 [10960/11051 (99%)]\tLoss: 0.026541\n",
      "Train Epoch: 14 [11040/11051 (100%)]\tLoss: 0.024014\n",
      "====> Epoch: 14 Average loss: 0.0224\n",
      "\n",
      "Started training epoch no. 16\n",
      "Train Epoch: 15 [0/11051 (0%)]\tLoss: 0.038360\n",
      "Train Epoch: 15 [80/11051 (1%)]\tLoss: 0.022285\n",
      "Train Epoch: 15 [160/11051 (1%)]\tLoss: 0.012336\n",
      "Train Epoch: 15 [240/11051 (2%)]\tLoss: 0.017428\n",
      "Train Epoch: 15 [320/11051 (3%)]\tLoss: 0.033128\n",
      "Train Epoch: 15 [400/11051 (4%)]\tLoss: 0.040032\n",
      "Train Epoch: 15 [480/11051 (4%)]\tLoss: 0.073847\n",
      "Train Epoch: 15 [560/11051 (5%)]\tLoss: 0.033528\n",
      "Train Epoch: 15 [640/11051 (6%)]\tLoss: 0.021300\n",
      "Train Epoch: 15 [720/11051 (7%)]\tLoss: 0.019248\n",
      "Train Epoch: 15 [800/11051 (7%)]\tLoss: 0.010448\n",
      "Train Epoch: 15 [880/11051 (8%)]\tLoss: 0.039058\n",
      "Train Epoch: 15 [960/11051 (9%)]\tLoss: 0.008849\n",
      "Train Epoch: 15 [1040/11051 (9%)]\tLoss: 0.014130\n",
      "Train Epoch: 15 [1120/11051 (10%)]\tLoss: 0.020317\n",
      "Train Epoch: 15 [1200/11051 (11%)]\tLoss: 0.025334\n",
      "Train Epoch: 15 [1280/11051 (12%)]\tLoss: 0.025641\n",
      "Train Epoch: 15 [1360/11051 (12%)]\tLoss: 0.021588\n",
      "Train Epoch: 15 [1440/11051 (13%)]\tLoss: 0.028966\n",
      "Train Epoch: 15 [1520/11051 (14%)]\tLoss: 0.012542\n",
      "Train Epoch: 15 [1600/11051 (14%)]\tLoss: 0.048315\n",
      "Train Epoch: 15 [1680/11051 (15%)]\tLoss: 0.021648\n",
      "Train Epoch: 15 [1760/11051 (16%)]\tLoss: 0.014640\n",
      "Train Epoch: 15 [1840/11051 (17%)]\tLoss: 0.039438\n",
      "Train Epoch: 15 [1920/11051 (17%)]\tLoss: 0.016718\n",
      "Train Epoch: 15 [2000/11051 (18%)]\tLoss: 0.004182\n",
      "Train Epoch: 15 [2080/11051 (19%)]\tLoss: 0.005525\n",
      "Train Epoch: 15 [2160/11051 (20%)]\tLoss: 0.028559\n",
      "Train Epoch: 15 [2240/11051 (20%)]\tLoss: 0.016157\n",
      "Train Epoch: 15 [2320/11051 (21%)]\tLoss: 0.013188\n",
      "Train Epoch: 15 [2400/11051 (22%)]\tLoss: 0.030700\n",
      "Train Epoch: 15 [2480/11051 (22%)]\tLoss: 0.008531\n",
      "Train Epoch: 15 [2560/11051 (23%)]\tLoss: 0.025445\n",
      "Train Epoch: 15 [2640/11051 (24%)]\tLoss: 0.014247\n",
      "Train Epoch: 15 [2720/11051 (25%)]\tLoss: 0.019872\n",
      "Train Epoch: 15 [2800/11051 (25%)]\tLoss: 0.014839\n",
      "Train Epoch: 15 [2880/11051 (26%)]\tLoss: 0.012095\n",
      "Train Epoch: 15 [2960/11051 (27%)]\tLoss: 0.028494\n",
      "Train Epoch: 15 [3040/11051 (27%)]\tLoss: 0.009114\n",
      "Train Epoch: 15 [3120/11051 (28%)]\tLoss: 0.023121\n",
      "Train Epoch: 15 [3200/11051 (29%)]\tLoss: 0.024165\n",
      "Train Epoch: 15 [3280/11051 (30%)]\tLoss: 0.009161\n",
      "Train Epoch: 15 [3360/11051 (30%)]\tLoss: 0.023857\n",
      "Train Epoch: 15 [3440/11051 (31%)]\tLoss: 0.016036\n",
      "Train Epoch: 15 [3520/11051 (32%)]\tLoss: 0.032159\n",
      "Train Epoch: 15 [3600/11051 (33%)]\tLoss: 0.017528\n",
      "Train Epoch: 15 [3680/11051 (33%)]\tLoss: 0.011194\n",
      "Train Epoch: 15 [3760/11051 (34%)]\tLoss: 0.024126\n",
      "Train Epoch: 15 [3840/11051 (35%)]\tLoss: 0.021585\n",
      "Train Epoch: 15 [3920/11051 (35%)]\tLoss: 0.023983\n",
      "Train Epoch: 15 [4000/11051 (36%)]\tLoss: 0.021389\n",
      "Train Epoch: 15 [4080/11051 (37%)]\tLoss: 0.027824\n",
      "Train Epoch: 15 [4160/11051 (38%)]\tLoss: 0.028154\n",
      "Train Epoch: 15 [4240/11051 (38%)]\tLoss: 0.022059\n",
      "Train Epoch: 15 [4320/11051 (39%)]\tLoss: 0.023809\n",
      "Train Epoch: 15 [4400/11051 (40%)]\tLoss: 0.025056\n",
      "Train Epoch: 15 [4480/11051 (41%)]\tLoss: 0.024894\n",
      "Train Epoch: 15 [4560/11051 (41%)]\tLoss: 0.017798\n",
      "Train Epoch: 15 [4640/11051 (42%)]\tLoss: 0.014251\n",
      "Train Epoch: 15 [4720/11051 (43%)]\tLoss: 0.017323\n",
      "Train Epoch: 15 [4800/11051 (43%)]\tLoss: 0.012472\n",
      "Train Epoch: 15 [4880/11051 (44%)]\tLoss: 0.022324\n",
      "Train Epoch: 15 [4960/11051 (45%)]\tLoss: 0.036081\n",
      "Train Epoch: 15 [5040/11051 (46%)]\tLoss: 0.021973\n",
      "Train Epoch: 15 [5120/11051 (46%)]\tLoss: 0.022048\n",
      "Train Epoch: 15 [5200/11051 (47%)]\tLoss: 0.031656\n",
      "Train Epoch: 15 [5280/11051 (48%)]\tLoss: 0.008389\n",
      "Train Epoch: 15 [5360/11051 (48%)]\tLoss: 0.012013\n",
      "Train Epoch: 15 [5440/11051 (49%)]\tLoss: 0.017741\n",
      "Train Epoch: 15 [5520/11051 (50%)]\tLoss: 0.014710\n",
      "Train Epoch: 15 [5600/11051 (51%)]\tLoss: 0.023500\n",
      "Train Epoch: 15 [5680/11051 (51%)]\tLoss: 0.012352\n",
      "Train Epoch: 15 [5760/11051 (52%)]\tLoss: 0.018663\n",
      "Train Epoch: 15 [5840/11051 (53%)]\tLoss: 0.022283\n",
      "Train Epoch: 15 [5920/11051 (54%)]\tLoss: 0.014492\n",
      "Train Epoch: 15 [6000/11051 (54%)]\tLoss: 0.021875\n",
      "Train Epoch: 15 [6080/11051 (55%)]\tLoss: 0.016415\n",
      "Train Epoch: 15 [6160/11051 (56%)]\tLoss: 0.022137\n",
      "Train Epoch: 15 [6240/11051 (56%)]\tLoss: 0.032235\n",
      "Train Epoch: 15 [6320/11051 (57%)]\tLoss: 0.036927\n",
      "Train Epoch: 15 [6400/11051 (58%)]\tLoss: 0.014465\n",
      "Train Epoch: 15 [6480/11051 (59%)]\tLoss: 0.012380\n",
      "Train Epoch: 15 [6560/11051 (59%)]\tLoss: 0.055051\n",
      "Train Epoch: 15 [6640/11051 (60%)]\tLoss: 0.016984\n",
      "Train Epoch: 15 [6720/11051 (61%)]\tLoss: 0.007297\n",
      "Train Epoch: 15 [6800/11051 (62%)]\tLoss: 0.014628\n",
      "Train Epoch: 15 [6880/11051 (62%)]\tLoss: 0.008335\n",
      "Train Epoch: 15 [6960/11051 (63%)]\tLoss: 0.022469\n",
      "Train Epoch: 15 [7040/11051 (64%)]\tLoss: 0.052369\n",
      "Train Epoch: 15 [7120/11051 (64%)]\tLoss: 0.038307\n",
      "Train Epoch: 15 [7200/11051 (65%)]\tLoss: 0.031042\n",
      "Train Epoch: 15 [7280/11051 (66%)]\tLoss: 0.031208\n",
      "Train Epoch: 15 [7360/11051 (67%)]\tLoss: 0.023151\n",
      "Train Epoch: 15 [7440/11051 (67%)]\tLoss: 0.013429\n",
      "Train Epoch: 15 [7520/11051 (68%)]\tLoss: 0.021993\n",
      "Train Epoch: 15 [7600/11051 (69%)]\tLoss: 0.033886\n",
      "Train Epoch: 15 [7680/11051 (69%)]\tLoss: 0.015088\n",
      "Train Epoch: 15 [7760/11051 (70%)]\tLoss: 0.029316\n",
      "Train Epoch: 15 [7840/11051 (71%)]\tLoss: 0.041977\n",
      "Train Epoch: 15 [7920/11051 (72%)]\tLoss: 0.004663\n",
      "Train Epoch: 15 [8000/11051 (72%)]\tLoss: 0.010026\n",
      "Train Epoch: 15 [8080/11051 (73%)]\tLoss: 0.040202\n",
      "Train Epoch: 15 [8160/11051 (74%)]\tLoss: 0.038036\n",
      "Train Epoch: 15 [8240/11051 (75%)]\tLoss: 0.034901\n",
      "Train Epoch: 15 [8320/11051 (75%)]\tLoss: 0.008634\n",
      "Train Epoch: 15 [8400/11051 (76%)]\tLoss: 0.014852\n",
      "Train Epoch: 15 [8480/11051 (77%)]\tLoss: 0.028096\n",
      "Train Epoch: 15 [8560/11051 (77%)]\tLoss: 0.024754\n",
      "Train Epoch: 15 [8640/11051 (78%)]\tLoss: 0.014267\n",
      "Train Epoch: 15 [8720/11051 (79%)]\tLoss: 0.020804\n",
      "Train Epoch: 15 [8800/11051 (80%)]\tLoss: 0.017510\n",
      "Train Epoch: 15 [8880/11051 (80%)]\tLoss: 0.008222\n",
      "Train Epoch: 15 [8960/11051 (81%)]\tLoss: 0.029318\n",
      "Train Epoch: 15 [9040/11051 (82%)]\tLoss: 0.016202\n",
      "Train Epoch: 15 [9120/11051 (82%)]\tLoss: 0.020276\n",
      "Train Epoch: 15 [9200/11051 (83%)]\tLoss: 0.019929\n",
      "Train Epoch: 15 [9280/11051 (84%)]\tLoss: 0.017196\n",
      "Train Epoch: 15 [9360/11051 (85%)]\tLoss: 0.018498\n",
      "Train Epoch: 15 [9440/11051 (85%)]\tLoss: 0.015998\n",
      "Train Epoch: 15 [9520/11051 (86%)]\tLoss: 0.037545\n",
      "Train Epoch: 15 [9600/11051 (87%)]\tLoss: 0.029414\n",
      "Train Epoch: 15 [9680/11051 (88%)]\tLoss: 0.016383\n",
      "Train Epoch: 15 [9760/11051 (88%)]\tLoss: 0.019724\n",
      "Train Epoch: 15 [9840/11051 (89%)]\tLoss: 0.018604\n",
      "Train Epoch: 15 [9920/11051 (90%)]\tLoss: 0.021936\n",
      "Train Epoch: 15 [10000/11051 (90%)]\tLoss: 0.011233\n",
      "Train Epoch: 15 [10080/11051 (91%)]\tLoss: 0.022468\n",
      "Train Epoch: 15 [10160/11051 (92%)]\tLoss: 0.014340\n",
      "Train Epoch: 15 [10240/11051 (93%)]\tLoss: 0.029517\n",
      "Train Epoch: 15 [10320/11051 (93%)]\tLoss: 0.003973\n",
      "Train Epoch: 15 [10400/11051 (94%)]\tLoss: 0.009500\n",
      "Train Epoch: 15 [10480/11051 (95%)]\tLoss: 0.027302\n",
      "Train Epoch: 15 [10560/11051 (96%)]\tLoss: 0.015140\n",
      "Train Epoch: 15 [10640/11051 (96%)]\tLoss: 0.024653\n",
      "Train Epoch: 15 [10720/11051 (97%)]\tLoss: 0.023160\n",
      "Train Epoch: 15 [10800/11051 (98%)]\tLoss: 0.013744\n",
      "Train Epoch: 15 [10880/11051 (98%)]\tLoss: 0.015724\n",
      "Train Epoch: 15 [10960/11051 (99%)]\tLoss: 0.013196\n",
      "Train Epoch: 15 [11040/11051 (100%)]\tLoss: 0.012069\n",
      "====> Epoch: 15 Average loss: 0.0223\n",
      "\n",
      "Started training epoch no. 17\n",
      "Train Epoch: 16 [0/11051 (0%)]\tLoss: 0.022192\n",
      "Train Epoch: 16 [80/11051 (1%)]\tLoss: 0.019738\n",
      "Train Epoch: 16 [160/11051 (1%)]\tLoss: 0.017303\n",
      "Train Epoch: 16 [240/11051 (2%)]\tLoss: 0.011955\n",
      "Train Epoch: 16 [320/11051 (3%)]\tLoss: 0.020786\n",
      "Train Epoch: 16 [400/11051 (4%)]\tLoss: 0.033999\n",
      "Train Epoch: 16 [480/11051 (4%)]\tLoss: 0.023277\n",
      "Train Epoch: 16 [560/11051 (5%)]\tLoss: 0.038292\n",
      "Train Epoch: 16 [640/11051 (6%)]\tLoss: 0.011546\n",
      "Train Epoch: 16 [720/11051 (7%)]\tLoss: 0.020629\n",
      "Train Epoch: 16 [800/11051 (7%)]\tLoss: 0.054336\n",
      "Train Epoch: 16 [880/11051 (8%)]\tLoss: 0.045881\n",
      "Train Epoch: 16 [960/11051 (9%)]\tLoss: 0.026154\n",
      "Train Epoch: 16 [1040/11051 (9%)]\tLoss: 0.026231\n",
      "Train Epoch: 16 [1120/11051 (10%)]\tLoss: 0.028919\n",
      "Train Epoch: 16 [1200/11051 (11%)]\tLoss: 0.014570\n",
      "Train Epoch: 16 [1280/11051 (12%)]\tLoss: 0.011759\n",
      "Train Epoch: 16 [1360/11051 (12%)]\tLoss: 0.019396\n",
      "Train Epoch: 16 [1440/11051 (13%)]\tLoss: 0.013916\n",
      "Train Epoch: 16 [1520/11051 (14%)]\tLoss: 0.021962\n",
      "Train Epoch: 16 [1600/11051 (14%)]\tLoss: 0.024722\n",
      "Train Epoch: 16 [1680/11051 (15%)]\tLoss: 0.018770\n",
      "Train Epoch: 16 [1760/11051 (16%)]\tLoss: 0.027940\n",
      "Train Epoch: 16 [1840/11051 (17%)]\tLoss: 0.018794\n",
      "Train Epoch: 16 [1920/11051 (17%)]\tLoss: 0.016703\n",
      "Train Epoch: 16 [2000/11051 (18%)]\tLoss: 0.017018\n",
      "Train Epoch: 16 [2080/11051 (19%)]\tLoss: 0.050348\n",
      "Train Epoch: 16 [2160/11051 (20%)]\tLoss: 0.025321\n",
      "Train Epoch: 16 [2240/11051 (20%)]\tLoss: 0.005607\n",
      "Train Epoch: 16 [2320/11051 (21%)]\tLoss: 0.033605\n",
      "Train Epoch: 16 [2400/11051 (22%)]\tLoss: 0.029691\n",
      "Train Epoch: 16 [2480/11051 (22%)]\tLoss: 0.024949\n",
      "Train Epoch: 16 [2560/11051 (23%)]\tLoss: 0.043835\n",
      "Train Epoch: 16 [2640/11051 (24%)]\tLoss: 0.037330\n",
      "Train Epoch: 16 [2720/11051 (25%)]\tLoss: 0.041312\n",
      "Train Epoch: 16 [2800/11051 (25%)]\tLoss: 0.009639\n",
      "Train Epoch: 16 [2880/11051 (26%)]\tLoss: 0.026215\n",
      "Train Epoch: 16 [2960/11051 (27%)]\tLoss: 0.022851\n",
      "Train Epoch: 16 [3040/11051 (27%)]\tLoss: 0.018453\n",
      "Train Epoch: 16 [3120/11051 (28%)]\tLoss: 0.024318\n",
      "Train Epoch: 16 [3200/11051 (29%)]\tLoss: 0.014999\n",
      "Train Epoch: 16 [3280/11051 (30%)]\tLoss: 0.016167\n",
      "Train Epoch: 16 [3360/11051 (30%)]\tLoss: 0.014249\n",
      "Train Epoch: 16 [3440/11051 (31%)]\tLoss: 0.011820\n",
      "Train Epoch: 16 [3520/11051 (32%)]\tLoss: 0.010928\n",
      "Train Epoch: 16 [3600/11051 (33%)]\tLoss: 0.009007\n",
      "Train Epoch: 16 [3680/11051 (33%)]\tLoss: 0.013746\n",
      "Train Epoch: 16 [3760/11051 (34%)]\tLoss: 0.017895\n",
      "Train Epoch: 16 [3840/11051 (35%)]\tLoss: 0.018396\n",
      "Train Epoch: 16 [3920/11051 (35%)]\tLoss: 0.020509\n",
      "Train Epoch: 16 [4000/11051 (36%)]\tLoss: 0.009877\n",
      "Train Epoch: 16 [4080/11051 (37%)]\tLoss: 0.010150\n",
      "Train Epoch: 16 [4160/11051 (38%)]\tLoss: 0.019977\n",
      "Train Epoch: 16 [4240/11051 (38%)]\tLoss: 0.041937\n",
      "Train Epoch: 16 [4320/11051 (39%)]\tLoss: 0.018054\n",
      "Train Epoch: 16 [4400/11051 (40%)]\tLoss: 0.016747\n",
      "Train Epoch: 16 [4480/11051 (41%)]\tLoss: 0.020075\n",
      "Train Epoch: 16 [4560/11051 (41%)]\tLoss: 0.025939\n",
      "Train Epoch: 16 [4640/11051 (42%)]\tLoss: 0.017473\n",
      "Train Epoch: 16 [4720/11051 (43%)]\tLoss: 0.016045\n",
      "Train Epoch: 16 [4800/11051 (43%)]\tLoss: 0.012639\n",
      "Train Epoch: 16 [4880/11051 (44%)]\tLoss: 0.022724\n",
      "Train Epoch: 16 [4960/11051 (45%)]\tLoss: 0.017489\n",
      "Train Epoch: 16 [5040/11051 (46%)]\tLoss: 0.018266\n",
      "Train Epoch: 16 [5120/11051 (46%)]\tLoss: 0.017082\n",
      "Train Epoch: 16 [5200/11051 (47%)]\tLoss: 0.035644\n",
      "Train Epoch: 16 [5280/11051 (48%)]\tLoss: 0.019375\n",
      "Train Epoch: 16 [5360/11051 (48%)]\tLoss: 0.018743\n",
      "Train Epoch: 16 [5440/11051 (49%)]\tLoss: 0.024904\n",
      "Train Epoch: 16 [5520/11051 (50%)]\tLoss: 0.019522\n",
      "Train Epoch: 16 [5600/11051 (51%)]\tLoss: 0.019015\n",
      "Train Epoch: 16 [5680/11051 (51%)]\tLoss: 0.024108\n",
      "Train Epoch: 16 [5760/11051 (52%)]\tLoss: 0.013290\n",
      "Train Epoch: 16 [5840/11051 (53%)]\tLoss: 0.022192\n",
      "Train Epoch: 16 [5920/11051 (54%)]\tLoss: 0.017178\n",
      "Train Epoch: 16 [6000/11051 (54%)]\tLoss: 0.013848\n",
      "Train Epoch: 16 [6080/11051 (55%)]\tLoss: 0.005897\n",
      "Train Epoch: 16 [6160/11051 (56%)]\tLoss: 0.017583\n",
      "Train Epoch: 16 [6240/11051 (56%)]\tLoss: 0.012710\n",
      "Train Epoch: 16 [6320/11051 (57%)]\tLoss: 0.029280\n",
      "Train Epoch: 16 [6400/11051 (58%)]\tLoss: 0.016640\n",
      "Train Epoch: 16 [6480/11051 (59%)]\tLoss: 0.036416\n",
      "Train Epoch: 16 [6560/11051 (59%)]\tLoss: 0.018792\n",
      "Train Epoch: 16 [6640/11051 (60%)]\tLoss: 0.020721\n",
      "Train Epoch: 16 [6720/11051 (61%)]\tLoss: 0.029951\n",
      "Train Epoch: 16 [6800/11051 (62%)]\tLoss: 0.008795\n",
      "Train Epoch: 16 [6880/11051 (62%)]\tLoss: 0.022203\n",
      "Train Epoch: 16 [6960/11051 (63%)]\tLoss: 0.021873\n",
      "Train Epoch: 16 [7040/11051 (64%)]\tLoss: 0.022871\n",
      "Train Epoch: 16 [7120/11051 (64%)]\tLoss: 0.008450\n",
      "Train Epoch: 16 [7200/11051 (65%)]\tLoss: 0.021051\n",
      "Train Epoch: 16 [7280/11051 (66%)]\tLoss: 0.016286\n",
      "Train Epoch: 16 [7360/11051 (67%)]\tLoss: 0.025237\n",
      "Train Epoch: 16 [7440/11051 (67%)]\tLoss: 0.023422\n",
      "Train Epoch: 16 [7520/11051 (68%)]\tLoss: 0.024252\n",
      "Train Epoch: 16 [7600/11051 (69%)]\tLoss: 0.031457\n",
      "Train Epoch: 16 [7680/11051 (69%)]\tLoss: 0.035738\n",
      "Train Epoch: 16 [7760/11051 (70%)]\tLoss: 0.034099\n",
      "Train Epoch: 16 [7840/11051 (71%)]\tLoss: 0.012042\n",
      "Train Epoch: 16 [7920/11051 (72%)]\tLoss: 0.008098\n",
      "Train Epoch: 16 [8000/11051 (72%)]\tLoss: 0.015540\n",
      "Train Epoch: 16 [8080/11051 (73%)]\tLoss: 0.017096\n",
      "Train Epoch: 16 [8160/11051 (74%)]\tLoss: 0.008593\n",
      "Train Epoch: 16 [8240/11051 (75%)]\tLoss: 0.032320\n",
      "Train Epoch: 16 [8320/11051 (75%)]\tLoss: 0.029855\n",
      "Train Epoch: 16 [8400/11051 (76%)]\tLoss: 0.049389\n",
      "Train Epoch: 16 [8480/11051 (77%)]\tLoss: 0.033603\n",
      "Train Epoch: 16 [8560/11051 (77%)]\tLoss: 0.026048\n",
      "Train Epoch: 16 [8640/11051 (78%)]\tLoss: 0.021700\n",
      "Train Epoch: 16 [8720/11051 (79%)]\tLoss: 0.028844\n",
      "Train Epoch: 16 [8800/11051 (80%)]\tLoss: 0.034796\n",
      "Train Epoch: 16 [8880/11051 (80%)]\tLoss: 0.009105\n",
      "Train Epoch: 16 [8960/11051 (81%)]\tLoss: 0.012014\n",
      "Train Epoch: 16 [9040/11051 (82%)]\tLoss: 0.029705\n",
      "Train Epoch: 16 [9120/11051 (82%)]\tLoss: 0.040270\n",
      "Train Epoch: 16 [9200/11051 (83%)]\tLoss: 0.024789\n",
      "Train Epoch: 16 [9280/11051 (84%)]\tLoss: 0.049655\n",
      "Train Epoch: 16 [9360/11051 (85%)]\tLoss: 0.029104\n",
      "Train Epoch: 16 [9440/11051 (85%)]\tLoss: 0.016692\n",
      "Train Epoch: 16 [9520/11051 (86%)]\tLoss: 0.020319\n",
      "Train Epoch: 16 [9600/11051 (87%)]\tLoss: 0.016393\n",
      "Train Epoch: 16 [9680/11051 (88%)]\tLoss: 0.017418\n",
      "Train Epoch: 16 [9760/11051 (88%)]\tLoss: 0.016689\n",
      "Train Epoch: 16 [9840/11051 (89%)]\tLoss: 0.014934\n",
      "Train Epoch: 16 [9920/11051 (90%)]\tLoss: 0.023644\n",
      "Train Epoch: 16 [10000/11051 (90%)]\tLoss: 0.019596\n",
      "Train Epoch: 16 [10080/11051 (91%)]\tLoss: 0.044430\n",
      "Train Epoch: 16 [10160/11051 (92%)]\tLoss: 0.010352\n",
      "Train Epoch: 16 [10240/11051 (93%)]\tLoss: 0.017714\n",
      "Train Epoch: 16 [10320/11051 (93%)]\tLoss: 0.033701\n",
      "Train Epoch: 16 [10400/11051 (94%)]\tLoss: 0.029778\n",
      "Train Epoch: 16 [10480/11051 (95%)]\tLoss: 0.007885\n",
      "Train Epoch: 16 [10560/11051 (96%)]\tLoss: 0.025429\n",
      "Train Epoch: 16 [10640/11051 (96%)]\tLoss: 0.033548\n",
      "Train Epoch: 16 [10720/11051 (97%)]\tLoss: 0.024565\n",
      "Train Epoch: 16 [10800/11051 (98%)]\tLoss: 0.025148\n",
      "Train Epoch: 16 [10880/11051 (98%)]\tLoss: 0.020718\n",
      "Train Epoch: 16 [10960/11051 (99%)]\tLoss: 0.010846\n",
      "Train Epoch: 16 [11040/11051 (100%)]\tLoss: 0.014518\n",
      "====> Epoch: 16 Average loss: 0.0223\n",
      "\n",
      "Started training epoch no. 18\n",
      "Train Epoch: 17 [0/11051 (0%)]\tLoss: 0.027463\n",
      "Train Epoch: 17 [80/11051 (1%)]\tLoss: 0.022537\n",
      "Train Epoch: 17 [160/11051 (1%)]\tLoss: 0.021506\n",
      "Train Epoch: 17 [240/11051 (2%)]\tLoss: 0.009612\n",
      "Train Epoch: 17 [320/11051 (3%)]\tLoss: 0.011877\n",
      "Train Epoch: 17 [400/11051 (4%)]\tLoss: 0.023194\n",
      "Train Epoch: 17 [480/11051 (4%)]\tLoss: 0.024272\n",
      "Train Epoch: 17 [560/11051 (5%)]\tLoss: 0.013254\n",
      "Train Epoch: 17 [640/11051 (6%)]\tLoss: 0.025048\n",
      "Train Epoch: 17 [720/11051 (7%)]\tLoss: 0.006838\n",
      "Train Epoch: 17 [800/11051 (7%)]\tLoss: 0.016556\n",
      "Train Epoch: 17 [880/11051 (8%)]\tLoss: 0.012858\n",
      "Train Epoch: 17 [960/11051 (9%)]\tLoss: 0.019009\n",
      "Train Epoch: 17 [1040/11051 (9%)]\tLoss: 0.022742\n",
      "Train Epoch: 17 [1120/11051 (10%)]\tLoss: 0.023220\n",
      "Train Epoch: 17 [1200/11051 (11%)]\tLoss: 0.018253\n",
      "Train Epoch: 17 [1280/11051 (12%)]\tLoss: 0.028361\n",
      "Train Epoch: 17 [1360/11051 (12%)]\tLoss: 0.015186\n",
      "Train Epoch: 17 [1440/11051 (13%)]\tLoss: 0.024648\n",
      "Train Epoch: 17 [1520/11051 (14%)]\tLoss: 0.017177\n",
      "Train Epoch: 17 [1600/11051 (14%)]\tLoss: 0.015541\n",
      "Train Epoch: 17 [1680/11051 (15%)]\tLoss: 0.004770\n",
      "Train Epoch: 17 [1760/11051 (16%)]\tLoss: 0.017953\n",
      "Train Epoch: 17 [1840/11051 (17%)]\tLoss: 0.025309\n",
      "Train Epoch: 17 [1920/11051 (17%)]\tLoss: 0.026011\n",
      "Train Epoch: 17 [2000/11051 (18%)]\tLoss: 0.017402\n",
      "Train Epoch: 17 [2080/11051 (19%)]\tLoss: 0.016193\n",
      "Train Epoch: 17 [2160/11051 (20%)]\tLoss: 0.044058\n",
      "Train Epoch: 17 [2240/11051 (20%)]\tLoss: 0.022892\n",
      "Train Epoch: 17 [2320/11051 (21%)]\tLoss: 0.031692\n",
      "Train Epoch: 17 [2400/11051 (22%)]\tLoss: 0.022876\n",
      "Train Epoch: 17 [2480/11051 (22%)]\tLoss: 0.031725\n",
      "Train Epoch: 17 [2560/11051 (23%)]\tLoss: 0.022986\n",
      "Train Epoch: 17 [2640/11051 (24%)]\tLoss: 0.017447\n",
      "Train Epoch: 17 [2720/11051 (25%)]\tLoss: 0.022237\n",
      "Train Epoch: 17 [2800/11051 (25%)]\tLoss: 0.015007\n",
      "Train Epoch: 17 [2880/11051 (26%)]\tLoss: 0.013618\n",
      "Train Epoch: 17 [2960/11051 (27%)]\tLoss: 0.014594\n",
      "Train Epoch: 17 [3040/11051 (27%)]\tLoss: 0.022001\n",
      "Train Epoch: 17 [3120/11051 (28%)]\tLoss: 0.045064\n",
      "Train Epoch: 17 [3200/11051 (29%)]\tLoss: 0.011250\n",
      "Train Epoch: 17 [3280/11051 (30%)]\tLoss: 0.032509\n",
      "Train Epoch: 17 [3360/11051 (30%)]\tLoss: 0.021706\n",
      "Train Epoch: 17 [3440/11051 (31%)]\tLoss: 0.010074\n",
      "Train Epoch: 17 [3520/11051 (32%)]\tLoss: 0.016274\n",
      "Train Epoch: 17 [3600/11051 (33%)]\tLoss: 0.030570\n",
      "Train Epoch: 17 [3680/11051 (33%)]\tLoss: 0.018014\n",
      "Train Epoch: 17 [3760/11051 (34%)]\tLoss: 0.010731\n",
      "Train Epoch: 17 [3840/11051 (35%)]\tLoss: 0.027025\n",
      "Train Epoch: 17 [3920/11051 (35%)]\tLoss: 0.014375\n",
      "Train Epoch: 17 [4000/11051 (36%)]\tLoss: 0.024936\n",
      "Train Epoch: 17 [4080/11051 (37%)]\tLoss: 0.012907\n",
      "Train Epoch: 17 [4160/11051 (38%)]\tLoss: 0.027910\n",
      "Train Epoch: 17 [4240/11051 (38%)]\tLoss: 0.013506\n",
      "Train Epoch: 17 [4320/11051 (39%)]\tLoss: 0.030163\n",
      "Train Epoch: 17 [4400/11051 (40%)]\tLoss: 0.020516\n",
      "Train Epoch: 17 [4480/11051 (41%)]\tLoss: 0.036660\n",
      "Train Epoch: 17 [4560/11051 (41%)]\tLoss: 0.016763\n",
      "Train Epoch: 17 [4640/11051 (42%)]\tLoss: 0.012873\n",
      "Train Epoch: 17 [4720/11051 (43%)]\tLoss: 0.021182\n",
      "Train Epoch: 17 [4800/11051 (43%)]\tLoss: 0.025581\n",
      "Train Epoch: 17 [4880/11051 (44%)]\tLoss: 0.017142\n",
      "Train Epoch: 17 [4960/11051 (45%)]\tLoss: 0.027784\n",
      "Train Epoch: 17 [5040/11051 (46%)]\tLoss: 0.021346\n",
      "Train Epoch: 17 [5120/11051 (46%)]\tLoss: 0.010633\n",
      "Train Epoch: 17 [5200/11051 (47%)]\tLoss: 0.007378\n",
      "Train Epoch: 17 [5280/11051 (48%)]\tLoss: 0.027436\n",
      "Train Epoch: 17 [5360/11051 (48%)]\tLoss: 0.023451\n",
      "Train Epoch: 17 [5440/11051 (49%)]\tLoss: 0.028057\n",
      "Train Epoch: 17 [5520/11051 (50%)]\tLoss: 0.025850\n",
      "Train Epoch: 17 [5600/11051 (51%)]\tLoss: 0.034962\n",
      "Train Epoch: 17 [5680/11051 (51%)]\tLoss: 0.015419\n",
      "Train Epoch: 17 [5760/11051 (52%)]\tLoss: 0.010274\n",
      "Train Epoch: 17 [5840/11051 (53%)]\tLoss: 0.019749\n",
      "Train Epoch: 17 [5920/11051 (54%)]\tLoss: 0.015595\n",
      "Train Epoch: 17 [6000/11051 (54%)]\tLoss: 0.026521\n",
      "Train Epoch: 17 [6080/11051 (55%)]\tLoss: 0.037310\n",
      "Train Epoch: 17 [6160/11051 (56%)]\tLoss: 0.019230\n",
      "Train Epoch: 17 [6240/11051 (56%)]\tLoss: 0.038845\n",
      "Train Epoch: 17 [6320/11051 (57%)]\tLoss: 0.023461\n",
      "Train Epoch: 17 [6400/11051 (58%)]\tLoss: 0.010060\n",
      "Train Epoch: 17 [6480/11051 (59%)]\tLoss: 0.008689\n",
      "Train Epoch: 17 [6560/11051 (59%)]\tLoss: 0.017321\n",
      "Train Epoch: 17 [6640/11051 (60%)]\tLoss: 0.033400\n",
      "Train Epoch: 17 [6720/11051 (61%)]\tLoss: 0.022581\n",
      "Train Epoch: 17 [6800/11051 (62%)]\tLoss: 0.021820\n",
      "Train Epoch: 17 [6880/11051 (62%)]\tLoss: 0.018504\n",
      "Train Epoch: 17 [6960/11051 (63%)]\tLoss: 0.020083\n",
      "Train Epoch: 17 [7040/11051 (64%)]\tLoss: 0.029895\n",
      "Train Epoch: 17 [7120/11051 (64%)]\tLoss: 0.016927\n",
      "Train Epoch: 17 [7200/11051 (65%)]\tLoss: 0.022105\n",
      "Train Epoch: 17 [7280/11051 (66%)]\tLoss: 0.015106\n",
      "Train Epoch: 17 [7360/11051 (67%)]\tLoss: 0.022746\n",
      "Train Epoch: 17 [7440/11051 (67%)]\tLoss: 0.014273\n",
      "Train Epoch: 17 [7520/11051 (68%)]\tLoss: 0.035443\n",
      "Train Epoch: 17 [7600/11051 (69%)]\tLoss: 0.045124\n",
      "Train Epoch: 17 [7680/11051 (69%)]\tLoss: 0.018124\n",
      "Train Epoch: 17 [7760/11051 (70%)]\tLoss: 0.033056\n",
      "Train Epoch: 17 [7840/11051 (71%)]\tLoss: 0.013071\n",
      "Train Epoch: 17 [7920/11051 (72%)]\tLoss: 0.011307\n",
      "Train Epoch: 17 [8000/11051 (72%)]\tLoss: 0.023092\n",
      "Train Epoch: 17 [8080/11051 (73%)]\tLoss: 0.033196\n",
      "Train Epoch: 17 [8160/11051 (74%)]\tLoss: 0.023317\n",
      "Train Epoch: 17 [8240/11051 (75%)]\tLoss: 0.030654\n",
      "Train Epoch: 17 [8320/11051 (75%)]\tLoss: 0.014503\n",
      "Train Epoch: 17 [8400/11051 (76%)]\tLoss: 0.033294\n",
      "Train Epoch: 17 [8480/11051 (77%)]\tLoss: 0.018030\n",
      "Train Epoch: 17 [8560/11051 (77%)]\tLoss: 0.011710\n",
      "Train Epoch: 17 [8640/11051 (78%)]\tLoss: 0.021362\n",
      "Train Epoch: 17 [8720/11051 (79%)]\tLoss: 0.028937\n",
      "Train Epoch: 17 [8800/11051 (80%)]\tLoss: 0.024261\n",
      "Train Epoch: 17 [8880/11051 (80%)]\tLoss: 0.018623\n",
      "Train Epoch: 17 [8960/11051 (81%)]\tLoss: 0.019068\n",
      "Train Epoch: 17 [9040/11051 (82%)]\tLoss: 0.009852\n",
      "Train Epoch: 17 [9120/11051 (82%)]\tLoss: 0.010926\n",
      "Train Epoch: 17 [9200/11051 (83%)]\tLoss: 0.012944\n",
      "Train Epoch: 17 [9280/11051 (84%)]\tLoss: 0.008148\n",
      "Train Epoch: 17 [9360/11051 (85%)]\tLoss: 0.015940\n",
      "Train Epoch: 17 [9440/11051 (85%)]\tLoss: 0.015401\n",
      "Train Epoch: 17 [9520/11051 (86%)]\tLoss: 0.016097\n",
      "Train Epoch: 17 [9600/11051 (87%)]\tLoss: 0.012578\n",
      "Train Epoch: 17 [9680/11051 (88%)]\tLoss: 0.026180\n",
      "Train Epoch: 17 [9760/11051 (88%)]\tLoss: 0.013254\n",
      "Train Epoch: 17 [9840/11051 (89%)]\tLoss: 0.020815\n",
      "Train Epoch: 17 [9920/11051 (90%)]\tLoss: 0.010064\n",
      "Train Epoch: 17 [10000/11051 (90%)]\tLoss: 0.027688\n",
      "Train Epoch: 17 [10080/11051 (91%)]\tLoss: 0.023476\n",
      "Train Epoch: 17 [10160/11051 (92%)]\tLoss: 0.009141\n",
      "Train Epoch: 17 [10240/11051 (93%)]\tLoss: 0.018454\n",
      "Train Epoch: 17 [10320/11051 (93%)]\tLoss: 0.016382\n",
      "Train Epoch: 17 [10400/11051 (94%)]\tLoss: 0.017690\n",
      "Train Epoch: 17 [10480/11051 (95%)]\tLoss: 0.026213\n",
      "Train Epoch: 17 [10560/11051 (96%)]\tLoss: 0.046225\n",
      "Train Epoch: 17 [10640/11051 (96%)]\tLoss: 0.017820\n",
      "Train Epoch: 17 [10720/11051 (97%)]\tLoss: 0.015931\n",
      "Train Epoch: 17 [10800/11051 (98%)]\tLoss: 0.023592\n",
      "Train Epoch: 17 [10880/11051 (98%)]\tLoss: 0.017041\n",
      "Train Epoch: 17 [10960/11051 (99%)]\tLoss: 0.016754\n",
      "Train Epoch: 17 [11040/11051 (100%)]\tLoss: 0.041023\n",
      "====> Epoch: 17 Average loss: 0.0221\n",
      "\n",
      "Started training epoch no. 19\n",
      "Train Epoch: 18 [0/11051 (0%)]\tLoss: 0.028121\n",
      "Train Epoch: 18 [80/11051 (1%)]\tLoss: 0.024592\n",
      "Train Epoch: 18 [160/11051 (1%)]\tLoss: 0.018309\n",
      "Train Epoch: 18 [240/11051 (2%)]\tLoss: 0.014452\n",
      "Train Epoch: 18 [320/11051 (3%)]\tLoss: 0.049322\n",
      "Train Epoch: 18 [400/11051 (4%)]\tLoss: 0.021214\n",
      "Train Epoch: 18 [480/11051 (4%)]\tLoss: 0.013096\n",
      "Train Epoch: 18 [560/11051 (5%)]\tLoss: 0.013125\n",
      "Train Epoch: 18 [640/11051 (6%)]\tLoss: 0.016834\n",
      "Train Epoch: 18 [720/11051 (7%)]\tLoss: 0.018011\n",
      "Train Epoch: 18 [800/11051 (7%)]\tLoss: 0.024612\n",
      "Train Epoch: 18 [880/11051 (8%)]\tLoss: 0.016842\n",
      "Train Epoch: 18 [960/11051 (9%)]\tLoss: 0.018430\n",
      "Train Epoch: 18 [1040/11051 (9%)]\tLoss: 0.008189\n",
      "Train Epoch: 18 [1120/11051 (10%)]\tLoss: 0.015187\n",
      "Train Epoch: 18 [1200/11051 (11%)]\tLoss: 0.018823\n",
      "Train Epoch: 18 [1280/11051 (12%)]\tLoss: 0.020899\n",
      "Train Epoch: 18 [1360/11051 (12%)]\tLoss: 0.039523\n",
      "Train Epoch: 18 [1440/11051 (13%)]\tLoss: 0.022982\n",
      "Train Epoch: 18 [1520/11051 (14%)]\tLoss: 0.017113\n",
      "Train Epoch: 18 [1600/11051 (14%)]\tLoss: 0.009993\n",
      "Train Epoch: 18 [1680/11051 (15%)]\tLoss: 0.039293\n",
      "Train Epoch: 18 [1760/11051 (16%)]\tLoss: 0.045609\n",
      "Train Epoch: 18 [1840/11051 (17%)]\tLoss: 0.023344\n",
      "Train Epoch: 18 [1920/11051 (17%)]\tLoss: 0.010028\n",
      "Train Epoch: 18 [2000/11051 (18%)]\tLoss: 0.019466\n",
      "Train Epoch: 18 [2080/11051 (19%)]\tLoss: 0.044177\n",
      "Train Epoch: 18 [2160/11051 (20%)]\tLoss: 0.028518\n",
      "Train Epoch: 18 [2240/11051 (20%)]\tLoss: 0.016686\n",
      "Train Epoch: 18 [2320/11051 (21%)]\tLoss: 0.017494\n",
      "Train Epoch: 18 [2400/11051 (22%)]\tLoss: 0.017419\n",
      "Train Epoch: 18 [2480/11051 (22%)]\tLoss: 0.012296\n",
      "Train Epoch: 18 [2560/11051 (23%)]\tLoss: 0.014085\n",
      "Train Epoch: 18 [2640/11051 (24%)]\tLoss: 0.034764\n",
      "Train Epoch: 18 [2720/11051 (25%)]\tLoss: 0.031390\n",
      "Train Epoch: 18 [2800/11051 (25%)]\tLoss: 0.038628\n",
      "Train Epoch: 18 [2880/11051 (26%)]\tLoss: 0.035714\n",
      "Train Epoch: 18 [2960/11051 (27%)]\tLoss: 0.033199\n",
      "Train Epoch: 18 [3040/11051 (27%)]\tLoss: 0.037007\n",
      "Train Epoch: 18 [3120/11051 (28%)]\tLoss: 0.015357\n",
      "Train Epoch: 18 [3200/11051 (29%)]\tLoss: 0.043918\n",
      "Train Epoch: 18 [3280/11051 (30%)]\tLoss: 0.021253\n",
      "Train Epoch: 18 [3360/11051 (30%)]\tLoss: 0.018874\n",
      "Train Epoch: 18 [3440/11051 (31%)]\tLoss: 0.014288\n",
      "Train Epoch: 18 [3520/11051 (32%)]\tLoss: 0.025904\n",
      "Train Epoch: 18 [3600/11051 (33%)]\tLoss: 0.031546\n",
      "Train Epoch: 18 [3680/11051 (33%)]\tLoss: 0.015500\n",
      "Train Epoch: 18 [3760/11051 (34%)]\tLoss: 0.040251\n",
      "Train Epoch: 18 [3840/11051 (35%)]\tLoss: 0.010755\n",
      "Train Epoch: 18 [3920/11051 (35%)]\tLoss: 0.017710\n",
      "Train Epoch: 18 [4000/11051 (36%)]\tLoss: 0.014634\n",
      "Train Epoch: 18 [4080/11051 (37%)]\tLoss: 0.015622\n",
      "Train Epoch: 18 [4160/11051 (38%)]\tLoss: 0.023054\n",
      "Train Epoch: 18 [4240/11051 (38%)]\tLoss: 0.018987\n",
      "Train Epoch: 18 [4320/11051 (39%)]\tLoss: 0.036178\n",
      "Train Epoch: 18 [4400/11051 (40%)]\tLoss: 0.020571\n",
      "Train Epoch: 18 [4480/11051 (41%)]\tLoss: 0.075592\n",
      "Train Epoch: 18 [4560/11051 (41%)]\tLoss: 0.016174\n",
      "Train Epoch: 18 [4640/11051 (42%)]\tLoss: 0.012666\n",
      "Train Epoch: 18 [4720/11051 (43%)]\tLoss: 0.018580\n",
      "Train Epoch: 18 [4800/11051 (43%)]\tLoss: 0.010494\n",
      "Train Epoch: 18 [4880/11051 (44%)]\tLoss: 0.010041\n",
      "Train Epoch: 18 [4960/11051 (45%)]\tLoss: 0.015125\n",
      "Train Epoch: 18 [5040/11051 (46%)]\tLoss: 0.023713\n",
      "Train Epoch: 18 [5120/11051 (46%)]\tLoss: 0.050176\n",
      "Train Epoch: 18 [5200/11051 (47%)]\tLoss: 0.011585\n",
      "Train Epoch: 18 [5280/11051 (48%)]\tLoss: 0.030108\n",
      "Train Epoch: 18 [5360/11051 (48%)]\tLoss: 0.034644\n",
      "Train Epoch: 18 [5440/11051 (49%)]\tLoss: 0.024512\n",
      "Train Epoch: 18 [5520/11051 (50%)]\tLoss: 0.038523\n",
      "Train Epoch: 18 [5600/11051 (51%)]\tLoss: 0.014377\n",
      "Train Epoch: 18 [5680/11051 (51%)]\tLoss: 0.037748\n",
      "Train Epoch: 18 [5760/11051 (52%)]\tLoss: 0.017977\n",
      "Train Epoch: 18 [5840/11051 (53%)]\tLoss: 0.011041\n",
      "Train Epoch: 18 [5920/11051 (54%)]\tLoss: 0.016838\n",
      "Train Epoch: 18 [6000/11051 (54%)]\tLoss: 0.010268\n",
      "Train Epoch: 18 [6080/11051 (55%)]\tLoss: 0.034639\n",
      "Train Epoch: 18 [6160/11051 (56%)]\tLoss: 0.020598\n",
      "Train Epoch: 18 [6240/11051 (56%)]\tLoss: 0.019745\n",
      "Train Epoch: 18 [6320/11051 (57%)]\tLoss: 0.015941\n",
      "Train Epoch: 18 [6400/11051 (58%)]\tLoss: 0.035838\n",
      "Train Epoch: 18 [6480/11051 (59%)]\tLoss: 0.013338\n",
      "Train Epoch: 18 [6560/11051 (59%)]\tLoss: 0.017300\n",
      "Train Epoch: 18 [6640/11051 (60%)]\tLoss: 0.013697\n",
      "Train Epoch: 18 [6720/11051 (61%)]\tLoss: 0.013739\n",
      "Train Epoch: 18 [6800/11051 (62%)]\tLoss: 0.014164\n",
      "Train Epoch: 18 [6880/11051 (62%)]\tLoss: 0.010662\n",
      "Train Epoch: 18 [6960/11051 (63%)]\tLoss: 0.020347\n",
      "Train Epoch: 18 [7040/11051 (64%)]\tLoss: 0.018123\n",
      "Train Epoch: 18 [7120/11051 (64%)]\tLoss: 0.020041\n",
      "Train Epoch: 18 [7200/11051 (65%)]\tLoss: 0.010468\n",
      "Train Epoch: 18 [7280/11051 (66%)]\tLoss: 0.027276\n",
      "Train Epoch: 18 [7360/11051 (67%)]\tLoss: 0.024220\n",
      "Train Epoch: 18 [7440/11051 (67%)]\tLoss: 0.023723\n",
      "Train Epoch: 18 [7520/11051 (68%)]\tLoss: 0.009636\n",
      "Train Epoch: 18 [7600/11051 (69%)]\tLoss: 0.019634\n",
      "Train Epoch: 18 [7680/11051 (69%)]\tLoss: 0.029921\n",
      "Train Epoch: 18 [7760/11051 (70%)]\tLoss: 0.019266\n",
      "Train Epoch: 18 [7840/11051 (71%)]\tLoss: 0.115114\n",
      "Train Epoch: 18 [7920/11051 (72%)]\tLoss: 0.041547\n",
      "Train Epoch: 18 [8000/11051 (72%)]\tLoss: 0.017750\n",
      "Train Epoch: 18 [8080/11051 (73%)]\tLoss: 0.017304\n",
      "Train Epoch: 18 [8160/11051 (74%)]\tLoss: 0.008000\n",
      "Train Epoch: 18 [8240/11051 (75%)]\tLoss: 0.040923\n",
      "Train Epoch: 18 [8320/11051 (75%)]\tLoss: 0.026405\n",
      "Train Epoch: 18 [8400/11051 (76%)]\tLoss: 0.009734\n",
      "Train Epoch: 18 [8480/11051 (77%)]\tLoss: 0.026557\n",
      "Train Epoch: 18 [8560/11051 (77%)]\tLoss: 0.019664\n",
      "Train Epoch: 18 [8640/11051 (78%)]\tLoss: 0.011715\n",
      "Train Epoch: 18 [8720/11051 (79%)]\tLoss: 0.026922\n",
      "Train Epoch: 18 [8800/11051 (80%)]\tLoss: 0.012281\n",
      "Train Epoch: 18 [8880/11051 (80%)]\tLoss: 0.021425\n",
      "Train Epoch: 18 [8960/11051 (81%)]\tLoss: 0.014070\n",
      "Train Epoch: 18 [9040/11051 (82%)]\tLoss: 0.033313\n",
      "Train Epoch: 18 [9120/11051 (82%)]\tLoss: 0.029808\n",
      "Train Epoch: 18 [9200/11051 (83%)]\tLoss: 0.019808\n",
      "Train Epoch: 18 [9280/11051 (84%)]\tLoss: 0.024148\n",
      "Train Epoch: 18 [9360/11051 (85%)]\tLoss: 0.014118\n",
      "Train Epoch: 18 [9440/11051 (85%)]\tLoss: 0.030354\n",
      "Train Epoch: 18 [9520/11051 (86%)]\tLoss: 0.018681\n",
      "Train Epoch: 18 [9600/11051 (87%)]\tLoss: 0.015931\n",
      "Train Epoch: 18 [9680/11051 (88%)]\tLoss: 0.020404\n",
      "Train Epoch: 18 [9760/11051 (88%)]\tLoss: 0.009519\n",
      "Train Epoch: 18 [9840/11051 (89%)]\tLoss: 0.013960\n",
      "Train Epoch: 18 [9920/11051 (90%)]\tLoss: 0.024632\n",
      "Train Epoch: 18 [10000/11051 (90%)]\tLoss: 0.024637\n",
      "Train Epoch: 18 [10080/11051 (91%)]\tLoss: 0.035320\n",
      "Train Epoch: 18 [10160/11051 (92%)]\tLoss: 0.024722\n",
      "Train Epoch: 18 [10240/11051 (93%)]\tLoss: 0.012547\n",
      "Train Epoch: 18 [10320/11051 (93%)]\tLoss: 0.005666\n",
      "Train Epoch: 18 [10400/11051 (94%)]\tLoss: 0.026086\n",
      "Train Epoch: 18 [10480/11051 (95%)]\tLoss: 0.011528\n",
      "Train Epoch: 18 [10560/11051 (96%)]\tLoss: 0.044633\n",
      "Train Epoch: 18 [10640/11051 (96%)]\tLoss: 0.018806\n",
      "Train Epoch: 18 [10720/11051 (97%)]\tLoss: 0.018255\n",
      "Train Epoch: 18 [10800/11051 (98%)]\tLoss: 0.018789\n",
      "Train Epoch: 18 [10880/11051 (98%)]\tLoss: 0.013712\n",
      "Train Epoch: 18 [10960/11051 (99%)]\tLoss: 0.016144\n",
      "Train Epoch: 18 [11040/11051 (100%)]\tLoss: 0.015555\n",
      "====> Epoch: 18 Average loss: 0.0221\n",
      "\n",
      "Started training epoch no. 20\n",
      "Train Epoch: 19 [0/11051 (0%)]\tLoss: 0.020955\n",
      "Train Epoch: 19 [80/11051 (1%)]\tLoss: 0.030763\n",
      "Train Epoch: 19 [160/11051 (1%)]\tLoss: 0.017893\n",
      "Train Epoch: 19 [240/11051 (2%)]\tLoss: 0.024695\n",
      "Train Epoch: 19 [320/11051 (3%)]\tLoss: 0.023868\n",
      "Train Epoch: 19 [400/11051 (4%)]\tLoss: 0.065604\n",
      "Train Epoch: 19 [480/11051 (4%)]\tLoss: 0.018495\n",
      "Train Epoch: 19 [560/11051 (5%)]\tLoss: 0.052406\n",
      "Train Epoch: 19 [640/11051 (6%)]\tLoss: 0.018243\n",
      "Train Epoch: 19 [720/11051 (7%)]\tLoss: 0.030931\n",
      "Train Epoch: 19 [800/11051 (7%)]\tLoss: 0.024515\n",
      "Train Epoch: 19 [880/11051 (8%)]\tLoss: 0.021102\n",
      "Train Epoch: 19 [960/11051 (9%)]\tLoss: 0.019773\n",
      "Train Epoch: 19 [1040/11051 (9%)]\tLoss: 0.031164\n",
      "Train Epoch: 19 [1120/11051 (10%)]\tLoss: 0.018407\n",
      "Train Epoch: 19 [1200/11051 (11%)]\tLoss: 0.023398\n",
      "Train Epoch: 19 [1280/11051 (12%)]\tLoss: 0.020671\n",
      "Train Epoch: 19 [1360/11051 (12%)]\tLoss: 0.017375\n",
      "Train Epoch: 19 [1440/11051 (13%)]\tLoss: 0.020319\n",
      "Train Epoch: 19 [1520/11051 (14%)]\tLoss: 0.018087\n",
      "Train Epoch: 19 [1600/11051 (14%)]\tLoss: 0.024777\n",
      "Train Epoch: 19 [1680/11051 (15%)]\tLoss: 0.019147\n",
      "Train Epoch: 19 [1760/11051 (16%)]\tLoss: 0.016035\n",
      "Train Epoch: 19 [1840/11051 (17%)]\tLoss: 0.018498\n",
      "Train Epoch: 19 [1920/11051 (17%)]\tLoss: 0.043392\n",
      "Train Epoch: 19 [2000/11051 (18%)]\tLoss: 0.006227\n",
      "Train Epoch: 19 [2080/11051 (19%)]\tLoss: 0.013856\n",
      "Train Epoch: 19 [2160/11051 (20%)]\tLoss: 0.027146\n",
      "Train Epoch: 19 [2240/11051 (20%)]\tLoss: 0.014066\n",
      "Train Epoch: 19 [2320/11051 (21%)]\tLoss: 0.007698\n",
      "Train Epoch: 19 [2400/11051 (22%)]\tLoss: 0.023376\n",
      "Train Epoch: 19 [2480/11051 (22%)]\tLoss: 0.019152\n",
      "Train Epoch: 19 [2560/11051 (23%)]\tLoss: 0.011233\n",
      "Train Epoch: 19 [2640/11051 (24%)]\tLoss: 0.031998\n",
      "Train Epoch: 19 [2720/11051 (25%)]\tLoss: 0.035616\n",
      "Train Epoch: 19 [2800/11051 (25%)]\tLoss: 0.040318\n",
      "Train Epoch: 19 [2880/11051 (26%)]\tLoss: 0.015942\n",
      "Train Epoch: 19 [2960/11051 (27%)]\tLoss: 0.017972\n",
      "Train Epoch: 19 [3040/11051 (27%)]\tLoss: 0.004270\n",
      "Train Epoch: 19 [3120/11051 (28%)]\tLoss: 0.009449\n",
      "Train Epoch: 19 [3200/11051 (29%)]\tLoss: 0.012394\n",
      "Train Epoch: 19 [3280/11051 (30%)]\tLoss: 0.032237\n",
      "Train Epoch: 19 [3360/11051 (30%)]\tLoss: 0.004573\n",
      "Train Epoch: 19 [3440/11051 (31%)]\tLoss: 0.045358\n",
      "Train Epoch: 19 [3520/11051 (32%)]\tLoss: 0.014641\n",
      "Train Epoch: 19 [3600/11051 (33%)]\tLoss: 0.014719\n",
      "Train Epoch: 19 [3680/11051 (33%)]\tLoss: 0.020057\n",
      "Train Epoch: 19 [3760/11051 (34%)]\tLoss: 0.018400\n",
      "Train Epoch: 19 [3840/11051 (35%)]\tLoss: 0.013223\n",
      "Train Epoch: 19 [3920/11051 (35%)]\tLoss: 0.022533\n",
      "Train Epoch: 19 [4000/11051 (36%)]\tLoss: 0.024734\n",
      "Train Epoch: 19 [4080/11051 (37%)]\tLoss: 0.020952\n",
      "Train Epoch: 19 [4160/11051 (38%)]\tLoss: 0.040020\n",
      "Train Epoch: 19 [4240/11051 (38%)]\tLoss: 0.019183\n",
      "Train Epoch: 19 [4320/11051 (39%)]\tLoss: 0.027965\n",
      "Train Epoch: 19 [4400/11051 (40%)]\tLoss: 0.017518\n",
      "Train Epoch: 19 [4480/11051 (41%)]\tLoss: 0.015998\n",
      "Train Epoch: 19 [4560/11051 (41%)]\tLoss: 0.011362\n",
      "Train Epoch: 19 [4640/11051 (42%)]\tLoss: 0.007571\n",
      "Train Epoch: 19 [4720/11051 (43%)]\tLoss: 0.014075\n",
      "Train Epoch: 19 [4800/11051 (43%)]\tLoss: 0.038443\n",
      "Train Epoch: 19 [4880/11051 (44%)]\tLoss: 0.017762\n",
      "Train Epoch: 19 [4960/11051 (45%)]\tLoss: 0.016647\n",
      "Train Epoch: 19 [5040/11051 (46%)]\tLoss: 0.020008\n",
      "Train Epoch: 19 [5120/11051 (46%)]\tLoss: 0.024439\n",
      "Train Epoch: 19 [5200/11051 (47%)]\tLoss: 0.012562\n",
      "Train Epoch: 19 [5280/11051 (48%)]\tLoss: 0.017567\n",
      "Train Epoch: 19 [5360/11051 (48%)]\tLoss: 0.021402\n",
      "Train Epoch: 19 [5440/11051 (49%)]\tLoss: 0.029288\n",
      "Train Epoch: 19 [5520/11051 (50%)]\tLoss: 0.017186\n",
      "Train Epoch: 19 [5600/11051 (51%)]\tLoss: 0.017568\n",
      "Train Epoch: 19 [5680/11051 (51%)]\tLoss: 0.013180\n",
      "Train Epoch: 19 [5760/11051 (52%)]\tLoss: 0.034154\n",
      "Train Epoch: 19 [5840/11051 (53%)]\tLoss: 0.028460\n",
      "Train Epoch: 19 [5920/11051 (54%)]\tLoss: 0.027497\n",
      "Train Epoch: 19 [6000/11051 (54%)]\tLoss: 0.026791\n",
      "Train Epoch: 19 [6080/11051 (55%)]\tLoss: 0.022294\n",
      "Train Epoch: 19 [6160/11051 (56%)]\tLoss: 0.035739\n",
      "Train Epoch: 19 [6240/11051 (56%)]\tLoss: 0.018414\n",
      "Train Epoch: 19 [6320/11051 (57%)]\tLoss: 0.031517\n",
      "Train Epoch: 19 [6400/11051 (58%)]\tLoss: 0.015501\n",
      "Train Epoch: 19 [6480/11051 (59%)]\tLoss: 0.033404\n",
      "Train Epoch: 19 [6560/11051 (59%)]\tLoss: 0.013335\n",
      "Train Epoch: 19 [6640/11051 (60%)]\tLoss: 0.012500\n",
      "Train Epoch: 19 [6720/11051 (61%)]\tLoss: 0.012738\n",
      "Train Epoch: 19 [6800/11051 (62%)]\tLoss: 0.015840\n",
      "Train Epoch: 19 [6880/11051 (62%)]\tLoss: 0.022254\n",
      "Train Epoch: 19 [6960/11051 (63%)]\tLoss: 0.011534\n",
      "Train Epoch: 19 [7040/11051 (64%)]\tLoss: 0.019304\n",
      "Train Epoch: 19 [7120/11051 (64%)]\tLoss: 0.020347\n",
      "Train Epoch: 19 [7200/11051 (65%)]\tLoss: 0.036376\n",
      "Train Epoch: 19 [7280/11051 (66%)]\tLoss: 0.023662\n",
      "Train Epoch: 19 [7360/11051 (67%)]\tLoss: 0.024306\n",
      "Train Epoch: 19 [7440/11051 (67%)]\tLoss: 0.014694\n",
      "Train Epoch: 19 [7520/11051 (68%)]\tLoss: 0.011774\n",
      "Train Epoch: 19 [7600/11051 (69%)]\tLoss: 0.019804\n",
      "Train Epoch: 19 [7680/11051 (69%)]\tLoss: 0.022958\n",
      "Train Epoch: 19 [7760/11051 (70%)]\tLoss: 0.025468\n",
      "Train Epoch: 19 [7840/11051 (71%)]\tLoss: 0.028106\n",
      "Train Epoch: 19 [7920/11051 (72%)]\tLoss: 0.007320\n",
      "Train Epoch: 19 [8000/11051 (72%)]\tLoss: 0.024181\n",
      "Train Epoch: 19 [8080/11051 (73%)]\tLoss: 0.032659\n",
      "Train Epoch: 19 [8160/11051 (74%)]\tLoss: 0.028391\n",
      "Train Epoch: 19 [8240/11051 (75%)]\tLoss: 0.019714\n",
      "Train Epoch: 19 [8320/11051 (75%)]\tLoss: 0.026478\n",
      "Train Epoch: 19 [8400/11051 (76%)]\tLoss: 0.023625\n",
      "Train Epoch: 19 [8480/11051 (77%)]\tLoss: 0.006807\n",
      "Train Epoch: 19 [8560/11051 (77%)]\tLoss: 0.007573\n",
      "Train Epoch: 19 [8640/11051 (78%)]\tLoss: 0.021566\n",
      "Train Epoch: 19 [8720/11051 (79%)]\tLoss: 0.018701\n",
      "Train Epoch: 19 [8800/11051 (80%)]\tLoss: 0.035255\n",
      "Train Epoch: 19 [8880/11051 (80%)]\tLoss: 0.014715\n",
      "Train Epoch: 19 [8960/11051 (81%)]\tLoss: 0.011245\n",
      "Train Epoch: 19 [9040/11051 (82%)]\tLoss: 0.027639\n",
      "Train Epoch: 19 [9120/11051 (82%)]\tLoss: 0.020568\n",
      "Train Epoch: 19 [9200/11051 (83%)]\tLoss: 0.014090\n",
      "Train Epoch: 19 [9280/11051 (84%)]\tLoss: 0.030400\n",
      "Train Epoch: 19 [9360/11051 (85%)]\tLoss: 0.017819\n",
      "Train Epoch: 19 [9440/11051 (85%)]\tLoss: 0.013750\n",
      "Train Epoch: 19 [9520/11051 (86%)]\tLoss: 0.017240\n",
      "Train Epoch: 19 [9600/11051 (87%)]\tLoss: 0.022302\n",
      "Train Epoch: 19 [9680/11051 (88%)]\tLoss: 0.008170\n",
      "Train Epoch: 19 [9760/11051 (88%)]\tLoss: 0.010038\n",
      "Train Epoch: 19 [9840/11051 (89%)]\tLoss: 0.031912\n",
      "Train Epoch: 19 [9920/11051 (90%)]\tLoss: 0.019735\n",
      "Train Epoch: 19 [10000/11051 (90%)]\tLoss: 0.031859\n",
      "Train Epoch: 19 [10080/11051 (91%)]\tLoss: 0.008066\n",
      "Train Epoch: 19 [10160/11051 (92%)]\tLoss: 0.019668\n",
      "Train Epoch: 19 [10240/11051 (93%)]\tLoss: 0.024083\n",
      "Train Epoch: 19 [10320/11051 (93%)]\tLoss: 0.024791\n",
      "Train Epoch: 19 [10400/11051 (94%)]\tLoss: 0.023336\n",
      "Train Epoch: 19 [10480/11051 (95%)]\tLoss: 0.027137\n",
      "Train Epoch: 19 [10560/11051 (96%)]\tLoss: 0.025958\n",
      "Train Epoch: 19 [10640/11051 (96%)]\tLoss: 0.021520\n",
      "Train Epoch: 19 [10720/11051 (97%)]\tLoss: 0.020016\n",
      "Train Epoch: 19 [10800/11051 (98%)]\tLoss: 0.004405\n",
      "Train Epoch: 19 [10880/11051 (98%)]\tLoss: 0.017237\n",
      "Train Epoch: 19 [10960/11051 (99%)]\tLoss: 0.036850\n",
      "Train Epoch: 19 [11040/11051 (100%)]\tLoss: 0.014228\n",
      "====> Epoch: 19 Average loss: 0.0220\n",
      "\n",
      "Started training epoch no. 21\n",
      "Train Epoch: 20 [0/11051 (0%)]\tLoss: 0.018353\n",
      "Train Epoch: 20 [80/11051 (1%)]\tLoss: 0.041909\n",
      "Train Epoch: 20 [160/11051 (1%)]\tLoss: 0.023429\n",
      "Train Epoch: 20 [240/11051 (2%)]\tLoss: 0.018944\n",
      "Train Epoch: 20 [320/11051 (3%)]\tLoss: 0.041660\n",
      "Train Epoch: 20 [400/11051 (4%)]\tLoss: 0.023352\n",
      "Train Epoch: 20 [480/11051 (4%)]\tLoss: 0.024388\n",
      "Train Epoch: 20 [560/11051 (5%)]\tLoss: 0.020126\n",
      "Train Epoch: 20 [640/11051 (6%)]\tLoss: 0.013901\n",
      "Train Epoch: 20 [720/11051 (7%)]\tLoss: 0.013239\n",
      "Train Epoch: 20 [800/11051 (7%)]\tLoss: 0.030044\n",
      "Train Epoch: 20 [880/11051 (8%)]\tLoss: 0.037323\n",
      "Train Epoch: 20 [960/11051 (9%)]\tLoss: 0.031385\n",
      "Train Epoch: 20 [1040/11051 (9%)]\tLoss: 0.026986\n",
      "Train Epoch: 20 [1120/11051 (10%)]\tLoss: 0.023947\n",
      "Train Epoch: 20 [1200/11051 (11%)]\tLoss: 0.022184\n",
      "Train Epoch: 20 [1280/11051 (12%)]\tLoss: 0.017208\n",
      "Train Epoch: 20 [1360/11051 (12%)]\tLoss: 0.031475\n",
      "Train Epoch: 20 [1440/11051 (13%)]\tLoss: 0.017430\n",
      "Train Epoch: 20 [1520/11051 (14%)]\tLoss: 0.018419\n",
      "Train Epoch: 20 [1600/11051 (14%)]\tLoss: 0.020949\n",
      "Train Epoch: 20 [1680/11051 (15%)]\tLoss: 0.027796\n",
      "Train Epoch: 20 [1760/11051 (16%)]\tLoss: 0.010800\n",
      "Train Epoch: 20 [1840/11051 (17%)]\tLoss: 0.021505\n",
      "Train Epoch: 20 [1920/11051 (17%)]\tLoss: 0.061694\n",
      "Train Epoch: 20 [2000/11051 (18%)]\tLoss: 0.026134\n",
      "Train Epoch: 20 [2080/11051 (19%)]\tLoss: 0.021592\n",
      "Train Epoch: 20 [2160/11051 (20%)]\tLoss: 0.017002\n",
      "Train Epoch: 20 [2240/11051 (20%)]\tLoss: 0.015263\n",
      "Train Epoch: 20 [2320/11051 (21%)]\tLoss: 0.022691\n",
      "Train Epoch: 20 [2400/11051 (22%)]\tLoss: 0.011569\n",
      "Train Epoch: 20 [2480/11051 (22%)]\tLoss: 0.007157\n",
      "Train Epoch: 20 [2560/11051 (23%)]\tLoss: 0.006843\n",
      "Train Epoch: 20 [2640/11051 (24%)]\tLoss: 0.012230\n",
      "Train Epoch: 20 [2720/11051 (25%)]\tLoss: 0.033480\n",
      "Train Epoch: 20 [2800/11051 (25%)]\tLoss: 0.018788\n",
      "Train Epoch: 20 [2880/11051 (26%)]\tLoss: 0.022285\n",
      "Train Epoch: 20 [2960/11051 (27%)]\tLoss: 0.046994\n",
      "Train Epoch: 20 [3040/11051 (27%)]\tLoss: 0.013648\n",
      "Train Epoch: 20 [3120/11051 (28%)]\tLoss: 0.022813\n",
      "Train Epoch: 20 [3200/11051 (29%)]\tLoss: 0.021007\n",
      "Train Epoch: 20 [3280/11051 (30%)]\tLoss: 0.022744\n",
      "Train Epoch: 20 [3360/11051 (30%)]\tLoss: 0.010082\n",
      "Train Epoch: 20 [3440/11051 (31%)]\tLoss: 0.016439\n",
      "Train Epoch: 20 [3520/11051 (32%)]\tLoss: 0.019265\n",
      "Train Epoch: 20 [3600/11051 (33%)]\tLoss: 0.011679\n",
      "Train Epoch: 20 [3680/11051 (33%)]\tLoss: 0.013468\n",
      "Train Epoch: 20 [3760/11051 (34%)]\tLoss: 0.023850\n",
      "Train Epoch: 20 [3840/11051 (35%)]\tLoss: 0.011521\n",
      "Train Epoch: 20 [3920/11051 (35%)]\tLoss: 0.028735\n",
      "Train Epoch: 20 [4000/11051 (36%)]\tLoss: 0.016879\n",
      "Train Epoch: 20 [4080/11051 (37%)]\tLoss: 0.021844\n",
      "Train Epoch: 20 [4160/11051 (38%)]\tLoss: 0.008621\n",
      "Train Epoch: 20 [4240/11051 (38%)]\tLoss: 0.020459\n",
      "Train Epoch: 20 [4320/11051 (39%)]\tLoss: 0.017218\n",
      "Train Epoch: 20 [4400/11051 (40%)]\tLoss: 0.020747\n",
      "Train Epoch: 20 [4480/11051 (41%)]\tLoss: 0.039124\n",
      "Train Epoch: 20 [4560/11051 (41%)]\tLoss: 0.021748\n",
      "Train Epoch: 20 [4640/11051 (42%)]\tLoss: 0.012058\n",
      "Train Epoch: 20 [4720/11051 (43%)]\tLoss: 0.016635\n",
      "Train Epoch: 20 [4800/11051 (43%)]\tLoss: 0.027811\n",
      "Train Epoch: 20 [4880/11051 (44%)]\tLoss: 0.017073\n",
      "Train Epoch: 20 [4960/11051 (45%)]\tLoss: 0.011179\n",
      "Train Epoch: 20 [5040/11051 (46%)]\tLoss: 0.010674\n",
      "Train Epoch: 20 [5120/11051 (46%)]\tLoss: 0.009852\n",
      "Train Epoch: 20 [5200/11051 (47%)]\tLoss: 0.035190\n",
      "Train Epoch: 20 [5280/11051 (48%)]\tLoss: 0.022579\n",
      "Train Epoch: 20 [5360/11051 (48%)]\tLoss: 0.016356\n",
      "Train Epoch: 20 [5440/11051 (49%)]\tLoss: 0.015939\n",
      "Train Epoch: 20 [5520/11051 (50%)]\tLoss: 0.023643\n",
      "Train Epoch: 20 [5600/11051 (51%)]\tLoss: 0.027645\n",
      "Train Epoch: 20 [5680/11051 (51%)]\tLoss: 0.025931\n",
      "Train Epoch: 20 [5760/11051 (52%)]\tLoss: 0.034563\n",
      "Train Epoch: 20 [5840/11051 (53%)]\tLoss: 0.021845\n",
      "Train Epoch: 20 [5920/11051 (54%)]\tLoss: 0.020680\n",
      "Train Epoch: 20 [6000/11051 (54%)]\tLoss: 0.024994\n",
      "Train Epoch: 20 [6080/11051 (55%)]\tLoss: 0.017619\n",
      "Train Epoch: 20 [6160/11051 (56%)]\tLoss: 0.029845\n",
      "Train Epoch: 20 [6240/11051 (56%)]\tLoss: 0.015916\n",
      "Train Epoch: 20 [6320/11051 (57%)]\tLoss: 0.031827\n",
      "Train Epoch: 20 [6400/11051 (58%)]\tLoss: 0.015522\n",
      "Train Epoch: 20 [6480/11051 (59%)]\tLoss: 0.031286\n",
      "Train Epoch: 20 [6560/11051 (59%)]\tLoss: 0.020511\n",
      "Train Epoch: 20 [6640/11051 (60%)]\tLoss: 0.013156\n",
      "Train Epoch: 20 [6720/11051 (61%)]\tLoss: 0.024551\n",
      "Train Epoch: 20 [6800/11051 (62%)]\tLoss: 0.010281\n",
      "Train Epoch: 20 [6880/11051 (62%)]\tLoss: 0.022042\n",
      "Train Epoch: 20 [6960/11051 (63%)]\tLoss: 0.027102\n",
      "Train Epoch: 20 [7040/11051 (64%)]\tLoss: 0.021630\n",
      "Train Epoch: 20 [7120/11051 (64%)]\tLoss: 0.016327\n",
      "Train Epoch: 20 [7200/11051 (65%)]\tLoss: 0.007573\n",
      "Train Epoch: 20 [7280/11051 (66%)]\tLoss: 0.045638\n",
      "Train Epoch: 20 [7360/11051 (67%)]\tLoss: 0.029489\n",
      "Train Epoch: 20 [7440/11051 (67%)]\tLoss: 0.021334\n",
      "Train Epoch: 20 [7520/11051 (68%)]\tLoss: 0.019114\n",
      "Train Epoch: 20 [7600/11051 (69%)]\tLoss: 0.026792\n",
      "Train Epoch: 20 [7680/11051 (69%)]\tLoss: 0.008189\n",
      "Train Epoch: 20 [7760/11051 (70%)]\tLoss: 0.021068\n",
      "Train Epoch: 20 [7840/11051 (71%)]\tLoss: 0.003278\n",
      "Train Epoch: 20 [7920/11051 (72%)]\tLoss: 0.015027\n",
      "Train Epoch: 20 [8000/11051 (72%)]\tLoss: 0.024988\n",
      "Train Epoch: 20 [8080/11051 (73%)]\tLoss: 0.011315\n",
      "Train Epoch: 20 [8160/11051 (74%)]\tLoss: 0.016574\n",
      "Train Epoch: 20 [8240/11051 (75%)]\tLoss: 0.010477\n",
      "Train Epoch: 20 [8320/11051 (75%)]\tLoss: 0.030054\n",
      "Train Epoch: 20 [8400/11051 (76%)]\tLoss: 0.022269\n",
      "Train Epoch: 20 [8480/11051 (77%)]\tLoss: 0.017417\n",
      "Train Epoch: 20 [8560/11051 (77%)]\tLoss: 0.007959\n",
      "Train Epoch: 20 [8640/11051 (78%)]\tLoss: 0.037854\n",
      "Train Epoch: 20 [8720/11051 (79%)]\tLoss: 0.024468\n",
      "Train Epoch: 20 [8800/11051 (80%)]\tLoss: 0.020678\n",
      "Train Epoch: 20 [8880/11051 (80%)]\tLoss: 0.022332\n",
      "Train Epoch: 20 [8960/11051 (81%)]\tLoss: 0.025573\n",
      "Train Epoch: 20 [9040/11051 (82%)]\tLoss: 0.009958\n",
      "Train Epoch: 20 [9120/11051 (82%)]\tLoss: 0.030055\n",
      "Train Epoch: 20 [9200/11051 (83%)]\tLoss: 0.027929\n",
      "Train Epoch: 20 [9280/11051 (84%)]\tLoss: 0.043881\n",
      "Train Epoch: 20 [9360/11051 (85%)]\tLoss: 0.015005\n",
      "Train Epoch: 20 [9440/11051 (85%)]\tLoss: 0.003948\n",
      "Train Epoch: 20 [9520/11051 (86%)]\tLoss: 0.017239\n",
      "Train Epoch: 20 [9600/11051 (87%)]\tLoss: 0.012513\n",
      "Train Epoch: 20 [9680/11051 (88%)]\tLoss: 0.018465\n",
      "Train Epoch: 20 [9760/11051 (88%)]\tLoss: 0.013224\n",
      "Train Epoch: 20 [9840/11051 (89%)]\tLoss: 0.045623\n",
      "Train Epoch: 20 [9920/11051 (90%)]\tLoss: 0.013376\n",
      "Train Epoch: 20 [10000/11051 (90%)]\tLoss: 0.027506\n",
      "Train Epoch: 20 [10080/11051 (91%)]\tLoss: 0.026139\n",
      "Train Epoch: 20 [10160/11051 (92%)]\tLoss: 0.030869\n",
      "Train Epoch: 20 [10240/11051 (93%)]\tLoss: 0.026012\n",
      "Train Epoch: 20 [10320/11051 (93%)]\tLoss: 0.034970\n",
      "Train Epoch: 20 [10400/11051 (94%)]\tLoss: 0.017498\n",
      "Train Epoch: 20 [10480/11051 (95%)]\tLoss: 0.021241\n",
      "Train Epoch: 20 [10560/11051 (96%)]\tLoss: 0.022169\n",
      "Train Epoch: 20 [10640/11051 (96%)]\tLoss: 0.015225\n",
      "Train Epoch: 20 [10720/11051 (97%)]\tLoss: 0.017110\n",
      "Train Epoch: 20 [10800/11051 (98%)]\tLoss: 0.021479\n",
      "Train Epoch: 20 [10880/11051 (98%)]\tLoss: 0.027522\n",
      "Train Epoch: 20 [10960/11051 (99%)]\tLoss: 0.014604\n",
      "Train Epoch: 20 [11040/11051 (100%)]\tLoss: 0.013549\n",
      "====> Epoch: 20 Average loss: 0.0219\n",
      "\n",
      "Started training epoch no. 22\n",
      "Train Epoch: 21 [0/11051 (0%)]\tLoss: 0.021942\n",
      "Train Epoch: 21 [80/11051 (1%)]\tLoss: 0.025335\n",
      "Train Epoch: 21 [160/11051 (1%)]\tLoss: 0.025209\n",
      "Train Epoch: 21 [240/11051 (2%)]\tLoss: 0.017095\n",
      "Train Epoch: 21 [320/11051 (3%)]\tLoss: 0.034604\n",
      "Train Epoch: 21 [400/11051 (4%)]\tLoss: 0.019937\n",
      "Train Epoch: 21 [480/11051 (4%)]\tLoss: 0.018699\n",
      "Train Epoch: 21 [560/11051 (5%)]\tLoss: 0.012154\n",
      "Train Epoch: 21 [640/11051 (6%)]\tLoss: 0.040347\n",
      "Train Epoch: 21 [720/11051 (7%)]\tLoss: 0.016330\n",
      "Train Epoch: 21 [800/11051 (7%)]\tLoss: 0.012797\n",
      "Train Epoch: 21 [880/11051 (8%)]\tLoss: 0.018043\n",
      "Train Epoch: 21 [960/11051 (9%)]\tLoss: 0.020789\n",
      "Train Epoch: 21 [1040/11051 (9%)]\tLoss: 0.047411\n",
      "Train Epoch: 21 [1120/11051 (10%)]\tLoss: 0.020723\n",
      "Train Epoch: 21 [1200/11051 (11%)]\tLoss: 0.036034\n",
      "Train Epoch: 21 [1280/11051 (12%)]\tLoss: 0.035524\n",
      "Train Epoch: 21 [1360/11051 (12%)]\tLoss: 0.029692\n",
      "Train Epoch: 21 [1440/11051 (13%)]\tLoss: 0.005260\n",
      "Train Epoch: 21 [1520/11051 (14%)]\tLoss: 0.015942\n",
      "Train Epoch: 21 [1600/11051 (14%)]\tLoss: 0.011385\n",
      "Train Epoch: 21 [1680/11051 (15%)]\tLoss: 0.018588\n",
      "Train Epoch: 21 [1760/11051 (16%)]\tLoss: 0.018957\n",
      "Train Epoch: 21 [1840/11051 (17%)]\tLoss: 0.019770\n",
      "Train Epoch: 21 [1920/11051 (17%)]\tLoss: 0.027036\n",
      "Train Epoch: 21 [2000/11051 (18%)]\tLoss: 0.013729\n",
      "Train Epoch: 21 [2080/11051 (19%)]\tLoss: 0.014790\n",
      "Train Epoch: 21 [2160/11051 (20%)]\tLoss: 0.030485\n",
      "Train Epoch: 21 [2240/11051 (20%)]\tLoss: 0.019067\n",
      "Train Epoch: 21 [2320/11051 (21%)]\tLoss: 0.021994\n",
      "Train Epoch: 21 [2400/11051 (22%)]\tLoss: 0.018696\n",
      "Train Epoch: 21 [2480/11051 (22%)]\tLoss: 0.022850\n",
      "Train Epoch: 21 [2560/11051 (23%)]\tLoss: 0.032744\n",
      "Train Epoch: 21 [2640/11051 (24%)]\tLoss: 0.012456\n",
      "Train Epoch: 21 [2720/11051 (25%)]\tLoss: 0.019274\n",
      "Train Epoch: 21 [2800/11051 (25%)]\tLoss: 0.013342\n",
      "Train Epoch: 21 [2880/11051 (26%)]\tLoss: 0.004968\n",
      "Train Epoch: 21 [2960/11051 (27%)]\tLoss: 0.019988\n",
      "Train Epoch: 21 [3040/11051 (27%)]\tLoss: 0.035794\n",
      "Train Epoch: 21 [3120/11051 (28%)]\tLoss: 0.026400\n",
      "Train Epoch: 21 [3200/11051 (29%)]\tLoss: 0.023663\n",
      "Train Epoch: 21 [3280/11051 (30%)]\tLoss: 0.025384\n",
      "Train Epoch: 21 [3360/11051 (30%)]\tLoss: 0.012717\n",
      "Train Epoch: 21 [3440/11051 (31%)]\tLoss: 0.031083\n",
      "Train Epoch: 21 [3520/11051 (32%)]\tLoss: 0.029036\n",
      "Train Epoch: 21 [3600/11051 (33%)]\tLoss: 0.026130\n",
      "Train Epoch: 21 [3680/11051 (33%)]\tLoss: 0.012849\n",
      "Train Epoch: 21 [3760/11051 (34%)]\tLoss: 0.020271\n",
      "Train Epoch: 21 [3840/11051 (35%)]\tLoss: 0.004395\n",
      "Train Epoch: 21 [3920/11051 (35%)]\tLoss: 0.018287\n",
      "Train Epoch: 21 [4000/11051 (36%)]\tLoss: 0.024078\n",
      "Train Epoch: 21 [4080/11051 (37%)]\tLoss: 0.030758\n",
      "Train Epoch: 21 [4160/11051 (38%)]\tLoss: 0.010440\n",
      "Train Epoch: 21 [4240/11051 (38%)]\tLoss: 0.019628\n",
      "Train Epoch: 21 [4320/11051 (39%)]\tLoss: 0.024846\n",
      "Train Epoch: 21 [4400/11051 (40%)]\tLoss: 0.028227\n",
      "Train Epoch: 21 [4480/11051 (41%)]\tLoss: 0.014518\n",
      "Train Epoch: 21 [4560/11051 (41%)]\tLoss: 0.017120\n",
      "Train Epoch: 21 [4640/11051 (42%)]\tLoss: 0.012757\n",
      "Train Epoch: 21 [4720/11051 (43%)]\tLoss: 0.013266\n",
      "Train Epoch: 21 [4800/11051 (43%)]\tLoss: 0.025479\n",
      "Train Epoch: 21 [4880/11051 (44%)]\tLoss: 0.026615\n",
      "Train Epoch: 21 [4960/11051 (45%)]\tLoss: 0.024175\n",
      "Train Epoch: 21 [5040/11051 (46%)]\tLoss: 0.030147\n",
      "Train Epoch: 21 [5120/11051 (46%)]\tLoss: 0.039579\n",
      "Train Epoch: 21 [5200/11051 (47%)]\tLoss: 0.035220\n",
      "Train Epoch: 21 [5280/11051 (48%)]\tLoss: 0.051760\n",
      "Train Epoch: 21 [5360/11051 (48%)]\tLoss: 0.031923\n",
      "Train Epoch: 21 [5440/11051 (49%)]\tLoss: 0.008821\n",
      "Train Epoch: 21 [5520/11051 (50%)]\tLoss: 0.014789\n",
      "Train Epoch: 21 [5600/11051 (51%)]\tLoss: 0.019684\n",
      "Train Epoch: 21 [5680/11051 (51%)]\tLoss: 0.010473\n",
      "Train Epoch: 21 [5760/11051 (52%)]\tLoss: 0.026839\n",
      "Train Epoch: 21 [5840/11051 (53%)]\tLoss: 0.011960\n",
      "Train Epoch: 21 [5920/11051 (54%)]\tLoss: 0.015643\n",
      "Train Epoch: 21 [6000/11051 (54%)]\tLoss: 0.021245\n",
      "Train Epoch: 21 [6080/11051 (55%)]\tLoss: 0.010537\n",
      "Train Epoch: 21 [6160/11051 (56%)]\tLoss: 0.022281\n",
      "Train Epoch: 21 [6240/11051 (56%)]\tLoss: 0.027333\n",
      "Train Epoch: 21 [6320/11051 (57%)]\tLoss: 0.026743\n",
      "Train Epoch: 21 [6400/11051 (58%)]\tLoss: 0.023236\n",
      "Train Epoch: 21 [6480/11051 (59%)]\tLoss: 0.029808\n",
      "Train Epoch: 21 [6560/11051 (59%)]\tLoss: 0.005785\n",
      "Train Epoch: 21 [6640/11051 (60%)]\tLoss: 0.010511\n",
      "Train Epoch: 21 [6720/11051 (61%)]\tLoss: 0.021316\n",
      "Train Epoch: 21 [6800/11051 (62%)]\tLoss: 0.022649\n",
      "Train Epoch: 21 [6880/11051 (62%)]\tLoss: 0.053720\n",
      "Train Epoch: 21 [6960/11051 (63%)]\tLoss: 0.018922\n",
      "Train Epoch: 21 [7040/11051 (64%)]\tLoss: 0.035320\n",
      "Train Epoch: 21 [7120/11051 (64%)]\tLoss: 0.024871\n",
      "Train Epoch: 21 [7200/11051 (65%)]\tLoss: 0.021955\n",
      "Train Epoch: 21 [7280/11051 (66%)]\tLoss: 0.014548\n",
      "Train Epoch: 21 [7360/11051 (67%)]\tLoss: 0.024425\n",
      "Train Epoch: 21 [7440/11051 (67%)]\tLoss: 0.016882\n",
      "Train Epoch: 21 [7520/11051 (68%)]\tLoss: 0.019017\n",
      "Train Epoch: 21 [7600/11051 (69%)]\tLoss: 0.027618\n",
      "Train Epoch: 21 [7680/11051 (69%)]\tLoss: 0.014772\n",
      "Train Epoch: 21 [7760/11051 (70%)]\tLoss: 0.013290\n",
      "Train Epoch: 21 [7840/11051 (71%)]\tLoss: 0.012293\n",
      "Train Epoch: 21 [7920/11051 (72%)]\tLoss: 0.029106\n",
      "Train Epoch: 21 [8000/11051 (72%)]\tLoss: 0.025160\n",
      "Train Epoch: 21 [8080/11051 (73%)]\tLoss: 0.009380\n",
      "Train Epoch: 21 [8160/11051 (74%)]\tLoss: 0.009460\n",
      "Train Epoch: 21 [8240/11051 (75%)]\tLoss: 0.032161\n",
      "Train Epoch: 21 [8320/11051 (75%)]\tLoss: 0.012562\n",
      "Train Epoch: 21 [8400/11051 (76%)]\tLoss: 0.022326\n",
      "Train Epoch: 21 [8480/11051 (77%)]\tLoss: 0.005224\n",
      "Train Epoch: 21 [8560/11051 (77%)]\tLoss: 0.021788\n",
      "Train Epoch: 21 [8640/11051 (78%)]\tLoss: 0.007368\n",
      "Train Epoch: 21 [8720/11051 (79%)]\tLoss: 0.016768\n",
      "Train Epoch: 21 [8800/11051 (80%)]\tLoss: 0.023839\n",
      "Train Epoch: 21 [8880/11051 (80%)]\tLoss: 0.018768\n",
      "Train Epoch: 21 [8960/11051 (81%)]\tLoss: 0.026079\n",
      "Train Epoch: 21 [9040/11051 (82%)]\tLoss: 0.023203\n",
      "Train Epoch: 21 [9120/11051 (82%)]\tLoss: 0.017136\n",
      "Train Epoch: 21 [9200/11051 (83%)]\tLoss: 0.023153\n",
      "Train Epoch: 21 [9280/11051 (84%)]\tLoss: 0.031298\n",
      "Train Epoch: 21 [9360/11051 (85%)]\tLoss: 0.020350\n",
      "Train Epoch: 21 [9440/11051 (85%)]\tLoss: 0.015460\n",
      "Train Epoch: 21 [9520/11051 (86%)]\tLoss: 0.024036\n",
      "Train Epoch: 21 [9600/11051 (87%)]\tLoss: 0.028020\n",
      "Train Epoch: 21 [9680/11051 (88%)]\tLoss: 0.014803\n",
      "Train Epoch: 21 [9760/11051 (88%)]\tLoss: 0.013502\n",
      "Train Epoch: 21 [9840/11051 (89%)]\tLoss: 0.022410\n",
      "Train Epoch: 21 [9920/11051 (90%)]\tLoss: 0.029567\n",
      "Train Epoch: 21 [10000/11051 (90%)]\tLoss: 0.011456\n",
      "Train Epoch: 21 [10080/11051 (91%)]\tLoss: 0.014259\n",
      "Train Epoch: 21 [10160/11051 (92%)]\tLoss: 0.022305\n",
      "Train Epoch: 21 [10240/11051 (93%)]\tLoss: 0.012348\n",
      "Train Epoch: 21 [10320/11051 (93%)]\tLoss: 0.031639\n",
      "Train Epoch: 21 [10400/11051 (94%)]\tLoss: 0.012879\n",
      "Train Epoch: 21 [10480/11051 (95%)]\tLoss: 0.022883\n",
      "Train Epoch: 21 [10560/11051 (96%)]\tLoss: 0.012627\n",
      "Train Epoch: 21 [10640/11051 (96%)]\tLoss: 0.019424\n",
      "Train Epoch: 21 [10720/11051 (97%)]\tLoss: 0.008995\n",
      "Train Epoch: 21 [10800/11051 (98%)]\tLoss: 0.034588\n",
      "Train Epoch: 21 [10880/11051 (98%)]\tLoss: 0.017675\n",
      "Train Epoch: 21 [10960/11051 (99%)]\tLoss: 0.024535\n",
      "Train Epoch: 21 [11040/11051 (100%)]\tLoss: 0.013125\n",
      "====> Epoch: 21 Average loss: 0.0219\n",
      "\n",
      "Started training epoch no. 23\n",
      "Train Epoch: 22 [0/11051 (0%)]\tLoss: 0.009267\n",
      "Train Epoch: 22 [80/11051 (1%)]\tLoss: 0.006123\n",
      "Train Epoch: 22 [160/11051 (1%)]\tLoss: 0.016436\n",
      "Train Epoch: 22 [240/11051 (2%)]\tLoss: 0.018732\n",
      "Train Epoch: 22 [320/11051 (3%)]\tLoss: 0.015594\n",
      "Train Epoch: 22 [400/11051 (4%)]\tLoss: 0.010455\n",
      "Train Epoch: 22 [480/11051 (4%)]\tLoss: 0.023199\n",
      "Train Epoch: 22 [560/11051 (5%)]\tLoss: 0.031236\n",
      "Train Epoch: 22 [640/11051 (6%)]\tLoss: 0.029924\n",
      "Train Epoch: 22 [720/11051 (7%)]\tLoss: 0.022008\n",
      "Train Epoch: 22 [800/11051 (7%)]\tLoss: 0.021026\n",
      "Train Epoch: 22 [880/11051 (8%)]\tLoss: 0.015272\n",
      "Train Epoch: 22 [960/11051 (9%)]\tLoss: 0.022143\n",
      "Train Epoch: 22 [1040/11051 (9%)]\tLoss: 0.004244\n",
      "Train Epoch: 22 [1120/11051 (10%)]\tLoss: 0.040833\n",
      "Train Epoch: 22 [1200/11051 (11%)]\tLoss: 0.010854\n",
      "Train Epoch: 22 [1280/11051 (12%)]\tLoss: 0.033429\n",
      "Train Epoch: 22 [1360/11051 (12%)]\tLoss: 0.017241\n",
      "Train Epoch: 22 [1440/11051 (13%)]\tLoss: 0.014871\n",
      "Train Epoch: 22 [1520/11051 (14%)]\tLoss: 0.010289\n",
      "Train Epoch: 22 [1600/11051 (14%)]\tLoss: 0.021041\n",
      "Train Epoch: 22 [1680/11051 (15%)]\tLoss: 0.023788\n",
      "Train Epoch: 22 [1760/11051 (16%)]\tLoss: 0.016877\n",
      "Train Epoch: 22 [1840/11051 (17%)]\tLoss: 0.011831\n",
      "Train Epoch: 22 [1920/11051 (17%)]\tLoss: 0.037284\n",
      "Train Epoch: 22 [2000/11051 (18%)]\tLoss: 0.034822\n",
      "Train Epoch: 22 [2080/11051 (19%)]\tLoss: 0.028845\n",
      "Train Epoch: 22 [2160/11051 (20%)]\tLoss: 0.024286\n",
      "Train Epoch: 22 [2240/11051 (20%)]\tLoss: 0.012989\n",
      "Train Epoch: 22 [2320/11051 (21%)]\tLoss: 0.024222\n",
      "Train Epoch: 22 [2400/11051 (22%)]\tLoss: 0.018676\n",
      "Train Epoch: 22 [2480/11051 (22%)]\tLoss: 0.022677\n",
      "Train Epoch: 22 [2560/11051 (23%)]\tLoss: 0.022700\n",
      "Train Epoch: 22 [2640/11051 (24%)]\tLoss: 0.015613\n",
      "Train Epoch: 22 [2720/11051 (25%)]\tLoss: 0.019692\n",
      "Train Epoch: 22 [2800/11051 (25%)]\tLoss: 0.033266\n",
      "Train Epoch: 22 [2880/11051 (26%)]\tLoss: 0.031343\n",
      "Train Epoch: 22 [2960/11051 (27%)]\tLoss: 0.014585\n",
      "Train Epoch: 22 [3040/11051 (27%)]\tLoss: 0.021514\n",
      "Train Epoch: 22 [3120/11051 (28%)]\tLoss: 0.014003\n",
      "Train Epoch: 22 [3200/11051 (29%)]\tLoss: 0.009667\n",
      "Train Epoch: 22 [3280/11051 (30%)]\tLoss: 0.013914\n",
      "Train Epoch: 22 [3360/11051 (30%)]\tLoss: 0.017963\n",
      "Train Epoch: 22 [3440/11051 (31%)]\tLoss: 0.016675\n",
      "Train Epoch: 22 [3520/11051 (32%)]\tLoss: 0.024106\n",
      "Train Epoch: 22 [3600/11051 (33%)]\tLoss: 0.019655\n",
      "Train Epoch: 22 [3680/11051 (33%)]\tLoss: 0.059762\n",
      "Train Epoch: 22 [3760/11051 (34%)]\tLoss: 0.029255\n",
      "Train Epoch: 22 [3840/11051 (35%)]\tLoss: 0.011026\n",
      "Train Epoch: 22 [3920/11051 (35%)]\tLoss: 0.023707\n",
      "Train Epoch: 22 [4000/11051 (36%)]\tLoss: 0.022727\n",
      "Train Epoch: 22 [4080/11051 (37%)]\tLoss: 0.014737\n",
      "Train Epoch: 22 [4160/11051 (38%)]\tLoss: 0.005269\n",
      "Train Epoch: 22 [4240/11051 (38%)]\tLoss: 0.022083\n",
      "Train Epoch: 22 [4320/11051 (39%)]\tLoss: 0.020889\n",
      "Train Epoch: 22 [4400/11051 (40%)]\tLoss: 0.016090\n",
      "Train Epoch: 22 [4480/11051 (41%)]\tLoss: 0.016189\n",
      "Train Epoch: 22 [4560/11051 (41%)]\tLoss: 0.019396\n",
      "Train Epoch: 22 [4640/11051 (42%)]\tLoss: 0.014230\n",
      "Train Epoch: 22 [4720/11051 (43%)]\tLoss: 0.013418\n",
      "Train Epoch: 22 [4800/11051 (43%)]\tLoss: 0.032856\n",
      "Train Epoch: 22 [4880/11051 (44%)]\tLoss: 0.023381\n",
      "Train Epoch: 22 [4960/11051 (45%)]\tLoss: 0.015683\n",
      "Train Epoch: 22 [5040/11051 (46%)]\tLoss: 0.027213\n",
      "Train Epoch: 22 [5120/11051 (46%)]\tLoss: 0.040726\n",
      "Train Epoch: 22 [5200/11051 (47%)]\tLoss: 0.089160\n",
      "Train Epoch: 22 [5280/11051 (48%)]\tLoss: 0.022916\n",
      "Train Epoch: 22 [5360/11051 (48%)]\tLoss: 0.021051\n",
      "Train Epoch: 22 [5440/11051 (49%)]\tLoss: 0.017306\n",
      "Train Epoch: 22 [5520/11051 (50%)]\tLoss: 0.010791\n",
      "Train Epoch: 22 [5600/11051 (51%)]\tLoss: 0.015338\n",
      "Train Epoch: 22 [5680/11051 (51%)]\tLoss: 0.020800\n",
      "Train Epoch: 22 [5760/11051 (52%)]\tLoss: 0.031703\n",
      "Train Epoch: 22 [5840/11051 (53%)]\tLoss: 0.023285\n",
      "Train Epoch: 22 [5920/11051 (54%)]\tLoss: 0.032106\n",
      "Train Epoch: 22 [6000/11051 (54%)]\tLoss: 0.006693\n",
      "Train Epoch: 22 [6080/11051 (55%)]\tLoss: 0.036578\n",
      "Train Epoch: 22 [6160/11051 (56%)]\tLoss: 0.017219\n",
      "Train Epoch: 22 [6240/11051 (56%)]\tLoss: 0.018544\n",
      "Train Epoch: 22 [6320/11051 (57%)]\tLoss: 0.016449\n",
      "Train Epoch: 22 [6400/11051 (58%)]\tLoss: 0.011034\n",
      "Train Epoch: 22 [6480/11051 (59%)]\tLoss: 0.016634\n",
      "Train Epoch: 22 [6560/11051 (59%)]\tLoss: 0.032626\n",
      "Train Epoch: 22 [6640/11051 (60%)]\tLoss: 0.019125\n",
      "Train Epoch: 22 [6720/11051 (61%)]\tLoss: 0.008364\n",
      "Train Epoch: 22 [6800/11051 (62%)]\tLoss: 0.015857\n",
      "Train Epoch: 22 [6880/11051 (62%)]\tLoss: 0.023966\n",
      "Train Epoch: 22 [6960/11051 (63%)]\tLoss: 0.029473\n",
      "Train Epoch: 22 [7040/11051 (64%)]\tLoss: 0.015440\n",
      "Train Epoch: 22 [7120/11051 (64%)]\tLoss: 0.018144\n",
      "Train Epoch: 22 [7200/11051 (65%)]\tLoss: 0.030164\n",
      "Train Epoch: 22 [7280/11051 (66%)]\tLoss: 0.017792\n",
      "Train Epoch: 22 [7360/11051 (67%)]\tLoss: 0.017999\n",
      "Train Epoch: 22 [7440/11051 (67%)]\tLoss: 0.014223\n",
      "Train Epoch: 22 [7520/11051 (68%)]\tLoss: 0.022142\n",
      "Train Epoch: 22 [7600/11051 (69%)]\tLoss: 0.011676\n",
      "Train Epoch: 22 [7680/11051 (69%)]\tLoss: 0.016900\n",
      "Train Epoch: 22 [7760/11051 (70%)]\tLoss: 0.018297\n",
      "Train Epoch: 22 [7840/11051 (71%)]\tLoss: 0.022361\n",
      "Train Epoch: 22 [7920/11051 (72%)]\tLoss: 0.013921\n",
      "Train Epoch: 22 [8000/11051 (72%)]\tLoss: 0.057790\n",
      "Train Epoch: 22 [8080/11051 (73%)]\tLoss: 0.020575\n",
      "Train Epoch: 22 [8160/11051 (74%)]\tLoss: 0.017396\n",
      "Train Epoch: 22 [8240/11051 (75%)]\tLoss: 0.010428\n",
      "Train Epoch: 22 [8320/11051 (75%)]\tLoss: 0.010962\n",
      "Train Epoch: 22 [8400/11051 (76%)]\tLoss: 0.025476\n",
      "Train Epoch: 22 [8480/11051 (77%)]\tLoss: 0.024236\n",
      "Train Epoch: 22 [8560/11051 (77%)]\tLoss: 0.009319\n",
      "Train Epoch: 22 [8640/11051 (78%)]\tLoss: 0.018137\n",
      "Train Epoch: 22 [8720/11051 (79%)]\tLoss: 0.007679\n",
      "Train Epoch: 22 [8800/11051 (80%)]\tLoss: 0.011841\n",
      "Train Epoch: 22 [8880/11051 (80%)]\tLoss: 0.023359\n",
      "Train Epoch: 22 [8960/11051 (81%)]\tLoss: 0.019233\n",
      "Train Epoch: 22 [9040/11051 (82%)]\tLoss: 0.011925\n",
      "Train Epoch: 22 [9120/11051 (82%)]\tLoss: 0.027247\n",
      "Train Epoch: 22 [9200/11051 (83%)]\tLoss: 0.012018\n",
      "Train Epoch: 22 [9280/11051 (84%)]\tLoss: 0.016195\n",
      "Train Epoch: 22 [9360/11051 (85%)]\tLoss: 0.045167\n",
      "Train Epoch: 22 [9440/11051 (85%)]\tLoss: 0.025110\n",
      "Train Epoch: 22 [9520/11051 (86%)]\tLoss: 0.008959\n",
      "Train Epoch: 22 [9600/11051 (87%)]\tLoss: 0.015588\n",
      "Train Epoch: 22 [9680/11051 (88%)]\tLoss: 0.016558\n",
      "Train Epoch: 22 [9760/11051 (88%)]\tLoss: 0.033007\n",
      "Train Epoch: 22 [9840/11051 (89%)]\tLoss: 0.040074\n",
      "Train Epoch: 22 [9920/11051 (90%)]\tLoss: 0.009686\n",
      "Train Epoch: 22 [10000/11051 (90%)]\tLoss: 0.024430\n",
      "Train Epoch: 22 [10080/11051 (91%)]\tLoss: 0.031863\n",
      "Train Epoch: 22 [10160/11051 (92%)]\tLoss: 0.020166\n",
      "Train Epoch: 22 [10240/11051 (93%)]\tLoss: 0.011018\n",
      "Train Epoch: 22 [10320/11051 (93%)]\tLoss: 0.014943\n",
      "Train Epoch: 22 [10400/11051 (94%)]\tLoss: 0.010314\n",
      "Train Epoch: 22 [10480/11051 (95%)]\tLoss: 0.034953\n",
      "Train Epoch: 22 [10560/11051 (96%)]\tLoss: 0.016753\n",
      "Train Epoch: 22 [10640/11051 (96%)]\tLoss: 0.077976\n",
      "Train Epoch: 22 [10720/11051 (97%)]\tLoss: 0.020861\n",
      "Train Epoch: 22 [10800/11051 (98%)]\tLoss: 0.031522\n",
      "Train Epoch: 22 [10880/11051 (98%)]\tLoss: 0.011554\n",
      "Train Epoch: 22 [10960/11051 (99%)]\tLoss: 0.019957\n",
      "Train Epoch: 22 [11040/11051 (100%)]\tLoss: 0.017720\n",
      "====> Epoch: 22 Average loss: 0.0218\n",
      "\n",
      "Started training epoch no. 24\n",
      "Train Epoch: 23 [0/11051 (0%)]\tLoss: 0.036446\n",
      "Train Epoch: 23 [80/11051 (1%)]\tLoss: 0.011566\n",
      "Train Epoch: 23 [160/11051 (1%)]\tLoss: 0.029794\n",
      "Train Epoch: 23 [240/11051 (2%)]\tLoss: 0.025860\n",
      "Train Epoch: 23 [320/11051 (3%)]\tLoss: 0.041941\n",
      "Train Epoch: 23 [400/11051 (4%)]\tLoss: 0.019735\n",
      "Train Epoch: 23 [480/11051 (4%)]\tLoss: 0.014494\n",
      "Train Epoch: 23 [560/11051 (5%)]\tLoss: 0.036922\n",
      "Train Epoch: 23 [640/11051 (6%)]\tLoss: 0.009783\n",
      "Train Epoch: 23 [720/11051 (7%)]\tLoss: 0.027375\n",
      "Train Epoch: 23 [800/11051 (7%)]\tLoss: 0.022944\n",
      "Train Epoch: 23 [880/11051 (8%)]\tLoss: 0.035181\n",
      "Train Epoch: 23 [960/11051 (9%)]\tLoss: 0.021067\n",
      "Train Epoch: 23 [1040/11051 (9%)]\tLoss: 0.017279\n",
      "Train Epoch: 23 [1120/11051 (10%)]\tLoss: 0.006689\n",
      "Train Epoch: 23 [1200/11051 (11%)]\tLoss: 0.024599\n",
      "Train Epoch: 23 [1280/11051 (12%)]\tLoss: 0.005666\n",
      "Train Epoch: 23 [1360/11051 (12%)]\tLoss: 0.024825\n",
      "Train Epoch: 23 [1440/11051 (13%)]\tLoss: 0.033081\n",
      "Train Epoch: 23 [1520/11051 (14%)]\tLoss: 0.029842\n",
      "Train Epoch: 23 [1600/11051 (14%)]\tLoss: 0.020483\n",
      "Train Epoch: 23 [1680/11051 (15%)]\tLoss: 0.028412\n",
      "Train Epoch: 23 [1760/11051 (16%)]\tLoss: 0.018904\n",
      "Train Epoch: 23 [1840/11051 (17%)]\tLoss: 0.015513\n",
      "Train Epoch: 23 [1920/11051 (17%)]\tLoss: 0.008273\n",
      "Train Epoch: 23 [2000/11051 (18%)]\tLoss: 0.017420\n",
      "Train Epoch: 23 [2080/11051 (19%)]\tLoss: 0.015066\n",
      "Train Epoch: 23 [2160/11051 (20%)]\tLoss: 0.020496\n",
      "Train Epoch: 23 [2240/11051 (20%)]\tLoss: 0.013757\n",
      "Train Epoch: 23 [2320/11051 (21%)]\tLoss: 0.028341\n",
      "Train Epoch: 23 [2400/11051 (22%)]\tLoss: 0.013493\n",
      "Train Epoch: 23 [2480/11051 (22%)]\tLoss: 0.016517\n",
      "Train Epoch: 23 [2560/11051 (23%)]\tLoss: 0.016844\n",
      "Train Epoch: 23 [2640/11051 (24%)]\tLoss: 0.022983\n",
      "Train Epoch: 23 [2720/11051 (25%)]\tLoss: 0.017712\n",
      "Train Epoch: 23 [2800/11051 (25%)]\tLoss: 0.005520\n",
      "Train Epoch: 23 [2880/11051 (26%)]\tLoss: 0.025599\n",
      "Train Epoch: 23 [2960/11051 (27%)]\tLoss: 0.022026\n",
      "Train Epoch: 23 [3040/11051 (27%)]\tLoss: 0.016860\n",
      "Train Epoch: 23 [3120/11051 (28%)]\tLoss: 0.012926\n",
      "Train Epoch: 23 [3200/11051 (29%)]\tLoss: 0.014658\n",
      "Train Epoch: 23 [3280/11051 (30%)]\tLoss: 0.032844\n",
      "Train Epoch: 23 [3360/11051 (30%)]\tLoss: 0.030069\n",
      "Train Epoch: 23 [3440/11051 (31%)]\tLoss: 0.023366\n",
      "Train Epoch: 23 [3520/11051 (32%)]\tLoss: 0.012318\n",
      "Train Epoch: 23 [3600/11051 (33%)]\tLoss: 0.007313\n",
      "Train Epoch: 23 [3680/11051 (33%)]\tLoss: 0.027561\n",
      "Train Epoch: 23 [3760/11051 (34%)]\tLoss: 0.021772\n",
      "Train Epoch: 23 [3840/11051 (35%)]\tLoss: 0.015474\n",
      "Train Epoch: 23 [3920/11051 (35%)]\tLoss: 0.030064\n",
      "Train Epoch: 23 [4000/11051 (36%)]\tLoss: 0.046882\n",
      "Train Epoch: 23 [4080/11051 (37%)]\tLoss: 0.029134\n",
      "Train Epoch: 23 [4160/11051 (38%)]\tLoss: 0.020678\n",
      "Train Epoch: 23 [4240/11051 (38%)]\tLoss: 0.017444\n",
      "Train Epoch: 23 [4320/11051 (39%)]\tLoss: 0.019617\n",
      "Train Epoch: 23 [4400/11051 (40%)]\tLoss: 0.009375\n",
      "Train Epoch: 23 [4480/11051 (41%)]\tLoss: 0.015718\n",
      "Train Epoch: 23 [4560/11051 (41%)]\tLoss: 0.005269\n",
      "Train Epoch: 23 [4640/11051 (42%)]\tLoss: 0.033080\n",
      "Train Epoch: 23 [4720/11051 (43%)]\tLoss: 0.010645\n",
      "Train Epoch: 23 [4800/11051 (43%)]\tLoss: 0.024519\n",
      "Train Epoch: 23 [4880/11051 (44%)]\tLoss: 0.008424\n",
      "Train Epoch: 23 [4960/11051 (45%)]\tLoss: 0.012920\n",
      "Train Epoch: 23 [5040/11051 (46%)]\tLoss: 0.020270\n",
      "Train Epoch: 23 [5120/11051 (46%)]\tLoss: 0.044255\n",
      "Train Epoch: 23 [5200/11051 (47%)]\tLoss: 0.030360\n",
      "Train Epoch: 23 [5280/11051 (48%)]\tLoss: 0.013087\n",
      "Train Epoch: 23 [5360/11051 (48%)]\tLoss: 0.012998\n",
      "Train Epoch: 23 [5440/11051 (49%)]\tLoss: 0.011454\n",
      "Train Epoch: 23 [5520/11051 (50%)]\tLoss: 0.018269\n",
      "Train Epoch: 23 [5600/11051 (51%)]\tLoss: 0.009707\n",
      "Train Epoch: 23 [5680/11051 (51%)]\tLoss: 0.010353\n",
      "Train Epoch: 23 [5760/11051 (52%)]\tLoss: 0.022684\n",
      "Train Epoch: 23 [5840/11051 (53%)]\tLoss: 0.009421\n",
      "Train Epoch: 23 [5920/11051 (54%)]\tLoss: 0.025552\n",
      "Train Epoch: 23 [6000/11051 (54%)]\tLoss: 0.043617\n",
      "Train Epoch: 23 [6080/11051 (55%)]\tLoss: 0.017205\n",
      "Train Epoch: 23 [6160/11051 (56%)]\tLoss: 0.016742\n",
      "Train Epoch: 23 [6240/11051 (56%)]\tLoss: 0.008075\n",
      "Train Epoch: 23 [6320/11051 (57%)]\tLoss: 0.017712\n",
      "Train Epoch: 23 [6400/11051 (58%)]\tLoss: 0.014387\n",
      "Train Epoch: 23 [6480/11051 (59%)]\tLoss: 0.036308\n",
      "Train Epoch: 23 [6560/11051 (59%)]\tLoss: 0.021665\n",
      "Train Epoch: 23 [6640/11051 (60%)]\tLoss: 0.019524\n",
      "Train Epoch: 23 [6720/11051 (61%)]\tLoss: 0.017511\n",
      "Train Epoch: 23 [6800/11051 (62%)]\tLoss: 0.025295\n",
      "Train Epoch: 23 [6880/11051 (62%)]\tLoss: 0.015800\n",
      "Train Epoch: 23 [6960/11051 (63%)]\tLoss: 0.010645\n",
      "Train Epoch: 23 [7040/11051 (64%)]\tLoss: 0.051609\n",
      "Train Epoch: 23 [7120/11051 (64%)]\tLoss: 0.010860\n",
      "Train Epoch: 23 [7200/11051 (65%)]\tLoss: 0.021707\n",
      "Train Epoch: 23 [7280/11051 (66%)]\tLoss: 0.024574\n",
      "Train Epoch: 23 [7360/11051 (67%)]\tLoss: 0.010689\n",
      "Train Epoch: 23 [7440/11051 (67%)]\tLoss: 0.030628\n",
      "Train Epoch: 23 [7520/11051 (68%)]\tLoss: 0.014978\n",
      "Train Epoch: 23 [7600/11051 (69%)]\tLoss: 0.016913\n",
      "Train Epoch: 23 [7680/11051 (69%)]\tLoss: 0.011221\n",
      "Train Epoch: 23 [7760/11051 (70%)]\tLoss: 0.023861\n",
      "Train Epoch: 23 [7840/11051 (71%)]\tLoss: 0.014643\n",
      "Train Epoch: 23 [7920/11051 (72%)]\tLoss: 0.014985\n",
      "Train Epoch: 23 [8000/11051 (72%)]\tLoss: 0.014143\n",
      "Train Epoch: 23 [8080/11051 (73%)]\tLoss: 0.009895\n",
      "Train Epoch: 23 [8160/11051 (74%)]\tLoss: 0.028519\n",
      "Train Epoch: 23 [8240/11051 (75%)]\tLoss: 0.017956\n",
      "Train Epoch: 23 [8320/11051 (75%)]\tLoss: 0.017365\n",
      "Train Epoch: 23 [8400/11051 (76%)]\tLoss: 0.021416\n",
      "Train Epoch: 23 [8480/11051 (77%)]\tLoss: 0.019225\n",
      "Train Epoch: 23 [8560/11051 (77%)]\tLoss: 0.022425\n",
      "Train Epoch: 23 [8640/11051 (78%)]\tLoss: 0.028066\n",
      "Train Epoch: 23 [8720/11051 (79%)]\tLoss: 0.023099\n",
      "Train Epoch: 23 [8800/11051 (80%)]\tLoss: 0.026074\n",
      "Train Epoch: 23 [8880/11051 (80%)]\tLoss: 0.020468\n",
      "Train Epoch: 23 [8960/11051 (81%)]\tLoss: 0.021728\n",
      "Train Epoch: 23 [9040/11051 (82%)]\tLoss: 0.004815\n",
      "Train Epoch: 23 [9120/11051 (82%)]\tLoss: 0.011925\n",
      "Train Epoch: 23 [9200/11051 (83%)]\tLoss: 0.011767\n",
      "Train Epoch: 23 [9280/11051 (84%)]\tLoss: 0.026193\n",
      "Train Epoch: 23 [9360/11051 (85%)]\tLoss: 0.010905\n",
      "Train Epoch: 23 [9440/11051 (85%)]\tLoss: 0.035659\n",
      "Train Epoch: 23 [9520/11051 (86%)]\tLoss: 0.041899\n",
      "Train Epoch: 23 [9600/11051 (87%)]\tLoss: 0.020657\n",
      "Train Epoch: 23 [9680/11051 (88%)]\tLoss: 0.020372\n",
      "Train Epoch: 23 [9760/11051 (88%)]\tLoss: 0.019750\n",
      "Train Epoch: 23 [9840/11051 (89%)]\tLoss: 0.015083\n",
      "Train Epoch: 23 [9920/11051 (90%)]\tLoss: 0.016165\n",
      "Train Epoch: 23 [10000/11051 (90%)]\tLoss: 0.018650\n",
      "Train Epoch: 23 [10080/11051 (91%)]\tLoss: 0.031265\n",
      "Train Epoch: 23 [10160/11051 (92%)]\tLoss: 0.024253\n",
      "Train Epoch: 23 [10240/11051 (93%)]\tLoss: 0.013929\n",
      "Train Epoch: 23 [10320/11051 (93%)]\tLoss: 0.027077\n",
      "Train Epoch: 23 [10400/11051 (94%)]\tLoss: 0.031421\n",
      "Train Epoch: 23 [10480/11051 (95%)]\tLoss: 0.022354\n",
      "Train Epoch: 23 [10560/11051 (96%)]\tLoss: 0.014702\n",
      "Train Epoch: 23 [10640/11051 (96%)]\tLoss: 0.034929\n",
      "Train Epoch: 23 [10720/11051 (97%)]\tLoss: 0.016791\n",
      "Train Epoch: 23 [10800/11051 (98%)]\tLoss: 0.023479\n",
      "Train Epoch: 23 [10880/11051 (98%)]\tLoss: 0.014688\n",
      "Train Epoch: 23 [10960/11051 (99%)]\tLoss: 0.026869\n",
      "Train Epoch: 23 [11040/11051 (100%)]\tLoss: 0.031888\n",
      "====> Epoch: 23 Average loss: 0.0218\n",
      "\n",
      "Started training epoch no. 25\n",
      "Train Epoch: 24 [0/11051 (0%)]\tLoss: 0.015577\n",
      "Train Epoch: 24 [80/11051 (1%)]\tLoss: 0.018402\n",
      "Train Epoch: 24 [160/11051 (1%)]\tLoss: 0.019823\n",
      "Train Epoch: 24 [240/11051 (2%)]\tLoss: 0.033687\n",
      "Train Epoch: 24 [320/11051 (3%)]\tLoss: 0.011886\n",
      "Train Epoch: 24 [400/11051 (4%)]\tLoss: 0.015859\n",
      "Train Epoch: 24 [480/11051 (4%)]\tLoss: 0.009517\n",
      "Train Epoch: 24 [560/11051 (5%)]\tLoss: 0.024930\n",
      "Train Epoch: 24 [640/11051 (6%)]\tLoss: 0.026609\n",
      "Train Epoch: 24 [720/11051 (7%)]\tLoss: 0.015588\n",
      "Train Epoch: 24 [800/11051 (7%)]\tLoss: 0.031510\n",
      "Train Epoch: 24 [880/11051 (8%)]\tLoss: 0.031651\n",
      "Train Epoch: 24 [960/11051 (9%)]\tLoss: 0.009055\n",
      "Train Epoch: 24 [1040/11051 (9%)]\tLoss: 0.040298\n",
      "Train Epoch: 24 [1120/11051 (10%)]\tLoss: 0.021893\n",
      "Train Epoch: 24 [1200/11051 (11%)]\tLoss: 0.012736\n",
      "Train Epoch: 24 [1280/11051 (12%)]\tLoss: 0.023682\n",
      "Train Epoch: 24 [1360/11051 (12%)]\tLoss: 0.016449\n",
      "Train Epoch: 24 [1440/11051 (13%)]\tLoss: 0.019480\n",
      "Train Epoch: 24 [1520/11051 (14%)]\tLoss: 0.019302\n",
      "Train Epoch: 24 [1600/11051 (14%)]\tLoss: 0.011404\n",
      "Train Epoch: 24 [1680/11051 (15%)]\tLoss: 0.013564\n",
      "Train Epoch: 24 [1760/11051 (16%)]\tLoss: 0.028532\n",
      "Train Epoch: 24 [1840/11051 (17%)]\tLoss: 0.021155\n",
      "Train Epoch: 24 [1920/11051 (17%)]\tLoss: 0.017741\n",
      "Train Epoch: 24 [2000/11051 (18%)]\tLoss: 0.016911\n",
      "Train Epoch: 24 [2080/11051 (19%)]\tLoss: 0.019610\n",
      "Train Epoch: 24 [2160/11051 (20%)]\tLoss: 0.026990\n",
      "Train Epoch: 24 [2240/11051 (20%)]\tLoss: 0.044027\n",
      "Train Epoch: 24 [2320/11051 (21%)]\tLoss: 0.014278\n",
      "Train Epoch: 24 [2400/11051 (22%)]\tLoss: 0.015419\n",
      "Train Epoch: 24 [2480/11051 (22%)]\tLoss: 0.015404\n",
      "Train Epoch: 24 [2560/11051 (23%)]\tLoss: 0.011419\n",
      "Train Epoch: 24 [2640/11051 (24%)]\tLoss: 0.030162\n",
      "Train Epoch: 24 [2720/11051 (25%)]\tLoss: 0.020113\n",
      "Train Epoch: 24 [2800/11051 (25%)]\tLoss: 0.015977\n",
      "Train Epoch: 24 [2880/11051 (26%)]\tLoss: 0.017409\n",
      "Train Epoch: 24 [2960/11051 (27%)]\tLoss: 0.038570\n",
      "Train Epoch: 24 [3040/11051 (27%)]\tLoss: 0.028785\n",
      "Train Epoch: 24 [3120/11051 (28%)]\tLoss: 0.040206\n",
      "Train Epoch: 24 [3200/11051 (29%)]\tLoss: 0.028360\n",
      "Train Epoch: 24 [3280/11051 (30%)]\tLoss: 0.026924\n",
      "Train Epoch: 24 [3360/11051 (30%)]\tLoss: 0.014134\n",
      "Train Epoch: 24 [3440/11051 (31%)]\tLoss: 0.018411\n",
      "Train Epoch: 24 [3520/11051 (32%)]\tLoss: 0.019753\n",
      "Train Epoch: 24 [3600/11051 (33%)]\tLoss: 0.021002\n",
      "Train Epoch: 24 [3680/11051 (33%)]\tLoss: 0.027994\n",
      "Train Epoch: 24 [3760/11051 (34%)]\tLoss: 0.016157\n",
      "Train Epoch: 24 [3840/11051 (35%)]\tLoss: 0.025677\n",
      "Train Epoch: 24 [3920/11051 (35%)]\tLoss: 0.014806\n",
      "Train Epoch: 24 [4000/11051 (36%)]\tLoss: 0.026813\n",
      "Train Epoch: 24 [4080/11051 (37%)]\tLoss: 0.014616\n",
      "Train Epoch: 24 [4160/11051 (38%)]\tLoss: 0.017700\n",
      "Train Epoch: 24 [4240/11051 (38%)]\tLoss: 0.024690\n",
      "Train Epoch: 24 [4320/11051 (39%)]\tLoss: 0.020626\n",
      "Train Epoch: 24 [4400/11051 (40%)]\tLoss: 0.012170\n",
      "Train Epoch: 24 [4480/11051 (41%)]\tLoss: 0.018133\n",
      "Train Epoch: 24 [4560/11051 (41%)]\tLoss: 0.016407\n",
      "Train Epoch: 24 [4640/11051 (42%)]\tLoss: 0.012208\n",
      "Train Epoch: 24 [4720/11051 (43%)]\tLoss: 0.035731\n",
      "Train Epoch: 24 [4800/11051 (43%)]\tLoss: 0.017425\n",
      "Train Epoch: 24 [4880/11051 (44%)]\tLoss: 0.008007\n",
      "Train Epoch: 24 [4960/11051 (45%)]\tLoss: 0.018134\n",
      "Train Epoch: 24 [5040/11051 (46%)]\tLoss: 0.025845\n",
      "Train Epoch: 24 [5120/11051 (46%)]\tLoss: 0.017425\n",
      "Train Epoch: 24 [5200/11051 (47%)]\tLoss: 0.031027\n",
      "Train Epoch: 24 [5280/11051 (48%)]\tLoss: 0.026970\n",
      "Train Epoch: 24 [5360/11051 (48%)]\tLoss: 0.010452\n",
      "Train Epoch: 24 [5440/11051 (49%)]\tLoss: 0.028421\n",
      "Train Epoch: 24 [5520/11051 (50%)]\tLoss: 0.020889\n",
      "Train Epoch: 24 [5600/11051 (51%)]\tLoss: 0.015926\n",
      "Train Epoch: 24 [5680/11051 (51%)]\tLoss: 0.016051\n",
      "Train Epoch: 24 [5760/11051 (52%)]\tLoss: 0.015702\n",
      "Train Epoch: 24 [5840/11051 (53%)]\tLoss: 0.016107\n",
      "Train Epoch: 24 [5920/11051 (54%)]\tLoss: 0.019495\n",
      "Train Epoch: 24 [6000/11051 (54%)]\tLoss: 0.029234\n",
      "Train Epoch: 24 [6080/11051 (55%)]\tLoss: 0.015941\n",
      "Train Epoch: 24 [6160/11051 (56%)]\tLoss: 0.018846\n",
      "Train Epoch: 24 [6240/11051 (56%)]\tLoss: 0.008507\n",
      "Train Epoch: 24 [6320/11051 (57%)]\tLoss: 0.014000\n",
      "Train Epoch: 24 [6400/11051 (58%)]\tLoss: 0.022154\n",
      "Train Epoch: 24 [6480/11051 (59%)]\tLoss: 0.014488\n",
      "Train Epoch: 24 [6560/11051 (59%)]\tLoss: 0.039566\n",
      "Train Epoch: 24 [6640/11051 (60%)]\tLoss: 0.018220\n",
      "Train Epoch: 24 [6720/11051 (61%)]\tLoss: 0.012346\n",
      "Train Epoch: 24 [6800/11051 (62%)]\tLoss: 0.031236\n",
      "Train Epoch: 24 [6880/11051 (62%)]\tLoss: 0.021347\n",
      "Train Epoch: 24 [6960/11051 (63%)]\tLoss: 0.019114\n",
      "Train Epoch: 24 [7040/11051 (64%)]\tLoss: 0.014567\n",
      "Train Epoch: 24 [7120/11051 (64%)]\tLoss: 0.016345\n",
      "Train Epoch: 24 [7200/11051 (65%)]\tLoss: 0.019563\n",
      "Train Epoch: 24 [7280/11051 (66%)]\tLoss: 0.012156\n",
      "Train Epoch: 24 [7360/11051 (67%)]\tLoss: 0.019371\n",
      "Train Epoch: 24 [7440/11051 (67%)]\tLoss: 0.011850\n",
      "Train Epoch: 24 [7520/11051 (68%)]\tLoss: 0.027291\n",
      "Train Epoch: 24 [7600/11051 (69%)]\tLoss: 0.016978\n",
      "Train Epoch: 24 [7680/11051 (69%)]\tLoss: 0.022889\n",
      "Train Epoch: 24 [7760/11051 (70%)]\tLoss: 0.040184\n",
      "Train Epoch: 24 [7840/11051 (71%)]\tLoss: 0.027053\n",
      "Train Epoch: 24 [7920/11051 (72%)]\tLoss: 0.018247\n",
      "Train Epoch: 24 [8000/11051 (72%)]\tLoss: 0.018475\n",
      "Train Epoch: 24 [8080/11051 (73%)]\tLoss: 0.011623\n",
      "Train Epoch: 24 [8160/11051 (74%)]\tLoss: 0.025065\n",
      "Train Epoch: 24 [8240/11051 (75%)]\tLoss: 0.013618\n",
      "Train Epoch: 24 [8320/11051 (75%)]\tLoss: 0.013860\n",
      "Train Epoch: 24 [8400/11051 (76%)]\tLoss: 0.019996\n",
      "Train Epoch: 24 [8480/11051 (77%)]\tLoss: 0.038025\n",
      "Train Epoch: 24 [8560/11051 (77%)]\tLoss: 0.022349\n",
      "Train Epoch: 24 [8640/11051 (78%)]\tLoss: 0.012601\n",
      "Train Epoch: 24 [8720/11051 (79%)]\tLoss: 0.046196\n",
      "Train Epoch: 24 [8800/11051 (80%)]\tLoss: 0.015423\n",
      "Train Epoch: 24 [8880/11051 (80%)]\tLoss: 0.017904\n",
      "Train Epoch: 24 [8960/11051 (81%)]\tLoss: 0.026199\n",
      "Train Epoch: 24 [9040/11051 (82%)]\tLoss: 0.024049\n",
      "Train Epoch: 24 [9120/11051 (82%)]\tLoss: 0.020529\n",
      "Train Epoch: 24 [9200/11051 (83%)]\tLoss: 0.020876\n",
      "Train Epoch: 24 [9280/11051 (84%)]\tLoss: 0.011032\n",
      "Train Epoch: 24 [9360/11051 (85%)]\tLoss: 0.031263\n",
      "Train Epoch: 24 [9440/11051 (85%)]\tLoss: 0.030188\n",
      "Train Epoch: 24 [9520/11051 (86%)]\tLoss: 0.016689\n",
      "Train Epoch: 24 [9600/11051 (87%)]\tLoss: 0.010791\n",
      "Train Epoch: 24 [9680/11051 (88%)]\tLoss: 0.042067\n",
      "Train Epoch: 24 [9760/11051 (88%)]\tLoss: 0.016292\n",
      "Train Epoch: 24 [9840/11051 (89%)]\tLoss: 0.012051\n",
      "Train Epoch: 24 [9920/11051 (90%)]\tLoss: 0.033617\n",
      "Train Epoch: 24 [10000/11051 (90%)]\tLoss: 0.030221\n",
      "Train Epoch: 24 [10080/11051 (91%)]\tLoss: 0.013420\n",
      "Train Epoch: 24 [10160/11051 (92%)]\tLoss: 0.011697\n",
      "Train Epoch: 24 [10240/11051 (93%)]\tLoss: 0.012587\n",
      "Train Epoch: 24 [10320/11051 (93%)]\tLoss: 0.007528\n",
      "Train Epoch: 24 [10400/11051 (94%)]\tLoss: 0.018933\n",
      "Train Epoch: 24 [10480/11051 (95%)]\tLoss: 0.028606\n",
      "Train Epoch: 24 [10560/11051 (96%)]\tLoss: 0.063482\n",
      "Train Epoch: 24 [10640/11051 (96%)]\tLoss: 0.020110\n",
      "Train Epoch: 24 [10720/11051 (97%)]\tLoss: 0.089994\n",
      "Train Epoch: 24 [10800/11051 (98%)]\tLoss: 0.008512\n",
      "Train Epoch: 24 [10880/11051 (98%)]\tLoss: 0.027419\n",
      "Train Epoch: 24 [10960/11051 (99%)]\tLoss: 0.022693\n",
      "Train Epoch: 24 [11040/11051 (100%)]\tLoss: 0.022582\n",
      "====> Epoch: 24 Average loss: 0.0217\n",
      "\n",
      "Started training epoch no. 26\n",
      "Train Epoch: 25 [0/11051 (0%)]\tLoss: 0.030103\n",
      "Train Epoch: 25 [80/11051 (1%)]\tLoss: 0.019221\n",
      "Train Epoch: 25 [160/11051 (1%)]\tLoss: 0.018513\n",
      "Train Epoch: 25 [240/11051 (2%)]\tLoss: 0.015002\n",
      "Train Epoch: 25 [320/11051 (3%)]\tLoss: 0.014937\n",
      "Train Epoch: 25 [400/11051 (4%)]\tLoss: 0.017476\n",
      "Train Epoch: 25 [480/11051 (4%)]\tLoss: 0.024050\n",
      "Train Epoch: 25 [560/11051 (5%)]\tLoss: 0.016843\n",
      "Train Epoch: 25 [640/11051 (6%)]\tLoss: 0.028931\n",
      "Train Epoch: 25 [720/11051 (7%)]\tLoss: 0.019920\n",
      "Train Epoch: 25 [800/11051 (7%)]\tLoss: 0.037715\n",
      "Train Epoch: 25 [880/11051 (8%)]\tLoss: 0.029251\n",
      "Train Epoch: 25 [960/11051 (9%)]\tLoss: 0.028441\n",
      "Train Epoch: 25 [1040/11051 (9%)]\tLoss: 0.034393\n",
      "Train Epoch: 25 [1120/11051 (10%)]\tLoss: 0.018181\n",
      "Train Epoch: 25 [1200/11051 (11%)]\tLoss: 0.022841\n",
      "Train Epoch: 25 [1280/11051 (12%)]\tLoss: 0.028074\n",
      "Train Epoch: 25 [1360/11051 (12%)]\tLoss: 0.027435\n",
      "Train Epoch: 25 [1440/11051 (13%)]\tLoss: 0.013411\n",
      "Train Epoch: 25 [1520/11051 (14%)]\tLoss: 0.004780\n",
      "Train Epoch: 25 [1600/11051 (14%)]\tLoss: 0.017572\n",
      "Train Epoch: 25 [1680/11051 (15%)]\tLoss: 0.010114\n",
      "Train Epoch: 25 [1760/11051 (16%)]\tLoss: 0.007928\n",
      "Train Epoch: 25 [1840/11051 (17%)]\tLoss: 0.018385\n",
      "Train Epoch: 25 [1920/11051 (17%)]\tLoss: 0.027164\n",
      "Train Epoch: 25 [2000/11051 (18%)]\tLoss: 0.020121\n",
      "Train Epoch: 25 [2080/11051 (19%)]\tLoss: 0.036716\n",
      "Train Epoch: 25 [2160/11051 (20%)]\tLoss: 0.024573\n",
      "Train Epoch: 25 [2240/11051 (20%)]\tLoss: 0.012782\n",
      "Train Epoch: 25 [2320/11051 (21%)]\tLoss: 0.009750\n",
      "Train Epoch: 25 [2400/11051 (22%)]\tLoss: 0.021977\n",
      "Train Epoch: 25 [2480/11051 (22%)]\tLoss: 0.029621\n",
      "Train Epoch: 25 [2560/11051 (23%)]\tLoss: 0.029131\n",
      "Train Epoch: 25 [2640/11051 (24%)]\tLoss: 0.021517\n",
      "Train Epoch: 25 [2720/11051 (25%)]\tLoss: 0.019556\n",
      "Train Epoch: 25 [2800/11051 (25%)]\tLoss: 0.019009\n",
      "Train Epoch: 25 [2880/11051 (26%)]\tLoss: 0.027902\n",
      "Train Epoch: 25 [2960/11051 (27%)]\tLoss: 0.006667\n",
      "Train Epoch: 25 [3040/11051 (27%)]\tLoss: 0.010402\n",
      "Train Epoch: 25 [3120/11051 (28%)]\tLoss: 0.027721\n",
      "Train Epoch: 25 [3200/11051 (29%)]\tLoss: 0.022856\n",
      "Train Epoch: 25 [3280/11051 (30%)]\tLoss: 0.014384\n",
      "Train Epoch: 25 [3360/11051 (30%)]\tLoss: 0.011078\n",
      "Train Epoch: 25 [3440/11051 (31%)]\tLoss: 0.027160\n",
      "Train Epoch: 25 [3520/11051 (32%)]\tLoss: 0.019605\n",
      "Train Epoch: 25 [3600/11051 (33%)]\tLoss: 0.011174\n",
      "Train Epoch: 25 [3680/11051 (33%)]\tLoss: 0.010771\n",
      "Train Epoch: 25 [3760/11051 (34%)]\tLoss: 0.019468\n",
      "Train Epoch: 25 [3840/11051 (35%)]\tLoss: 0.009411\n",
      "Train Epoch: 25 [3920/11051 (35%)]\tLoss: 0.026005\n",
      "Train Epoch: 25 [4000/11051 (36%)]\tLoss: 0.016998\n",
      "Train Epoch: 25 [4080/11051 (37%)]\tLoss: 0.011794\n",
      "Train Epoch: 25 [4160/11051 (38%)]\tLoss: 0.032251\n",
      "Train Epoch: 25 [4240/11051 (38%)]\tLoss: 0.035050\n",
      "Train Epoch: 25 [4320/11051 (39%)]\tLoss: 0.020591\n",
      "Train Epoch: 25 [4400/11051 (40%)]\tLoss: 0.027704\n",
      "Train Epoch: 25 [4480/11051 (41%)]\tLoss: 0.009553\n",
      "Train Epoch: 25 [4560/11051 (41%)]\tLoss: 0.028108\n",
      "Train Epoch: 25 [4640/11051 (42%)]\tLoss: 0.009083\n",
      "Train Epoch: 25 [4720/11051 (43%)]\tLoss: 0.009072\n",
      "Train Epoch: 25 [4800/11051 (43%)]\tLoss: 0.027111\n",
      "Train Epoch: 25 [4880/11051 (44%)]\tLoss: 0.019497\n",
      "Train Epoch: 25 [4960/11051 (45%)]\tLoss: 0.006834\n",
      "Train Epoch: 25 [5040/11051 (46%)]\tLoss: 0.035766\n",
      "Train Epoch: 25 [5120/11051 (46%)]\tLoss: 0.021850\n",
      "Train Epoch: 25 [5200/11051 (47%)]\tLoss: 0.014897\n",
      "Train Epoch: 25 [5280/11051 (48%)]\tLoss: 0.008950\n",
      "Train Epoch: 25 [5360/11051 (48%)]\tLoss: 0.027280\n",
      "Train Epoch: 25 [5440/11051 (49%)]\tLoss: 0.014898\n",
      "Train Epoch: 25 [5520/11051 (50%)]\tLoss: 0.012479\n",
      "Train Epoch: 25 [5600/11051 (51%)]\tLoss: 0.021754\n",
      "Train Epoch: 25 [5680/11051 (51%)]\tLoss: 0.022428\n",
      "Train Epoch: 25 [5760/11051 (52%)]\tLoss: 0.012843\n",
      "Train Epoch: 25 [5840/11051 (53%)]\tLoss: 0.018465\n",
      "Train Epoch: 25 [5920/11051 (54%)]\tLoss: 0.023471\n",
      "Train Epoch: 25 [6000/11051 (54%)]\tLoss: 0.020609\n",
      "Train Epoch: 25 [6080/11051 (55%)]\tLoss: 0.021186\n",
      "Train Epoch: 25 [6160/11051 (56%)]\tLoss: 0.023603\n",
      "Train Epoch: 25 [6240/11051 (56%)]\tLoss: 0.032587\n",
      "Train Epoch: 25 [6320/11051 (57%)]\tLoss: 0.031729\n",
      "Train Epoch: 25 [6400/11051 (58%)]\tLoss: 0.012629\n",
      "Train Epoch: 25 [6480/11051 (59%)]\tLoss: 0.006420\n",
      "Train Epoch: 25 [6560/11051 (59%)]\tLoss: 0.008607\n",
      "Train Epoch: 25 [6640/11051 (60%)]\tLoss: 0.037973\n",
      "Train Epoch: 25 [6720/11051 (61%)]\tLoss: 0.014560\n",
      "Train Epoch: 25 [6800/11051 (62%)]\tLoss: 0.020511\n",
      "Train Epoch: 25 [6880/11051 (62%)]\tLoss: 0.018931\n",
      "Train Epoch: 25 [6960/11051 (63%)]\tLoss: 0.022674\n",
      "Train Epoch: 25 [7040/11051 (64%)]\tLoss: 0.030566\n",
      "Train Epoch: 25 [7120/11051 (64%)]\tLoss: 0.013878\n",
      "Train Epoch: 25 [7200/11051 (65%)]\tLoss: 0.022313\n",
      "Train Epoch: 25 [7280/11051 (66%)]\tLoss: 0.019214\n",
      "Train Epoch: 25 [7360/11051 (67%)]\tLoss: 0.016518\n",
      "Train Epoch: 25 [7440/11051 (67%)]\tLoss: 0.025701\n",
      "Train Epoch: 25 [7520/11051 (68%)]\tLoss: 0.011684\n",
      "Train Epoch: 25 [7600/11051 (69%)]\tLoss: 0.010444\n",
      "Train Epoch: 25 [7680/11051 (69%)]\tLoss: 0.012038\n",
      "Train Epoch: 25 [7760/11051 (70%)]\tLoss: 0.026886\n",
      "Train Epoch: 25 [7840/11051 (71%)]\tLoss: 0.039369\n",
      "Train Epoch: 25 [7920/11051 (72%)]\tLoss: 0.031425\n",
      "Train Epoch: 25 [8000/11051 (72%)]\tLoss: 0.049218\n",
      "Train Epoch: 25 [8080/11051 (73%)]\tLoss: 0.017785\n",
      "Train Epoch: 25 [8160/11051 (74%)]\tLoss: 0.014414\n",
      "Train Epoch: 25 [8240/11051 (75%)]\tLoss: 0.009097\n",
      "Train Epoch: 25 [8320/11051 (75%)]\tLoss: 0.029272\n",
      "Train Epoch: 25 [8400/11051 (76%)]\tLoss: 0.018809\n",
      "Train Epoch: 25 [8480/11051 (77%)]\tLoss: 0.025183\n",
      "Train Epoch: 25 [8560/11051 (77%)]\tLoss: 0.014752\n",
      "Train Epoch: 25 [8640/11051 (78%)]\tLoss: 0.021463\n",
      "Train Epoch: 25 [8720/11051 (79%)]\tLoss: 0.025940\n",
      "Train Epoch: 25 [8800/11051 (80%)]\tLoss: 0.022961\n",
      "Train Epoch: 25 [8880/11051 (80%)]\tLoss: 0.016538\n",
      "Train Epoch: 25 [8960/11051 (81%)]\tLoss: 0.019485\n",
      "Train Epoch: 25 [9040/11051 (82%)]\tLoss: 0.023694\n",
      "Train Epoch: 25 [9120/11051 (82%)]\tLoss: 0.019014\n",
      "Train Epoch: 25 [9200/11051 (83%)]\tLoss: 0.022345\n",
      "Train Epoch: 25 [9280/11051 (84%)]\tLoss: 0.015541\n",
      "Train Epoch: 25 [9360/11051 (85%)]\tLoss: 0.033309\n",
      "Train Epoch: 25 [9440/11051 (85%)]\tLoss: 0.025514\n",
      "Train Epoch: 25 [9520/11051 (86%)]\tLoss: 0.013417\n",
      "Train Epoch: 25 [9600/11051 (87%)]\tLoss: 0.033462\n",
      "Train Epoch: 25 [9680/11051 (88%)]\tLoss: 0.015761\n",
      "Train Epoch: 25 [9760/11051 (88%)]\tLoss: 0.033117\n",
      "Train Epoch: 25 [9840/11051 (89%)]\tLoss: 0.035133\n",
      "Train Epoch: 25 [9920/11051 (90%)]\tLoss: 0.024885\n",
      "Train Epoch: 25 [10000/11051 (90%)]\tLoss: 0.018572\n",
      "Train Epoch: 25 [10080/11051 (91%)]\tLoss: 0.015587\n",
      "Train Epoch: 25 [10160/11051 (92%)]\tLoss: 0.016090\n",
      "Train Epoch: 25 [10240/11051 (93%)]\tLoss: 0.024540\n",
      "Train Epoch: 25 [10320/11051 (93%)]\tLoss: 0.018333\n",
      "Train Epoch: 25 [10400/11051 (94%)]\tLoss: 0.013454\n",
      "Train Epoch: 25 [10480/11051 (95%)]\tLoss: 0.009971\n",
      "Train Epoch: 25 [10560/11051 (96%)]\tLoss: 0.012462\n",
      "Train Epoch: 25 [10640/11051 (96%)]\tLoss: 0.035438\n",
      "Train Epoch: 25 [10720/11051 (97%)]\tLoss: 0.018942\n",
      "Train Epoch: 25 [10800/11051 (98%)]\tLoss: 0.019493\n",
      "Train Epoch: 25 [10880/11051 (98%)]\tLoss: 0.028023\n",
      "Train Epoch: 25 [10960/11051 (99%)]\tLoss: 0.053132\n",
      "Train Epoch: 25 [11040/11051 (100%)]\tLoss: 0.017842\n",
      "====> Epoch: 25 Average loss: 0.0216\n",
      "\n",
      "Started training epoch no. 27\n",
      "Train Epoch: 26 [0/11051 (0%)]\tLoss: 0.017677\n",
      "Train Epoch: 26 [80/11051 (1%)]\tLoss: 0.015262\n",
      "Train Epoch: 26 [160/11051 (1%)]\tLoss: 0.018570\n",
      "Train Epoch: 26 [240/11051 (2%)]\tLoss: 0.011003\n",
      "Train Epoch: 26 [320/11051 (3%)]\tLoss: 0.021156\n",
      "Train Epoch: 26 [400/11051 (4%)]\tLoss: 0.028171\n",
      "Train Epoch: 26 [480/11051 (4%)]\tLoss: 0.009073\n",
      "Train Epoch: 26 [560/11051 (5%)]\tLoss: 0.020400\n",
      "Train Epoch: 26 [640/11051 (6%)]\tLoss: 0.025519\n",
      "Train Epoch: 26 [720/11051 (7%)]\tLoss: 0.023841\n",
      "Train Epoch: 26 [800/11051 (7%)]\tLoss: 0.019256\n",
      "Train Epoch: 26 [880/11051 (8%)]\tLoss: 0.020475\n",
      "Train Epoch: 26 [960/11051 (9%)]\tLoss: 0.011460\n",
      "Train Epoch: 26 [1040/11051 (9%)]\tLoss: 0.022697\n",
      "Train Epoch: 26 [1120/11051 (10%)]\tLoss: 0.008799\n",
      "Train Epoch: 26 [1200/11051 (11%)]\tLoss: 0.020817\n",
      "Train Epoch: 26 [1280/11051 (12%)]\tLoss: 0.020144\n",
      "Train Epoch: 26 [1360/11051 (12%)]\tLoss: 0.018022\n",
      "Train Epoch: 26 [1440/11051 (13%)]\tLoss: 0.027600\n",
      "Train Epoch: 26 [1520/11051 (14%)]\tLoss: 0.027721\n",
      "Train Epoch: 26 [1600/11051 (14%)]\tLoss: 0.018729\n",
      "Train Epoch: 26 [1680/11051 (15%)]\tLoss: 0.020936\n",
      "Train Epoch: 26 [1760/11051 (16%)]\tLoss: 0.015047\n",
      "Train Epoch: 26 [1840/11051 (17%)]\tLoss: 0.021980\n",
      "Train Epoch: 26 [1920/11051 (17%)]\tLoss: 0.014024\n",
      "Train Epoch: 26 [2000/11051 (18%)]\tLoss: 0.016467\n",
      "Train Epoch: 26 [2080/11051 (19%)]\tLoss: 0.020326\n",
      "Train Epoch: 26 [2160/11051 (20%)]\tLoss: 0.031622\n",
      "Train Epoch: 26 [2240/11051 (20%)]\tLoss: 0.009318\n",
      "Train Epoch: 26 [2320/11051 (21%)]\tLoss: 0.012527\n",
      "Train Epoch: 26 [2400/11051 (22%)]\tLoss: 0.012844\n",
      "Train Epoch: 26 [2480/11051 (22%)]\tLoss: 0.027519\n",
      "Train Epoch: 26 [2560/11051 (23%)]\tLoss: 0.017824\n",
      "Train Epoch: 26 [2640/11051 (24%)]\tLoss: 0.021770\n",
      "Train Epoch: 26 [2720/11051 (25%)]\tLoss: 0.021300\n",
      "Train Epoch: 26 [2800/11051 (25%)]\tLoss: 0.011198\n",
      "Train Epoch: 26 [2880/11051 (26%)]\tLoss: 0.023457\n",
      "Train Epoch: 26 [2960/11051 (27%)]\tLoss: 0.014650\n",
      "Train Epoch: 26 [3040/11051 (27%)]\tLoss: 0.017120\n",
      "Train Epoch: 26 [3120/11051 (28%)]\tLoss: 0.015468\n",
      "Train Epoch: 26 [3200/11051 (29%)]\tLoss: 0.033718\n",
      "Train Epoch: 26 [3280/11051 (30%)]\tLoss: 0.033512\n",
      "Train Epoch: 26 [3360/11051 (30%)]\tLoss: 0.024719\n",
      "Train Epoch: 26 [3440/11051 (31%)]\tLoss: 0.009188\n",
      "Train Epoch: 26 [3520/11051 (32%)]\tLoss: 0.024767\n",
      "Train Epoch: 26 [3600/11051 (33%)]\tLoss: 0.027882\n",
      "Train Epoch: 26 [3680/11051 (33%)]\tLoss: 0.021691\n",
      "Train Epoch: 26 [3760/11051 (34%)]\tLoss: 0.005901\n",
      "Train Epoch: 26 [3840/11051 (35%)]\tLoss: 0.021259\n",
      "Train Epoch: 26 [3920/11051 (35%)]\tLoss: 0.016340\n",
      "Train Epoch: 26 [4000/11051 (36%)]\tLoss: 0.018758\n",
      "Train Epoch: 26 [4080/11051 (37%)]\tLoss: 0.015182\n",
      "Train Epoch: 26 [4160/11051 (38%)]\tLoss: 0.016848\n",
      "Train Epoch: 26 [4240/11051 (38%)]\tLoss: 0.025336\n",
      "Train Epoch: 26 [4320/11051 (39%)]\tLoss: 0.020225\n",
      "Train Epoch: 26 [4400/11051 (40%)]\tLoss: 0.034240\n",
      "Train Epoch: 26 [4480/11051 (41%)]\tLoss: 0.027493\n",
      "Train Epoch: 26 [4560/11051 (41%)]\tLoss: 0.024233\n",
      "Train Epoch: 26 [4640/11051 (42%)]\tLoss: 0.017490\n",
      "Train Epoch: 26 [4720/11051 (43%)]\tLoss: 0.014937\n",
      "Train Epoch: 26 [4800/11051 (43%)]\tLoss: 0.031028\n",
      "Train Epoch: 26 [4880/11051 (44%)]\tLoss: 0.013360\n",
      "Train Epoch: 26 [4960/11051 (45%)]\tLoss: 0.013728\n",
      "Train Epoch: 26 [5040/11051 (46%)]\tLoss: 0.009812\n",
      "Train Epoch: 26 [5120/11051 (46%)]\tLoss: 0.024473\n",
      "Train Epoch: 26 [5200/11051 (47%)]\tLoss: 0.028040\n",
      "Train Epoch: 26 [5280/11051 (48%)]\tLoss: 0.026447\n",
      "Train Epoch: 26 [5360/11051 (48%)]\tLoss: 0.032444\n",
      "Train Epoch: 26 [5440/11051 (49%)]\tLoss: 0.008343\n",
      "Train Epoch: 26 [5520/11051 (50%)]\tLoss: 0.027391\n",
      "Train Epoch: 26 [5600/11051 (51%)]\tLoss: 0.010568\n",
      "Train Epoch: 26 [5680/11051 (51%)]\tLoss: 0.016082\n",
      "Train Epoch: 26 [5760/11051 (52%)]\tLoss: 0.014328\n",
      "Train Epoch: 26 [5840/11051 (53%)]\tLoss: 0.045333\n",
      "Train Epoch: 26 [5920/11051 (54%)]\tLoss: 0.014397\n",
      "Train Epoch: 26 [6000/11051 (54%)]\tLoss: 0.016219\n",
      "Train Epoch: 26 [6080/11051 (55%)]\tLoss: 0.009089\n",
      "Train Epoch: 26 [6160/11051 (56%)]\tLoss: 0.021467\n",
      "Train Epoch: 26 [6240/11051 (56%)]\tLoss: 0.020592\n",
      "Train Epoch: 26 [6320/11051 (57%)]\tLoss: 0.021704\n",
      "Train Epoch: 26 [6400/11051 (58%)]\tLoss: 0.025200\n",
      "Train Epoch: 26 [6480/11051 (59%)]\tLoss: 0.017128\n",
      "Train Epoch: 26 [6560/11051 (59%)]\tLoss: 0.018734\n",
      "Train Epoch: 26 [6640/11051 (60%)]\tLoss: 0.019630\n",
      "Train Epoch: 26 [6720/11051 (61%)]\tLoss: 0.011996\n",
      "Train Epoch: 26 [6800/11051 (62%)]\tLoss: 0.030336\n",
      "Train Epoch: 26 [6880/11051 (62%)]\tLoss: 0.035460\n",
      "Train Epoch: 26 [6960/11051 (63%)]\tLoss: 0.022089\n",
      "Train Epoch: 26 [7040/11051 (64%)]\tLoss: 0.021241\n",
      "Train Epoch: 26 [7120/11051 (64%)]\tLoss: 0.042045\n",
      "Train Epoch: 26 [7200/11051 (65%)]\tLoss: 0.015684\n",
      "Train Epoch: 26 [7280/11051 (66%)]\tLoss: 0.023534\n",
      "Train Epoch: 26 [7360/11051 (67%)]\tLoss: 0.028570\n",
      "Train Epoch: 26 [7440/11051 (67%)]\tLoss: 0.017803\n",
      "Train Epoch: 26 [7520/11051 (68%)]\tLoss: 0.033774\n",
      "Train Epoch: 26 [7600/11051 (69%)]\tLoss: 0.020074\n",
      "Train Epoch: 26 [7680/11051 (69%)]\tLoss: 0.029897\n",
      "Train Epoch: 26 [7760/11051 (70%)]\tLoss: 0.024275\n",
      "Train Epoch: 26 [7840/11051 (71%)]\tLoss: 0.025825\n",
      "Train Epoch: 26 [7920/11051 (72%)]\tLoss: 0.022162\n",
      "Train Epoch: 26 [8000/11051 (72%)]\tLoss: 0.016065\n",
      "Train Epoch: 26 [8080/11051 (73%)]\tLoss: 0.010312\n",
      "Train Epoch: 26 [8160/11051 (74%)]\tLoss: 0.027111\n",
      "Train Epoch: 26 [8240/11051 (75%)]\tLoss: 0.007635\n",
      "Train Epoch: 26 [8320/11051 (75%)]\tLoss: 0.021861\n",
      "Train Epoch: 26 [8400/11051 (76%)]\tLoss: 0.027347\n",
      "Train Epoch: 26 [8480/11051 (77%)]\tLoss: 0.029431\n",
      "Train Epoch: 26 [8560/11051 (77%)]\tLoss: 0.021781\n",
      "Train Epoch: 26 [8640/11051 (78%)]\tLoss: 0.018001\n",
      "Train Epoch: 26 [8720/11051 (79%)]\tLoss: 0.023685\n",
      "Train Epoch: 26 [8800/11051 (80%)]\tLoss: 0.020133\n",
      "Train Epoch: 26 [8880/11051 (80%)]\tLoss: 0.011204\n",
      "Train Epoch: 26 [8960/11051 (81%)]\tLoss: 0.019621\n",
      "Train Epoch: 26 [9040/11051 (82%)]\tLoss: 0.015525\n",
      "Train Epoch: 26 [9120/11051 (82%)]\tLoss: 0.011631\n",
      "Train Epoch: 26 [9200/11051 (83%)]\tLoss: 0.041641\n",
      "Train Epoch: 26 [9280/11051 (84%)]\tLoss: 0.029565\n",
      "Train Epoch: 26 [9360/11051 (85%)]\tLoss: 0.020085\n",
      "Train Epoch: 26 [9440/11051 (85%)]\tLoss: 0.010709\n",
      "Train Epoch: 26 [9520/11051 (86%)]\tLoss: 0.015089\n",
      "Train Epoch: 26 [9600/11051 (87%)]\tLoss: 0.021004\n",
      "Train Epoch: 26 [9680/11051 (88%)]\tLoss: 0.025084\n",
      "Train Epoch: 26 [9760/11051 (88%)]\tLoss: 0.014114\n",
      "Train Epoch: 26 [9840/11051 (89%)]\tLoss: 0.014402\n",
      "Train Epoch: 26 [9920/11051 (90%)]\tLoss: 0.021969\n",
      "Train Epoch: 26 [10000/11051 (90%)]\tLoss: 0.031177\n",
      "Train Epoch: 26 [10080/11051 (91%)]\tLoss: 0.024240\n",
      "Train Epoch: 26 [10160/11051 (92%)]\tLoss: 0.032721\n",
      "Train Epoch: 26 [10240/11051 (93%)]\tLoss: 0.032211\n",
      "Train Epoch: 26 [10320/11051 (93%)]\tLoss: 0.027495\n",
      "Train Epoch: 26 [10400/11051 (94%)]\tLoss: 0.006727\n",
      "Train Epoch: 26 [10480/11051 (95%)]\tLoss: 0.007334\n",
      "Train Epoch: 26 [10560/11051 (96%)]\tLoss: 0.037019\n",
      "Train Epoch: 26 [10640/11051 (96%)]\tLoss: 0.008779\n",
      "Train Epoch: 26 [10720/11051 (97%)]\tLoss: 0.023320\n",
      "Train Epoch: 26 [10800/11051 (98%)]\tLoss: 0.012614\n",
      "Train Epoch: 26 [10880/11051 (98%)]\tLoss: 0.032737\n",
      "Train Epoch: 26 [10960/11051 (99%)]\tLoss: 0.016828\n",
      "Train Epoch: 26 [11040/11051 (100%)]\tLoss: 0.010791\n",
      "====> Epoch: 26 Average loss: 0.0217\n",
      "\n",
      "Started training epoch no. 28\n",
      "Train Epoch: 27 [0/11051 (0%)]\tLoss: 0.010082\n",
      "Train Epoch: 27 [80/11051 (1%)]\tLoss: 0.045344\n",
      "Train Epoch: 27 [160/11051 (1%)]\tLoss: 0.010719\n",
      "Train Epoch: 27 [240/11051 (2%)]\tLoss: 0.005178\n",
      "Train Epoch: 27 [320/11051 (3%)]\tLoss: 0.012597\n",
      "Train Epoch: 27 [400/11051 (4%)]\tLoss: 0.016289\n",
      "Train Epoch: 27 [480/11051 (4%)]\tLoss: 0.020907\n",
      "Train Epoch: 27 [560/11051 (5%)]\tLoss: 0.021309\n",
      "Train Epoch: 27 [640/11051 (6%)]\tLoss: 0.025548\n",
      "Train Epoch: 27 [720/11051 (7%)]\tLoss: 0.015591\n",
      "Train Epoch: 27 [800/11051 (7%)]\tLoss: 0.012345\n",
      "Train Epoch: 27 [880/11051 (8%)]\tLoss: 0.023550\n",
      "Train Epoch: 27 [960/11051 (9%)]\tLoss: 0.019159\n",
      "Train Epoch: 27 [1040/11051 (9%)]\tLoss: 0.038406\n",
      "Train Epoch: 27 [1120/11051 (10%)]\tLoss: 0.021784\n",
      "Train Epoch: 27 [1200/11051 (11%)]\tLoss: 0.096481\n",
      "Train Epoch: 27 [1280/11051 (12%)]\tLoss: 0.019892\n",
      "Train Epoch: 27 [1360/11051 (12%)]\tLoss: 0.007335\n",
      "Train Epoch: 27 [1440/11051 (13%)]\tLoss: 0.018411\n",
      "Train Epoch: 27 [1520/11051 (14%)]\tLoss: 0.013221\n",
      "Train Epoch: 27 [1600/11051 (14%)]\tLoss: 0.021965\n",
      "Train Epoch: 27 [1680/11051 (15%)]\tLoss: 0.017340\n",
      "Train Epoch: 27 [1760/11051 (16%)]\tLoss: 0.030025\n",
      "Train Epoch: 27 [1840/11051 (17%)]\tLoss: 0.027230\n",
      "Train Epoch: 27 [1920/11051 (17%)]\tLoss: 0.057513\n",
      "Train Epoch: 27 [2000/11051 (18%)]\tLoss: 0.023214\n",
      "Train Epoch: 27 [2080/11051 (19%)]\tLoss: 0.018090\n",
      "Train Epoch: 27 [2160/11051 (20%)]\tLoss: 0.012923\n",
      "Train Epoch: 27 [2240/11051 (20%)]\tLoss: 0.012445\n",
      "Train Epoch: 27 [2320/11051 (21%)]\tLoss: 0.010965\n",
      "Train Epoch: 27 [2400/11051 (22%)]\tLoss: 0.026424\n",
      "Train Epoch: 27 [2480/11051 (22%)]\tLoss: 0.012389\n",
      "Train Epoch: 27 [2560/11051 (23%)]\tLoss: 0.025517\n",
      "Train Epoch: 27 [2640/11051 (24%)]\tLoss: 0.027563\n",
      "Train Epoch: 27 [2720/11051 (25%)]\tLoss: 0.029632\n",
      "Train Epoch: 27 [2800/11051 (25%)]\tLoss: 0.037766\n",
      "Train Epoch: 27 [2880/11051 (26%)]\tLoss: 0.015964\n",
      "Train Epoch: 27 [2960/11051 (27%)]\tLoss: 0.020175\n",
      "Train Epoch: 27 [3040/11051 (27%)]\tLoss: 0.007158\n",
      "Train Epoch: 27 [3120/11051 (28%)]\tLoss: 0.019910\n",
      "Train Epoch: 27 [3200/11051 (29%)]\tLoss: 0.010663\n",
      "Train Epoch: 27 [3280/11051 (30%)]\tLoss: 0.012823\n",
      "Train Epoch: 27 [3360/11051 (30%)]\tLoss: 0.014359\n",
      "Train Epoch: 27 [3440/11051 (31%)]\tLoss: 0.010557\n",
      "Train Epoch: 27 [3520/11051 (32%)]\tLoss: 0.014006\n",
      "Train Epoch: 27 [3600/11051 (33%)]\tLoss: 0.025290\n",
      "Train Epoch: 27 [3680/11051 (33%)]\tLoss: 0.016977\n",
      "Train Epoch: 27 [3760/11051 (34%)]\tLoss: 0.029770\n",
      "Train Epoch: 27 [3840/11051 (35%)]\tLoss: 0.013261\n",
      "Train Epoch: 27 [3920/11051 (35%)]\tLoss: 0.008857\n",
      "Train Epoch: 27 [4000/11051 (36%)]\tLoss: 0.032923\n",
      "Train Epoch: 27 [4080/11051 (37%)]\tLoss: 0.022154\n",
      "Train Epoch: 27 [4160/11051 (38%)]\tLoss: 0.007825\n",
      "Train Epoch: 27 [4240/11051 (38%)]\tLoss: 0.025289\n",
      "Train Epoch: 27 [4320/11051 (39%)]\tLoss: 0.020180\n",
      "Train Epoch: 27 [4400/11051 (40%)]\tLoss: 0.014499\n",
      "Train Epoch: 27 [4480/11051 (41%)]\tLoss: 0.009388\n",
      "Train Epoch: 27 [4560/11051 (41%)]\tLoss: 0.023651\n",
      "Train Epoch: 27 [4640/11051 (42%)]\tLoss: 0.010775\n",
      "Train Epoch: 27 [4720/11051 (43%)]\tLoss: 0.027125\n",
      "Train Epoch: 27 [4800/11051 (43%)]\tLoss: 0.030685\n",
      "Train Epoch: 27 [4880/11051 (44%)]\tLoss: 0.021825\n",
      "Train Epoch: 27 [4960/11051 (45%)]\tLoss: 0.015689\n",
      "Train Epoch: 27 [5040/11051 (46%)]\tLoss: 0.015418\n",
      "Train Epoch: 27 [5120/11051 (46%)]\tLoss: 0.021428\n",
      "Train Epoch: 27 [5200/11051 (47%)]\tLoss: 0.020739\n",
      "Train Epoch: 27 [5280/11051 (48%)]\tLoss: 0.024317\n",
      "Train Epoch: 27 [5360/11051 (48%)]\tLoss: 0.026429\n",
      "Train Epoch: 27 [5440/11051 (49%)]\tLoss: 0.011863\n",
      "Train Epoch: 27 [5520/11051 (50%)]\tLoss: 0.009280\n",
      "Train Epoch: 27 [5600/11051 (51%)]\tLoss: 0.011556\n",
      "Train Epoch: 27 [5680/11051 (51%)]\tLoss: 0.012155\n",
      "Train Epoch: 27 [5760/11051 (52%)]\tLoss: 0.025033\n",
      "Train Epoch: 27 [5840/11051 (53%)]\tLoss: 0.019771\n",
      "Train Epoch: 27 [5920/11051 (54%)]\tLoss: 0.020875\n",
      "Train Epoch: 27 [6000/11051 (54%)]\tLoss: 0.022950\n",
      "Train Epoch: 27 [6080/11051 (55%)]\tLoss: 0.019796\n",
      "Train Epoch: 27 [6160/11051 (56%)]\tLoss: 0.028571\n",
      "Train Epoch: 27 [6240/11051 (56%)]\tLoss: 0.014022\n",
      "Train Epoch: 27 [6320/11051 (57%)]\tLoss: 0.021546\n",
      "Train Epoch: 27 [6400/11051 (58%)]\tLoss: 0.011520\n",
      "Train Epoch: 27 [6480/11051 (59%)]\tLoss: 0.032460\n",
      "Train Epoch: 27 [6560/11051 (59%)]\tLoss: 0.015406\n",
      "Train Epoch: 27 [6640/11051 (60%)]\tLoss: 0.028029\n",
      "Train Epoch: 27 [6720/11051 (61%)]\tLoss: 0.017616\n",
      "Train Epoch: 27 [6800/11051 (62%)]\tLoss: 0.011657\n",
      "Train Epoch: 27 [6880/11051 (62%)]\tLoss: 0.010725\n",
      "Train Epoch: 27 [6960/11051 (63%)]\tLoss: 0.032206\n",
      "Train Epoch: 27 [7040/11051 (64%)]\tLoss: 0.024039\n",
      "Train Epoch: 27 [7120/11051 (64%)]\tLoss: 0.025000\n",
      "Train Epoch: 27 [7200/11051 (65%)]\tLoss: 0.015497\n",
      "Train Epoch: 27 [7280/11051 (66%)]\tLoss: 0.014019\n",
      "Train Epoch: 27 [7360/11051 (67%)]\tLoss: 0.043784\n",
      "Train Epoch: 27 [7440/11051 (67%)]\tLoss: 0.026559\n",
      "Train Epoch: 27 [7520/11051 (68%)]\tLoss: 0.020096\n",
      "Train Epoch: 27 [7600/11051 (69%)]\tLoss: 0.014277\n",
      "Train Epoch: 27 [7680/11051 (69%)]\tLoss: 0.016831\n",
      "Train Epoch: 27 [7760/11051 (70%)]\tLoss: 0.019439\n",
      "Train Epoch: 27 [7840/11051 (71%)]\tLoss: 0.056098\n",
      "Train Epoch: 27 [7920/11051 (72%)]\tLoss: 0.011305\n",
      "Train Epoch: 27 [8000/11051 (72%)]\tLoss: 0.033478\n",
      "Train Epoch: 27 [8080/11051 (73%)]\tLoss: 0.038204\n",
      "Train Epoch: 27 [8160/11051 (74%)]\tLoss: 0.022277\n",
      "Train Epoch: 27 [8240/11051 (75%)]\tLoss: 0.024498\n",
      "Train Epoch: 27 [8320/11051 (75%)]\tLoss: 0.029089\n",
      "Train Epoch: 27 [8400/11051 (76%)]\tLoss: 0.017491\n",
      "Train Epoch: 27 [8480/11051 (77%)]\tLoss: 0.005786\n",
      "Train Epoch: 27 [8560/11051 (77%)]\tLoss: 0.033451\n",
      "Train Epoch: 27 [8640/11051 (78%)]\tLoss: 0.033944\n",
      "Train Epoch: 27 [8720/11051 (79%)]\tLoss: 0.019965\n",
      "Train Epoch: 27 [8800/11051 (80%)]\tLoss: 0.018076\n",
      "Train Epoch: 27 [8880/11051 (80%)]\tLoss: 0.013330\n",
      "Train Epoch: 27 [8960/11051 (81%)]\tLoss: 0.027253\n",
      "Train Epoch: 27 [9040/11051 (82%)]\tLoss: 0.036370\n",
      "Train Epoch: 27 [9120/11051 (82%)]\tLoss: 0.027233\n",
      "Train Epoch: 27 [9200/11051 (83%)]\tLoss: 0.008847\n",
      "Train Epoch: 27 [9280/11051 (84%)]\tLoss: 0.023533\n",
      "Train Epoch: 27 [9360/11051 (85%)]\tLoss: 0.025852\n",
      "Train Epoch: 27 [9440/11051 (85%)]\tLoss: 0.034111\n",
      "Train Epoch: 27 [9520/11051 (86%)]\tLoss: 0.020287\n",
      "Train Epoch: 27 [9600/11051 (87%)]\tLoss: 0.007491\n",
      "Train Epoch: 27 [9680/11051 (88%)]\tLoss: 0.032652\n",
      "Train Epoch: 27 [9760/11051 (88%)]\tLoss: 0.026378\n",
      "Train Epoch: 27 [9840/11051 (89%)]\tLoss: 0.022373\n",
      "Train Epoch: 27 [9920/11051 (90%)]\tLoss: 0.023392\n",
      "Train Epoch: 27 [10000/11051 (90%)]\tLoss: 0.018050\n",
      "Train Epoch: 27 [10080/11051 (91%)]\tLoss: 0.020209\n",
      "Train Epoch: 27 [10160/11051 (92%)]\tLoss: 0.033687\n",
      "Train Epoch: 27 [10240/11051 (93%)]\tLoss: 0.015680\n",
      "Train Epoch: 27 [10320/11051 (93%)]\tLoss: 0.014712\n",
      "Train Epoch: 27 [10400/11051 (94%)]\tLoss: 0.022579\n",
      "Train Epoch: 27 [10480/11051 (95%)]\tLoss: 0.017477\n",
      "Train Epoch: 27 [10560/11051 (96%)]\tLoss: 0.014469\n",
      "Train Epoch: 27 [10640/11051 (96%)]\tLoss: 0.015890\n",
      "Train Epoch: 27 [10720/11051 (97%)]\tLoss: 0.012550\n",
      "Train Epoch: 27 [10800/11051 (98%)]\tLoss: 0.013649\n",
      "Train Epoch: 27 [10880/11051 (98%)]\tLoss: 0.041517\n",
      "Train Epoch: 27 [10960/11051 (99%)]\tLoss: 0.014705\n",
      "Train Epoch: 27 [11040/11051 (100%)]\tLoss: 0.023342\n",
      "====> Epoch: 27 Average loss: 0.0216\n",
      "\n",
      "Started training epoch no. 29\n",
      "Train Epoch: 28 [0/11051 (0%)]\tLoss: 0.034096\n",
      "Train Epoch: 28 [80/11051 (1%)]\tLoss: 0.022102\n",
      "Train Epoch: 28 [160/11051 (1%)]\tLoss: 0.014867\n",
      "Train Epoch: 28 [240/11051 (2%)]\tLoss: 0.028359\n",
      "Train Epoch: 28 [320/11051 (3%)]\tLoss: 0.025371\n",
      "Train Epoch: 28 [400/11051 (4%)]\tLoss: 0.056786\n",
      "Train Epoch: 28 [480/11051 (4%)]\tLoss: 0.015591\n",
      "Train Epoch: 28 [560/11051 (5%)]\tLoss: 0.024761\n",
      "Train Epoch: 28 [640/11051 (6%)]\tLoss: 0.009310\n",
      "Train Epoch: 28 [720/11051 (7%)]\tLoss: 0.032646\n",
      "Train Epoch: 28 [800/11051 (7%)]\tLoss: 0.015937\n",
      "Train Epoch: 28 [880/11051 (8%)]\tLoss: 0.010440\n",
      "Train Epoch: 28 [960/11051 (9%)]\tLoss: 0.013077\n",
      "Train Epoch: 28 [1040/11051 (9%)]\tLoss: 0.018401\n",
      "Train Epoch: 28 [1120/11051 (10%)]\tLoss: 0.018373\n",
      "Train Epoch: 28 [1200/11051 (11%)]\tLoss: 0.024571\n",
      "Train Epoch: 28 [1280/11051 (12%)]\tLoss: 0.021079\n",
      "Train Epoch: 28 [1360/11051 (12%)]\tLoss: 0.014791\n",
      "Train Epoch: 28 [1440/11051 (13%)]\tLoss: 0.038834\n",
      "Train Epoch: 28 [1520/11051 (14%)]\tLoss: 0.025013\n",
      "Train Epoch: 28 [1600/11051 (14%)]\tLoss: 0.034715\n",
      "Train Epoch: 28 [1680/11051 (15%)]\tLoss: 0.014056\n",
      "Train Epoch: 28 [1760/11051 (16%)]\tLoss: 0.014117\n",
      "Train Epoch: 28 [1840/11051 (17%)]\tLoss: 0.046418\n",
      "Train Epoch: 28 [1920/11051 (17%)]\tLoss: 0.021889\n",
      "Train Epoch: 28 [2000/11051 (18%)]\tLoss: 0.025091\n",
      "Train Epoch: 28 [2080/11051 (19%)]\tLoss: 0.014617\n",
      "Train Epoch: 28 [2160/11051 (20%)]\tLoss: 0.012927\n",
      "Train Epoch: 28 [2240/11051 (20%)]\tLoss: 0.022517\n",
      "Train Epoch: 28 [2320/11051 (21%)]\tLoss: 0.019699\n",
      "Train Epoch: 28 [2400/11051 (22%)]\tLoss: 0.010376\n",
      "Train Epoch: 28 [2480/11051 (22%)]\tLoss: 0.019499\n",
      "Train Epoch: 28 [2560/11051 (23%)]\tLoss: 0.008536\n",
      "Train Epoch: 28 [2640/11051 (24%)]\tLoss: 0.030488\n",
      "Train Epoch: 28 [2720/11051 (25%)]\tLoss: 0.018181\n",
      "Train Epoch: 28 [2800/11051 (25%)]\tLoss: 0.024420\n",
      "Train Epoch: 28 [2880/11051 (26%)]\tLoss: 0.027381\n",
      "Train Epoch: 28 [2960/11051 (27%)]\tLoss: 0.012835\n",
      "Train Epoch: 28 [3040/11051 (27%)]\tLoss: 0.022689\n",
      "Train Epoch: 28 [3120/11051 (28%)]\tLoss: 0.015333\n",
      "Train Epoch: 28 [3200/11051 (29%)]\tLoss: 0.015220\n",
      "Train Epoch: 28 [3280/11051 (30%)]\tLoss: 0.051232\n",
      "Train Epoch: 28 [3360/11051 (30%)]\tLoss: 0.019968\n",
      "Train Epoch: 28 [3440/11051 (31%)]\tLoss: 0.041518\n",
      "Train Epoch: 28 [3520/11051 (32%)]\tLoss: 0.024417\n",
      "Train Epoch: 28 [3600/11051 (33%)]\tLoss: 0.053677\n",
      "Train Epoch: 28 [3680/11051 (33%)]\tLoss: 0.016966\n",
      "Train Epoch: 28 [3760/11051 (34%)]\tLoss: 0.028729\n",
      "Train Epoch: 28 [3840/11051 (35%)]\tLoss: 0.022308\n",
      "Train Epoch: 28 [3920/11051 (35%)]\tLoss: 0.030793\n",
      "Train Epoch: 28 [4000/11051 (36%)]\tLoss: 0.017932\n",
      "Train Epoch: 28 [4080/11051 (37%)]\tLoss: 0.029698\n",
      "Train Epoch: 28 [4160/11051 (38%)]\tLoss: 0.029766\n",
      "Train Epoch: 28 [4240/11051 (38%)]\tLoss: 0.018275\n",
      "Train Epoch: 28 [4320/11051 (39%)]\tLoss: 0.024162\n",
      "Train Epoch: 28 [4400/11051 (40%)]\tLoss: 0.012502\n",
      "Train Epoch: 28 [4480/11051 (41%)]\tLoss: 0.023588\n",
      "Train Epoch: 28 [4560/11051 (41%)]\tLoss: 0.019831\n",
      "Train Epoch: 28 [4640/11051 (42%)]\tLoss: 0.029769\n",
      "Train Epoch: 28 [4720/11051 (43%)]\tLoss: 0.033192\n",
      "Train Epoch: 28 [4800/11051 (43%)]\tLoss: 0.028692\n",
      "Train Epoch: 28 [4880/11051 (44%)]\tLoss: 0.011449\n",
      "Train Epoch: 28 [4960/11051 (45%)]\tLoss: 0.035202\n",
      "Train Epoch: 28 [5040/11051 (46%)]\tLoss: 0.019994\n",
      "Train Epoch: 28 [5120/11051 (46%)]\tLoss: 0.028654\n",
      "Train Epoch: 28 [5200/11051 (47%)]\tLoss: 0.011043\n",
      "Train Epoch: 28 [5280/11051 (48%)]\tLoss: 0.028845\n",
      "Train Epoch: 28 [5360/11051 (48%)]\tLoss: 0.025834\n",
      "Train Epoch: 28 [5440/11051 (49%)]\tLoss: 0.018957\n",
      "Train Epoch: 28 [5520/11051 (50%)]\tLoss: 0.012534\n",
      "Train Epoch: 28 [5600/11051 (51%)]\tLoss: 0.032755\n",
      "Train Epoch: 28 [5680/11051 (51%)]\tLoss: 0.021197\n",
      "Train Epoch: 28 [5760/11051 (52%)]\tLoss: 0.039791\n",
      "Train Epoch: 28 [5840/11051 (53%)]\tLoss: 0.012811\n",
      "Train Epoch: 28 [5920/11051 (54%)]\tLoss: 0.015841\n",
      "Train Epoch: 28 [6000/11051 (54%)]\tLoss: 0.015326\n",
      "Train Epoch: 28 [6080/11051 (55%)]\tLoss: 0.012623\n",
      "Train Epoch: 28 [6160/11051 (56%)]\tLoss: 0.011313\n",
      "Train Epoch: 28 [6240/11051 (56%)]\tLoss: 0.015160\n",
      "Train Epoch: 28 [6320/11051 (57%)]\tLoss: 0.009079\n",
      "Train Epoch: 28 [6400/11051 (58%)]\tLoss: 0.020249\n",
      "Train Epoch: 28 [6480/11051 (59%)]\tLoss: 0.024633\n",
      "Train Epoch: 28 [6560/11051 (59%)]\tLoss: 0.036059\n",
      "Train Epoch: 28 [6640/11051 (60%)]\tLoss: 0.011189\n",
      "Train Epoch: 28 [6720/11051 (61%)]\tLoss: 0.014116\n",
      "Train Epoch: 28 [6800/11051 (62%)]\tLoss: 0.031786\n",
      "Train Epoch: 28 [6880/11051 (62%)]\tLoss: 0.015073\n",
      "Train Epoch: 28 [6960/11051 (63%)]\tLoss: 0.022600\n",
      "Train Epoch: 28 [7040/11051 (64%)]\tLoss: 0.015468\n",
      "Train Epoch: 28 [7120/11051 (64%)]\tLoss: 0.015754\n",
      "Train Epoch: 28 [7200/11051 (65%)]\tLoss: 0.030273\n",
      "Train Epoch: 28 [7280/11051 (66%)]\tLoss: 0.028219\n",
      "Train Epoch: 28 [7360/11051 (67%)]\tLoss: 0.013596\n",
      "Train Epoch: 28 [7440/11051 (67%)]\tLoss: 0.022887\n",
      "Train Epoch: 28 [7520/11051 (68%)]\tLoss: 0.030050\n",
      "Train Epoch: 28 [7600/11051 (69%)]\tLoss: 0.021142\n",
      "Train Epoch: 28 [7680/11051 (69%)]\tLoss: 0.027792\n",
      "Train Epoch: 28 [7760/11051 (70%)]\tLoss: 0.023229\n",
      "Train Epoch: 28 [7840/11051 (71%)]\tLoss: 0.033356\n",
      "Train Epoch: 28 [7920/11051 (72%)]\tLoss: 0.016801\n",
      "Train Epoch: 28 [8000/11051 (72%)]\tLoss: 0.015351\n",
      "Train Epoch: 28 [8080/11051 (73%)]\tLoss: 0.022134\n",
      "Train Epoch: 28 [8160/11051 (74%)]\tLoss: 0.015609\n",
      "Train Epoch: 28 [8240/11051 (75%)]\tLoss: 0.017649\n",
      "Train Epoch: 28 [8320/11051 (75%)]\tLoss: 0.025143\n",
      "Train Epoch: 28 [8400/11051 (76%)]\tLoss: 0.011015\n",
      "Train Epoch: 28 [8480/11051 (77%)]\tLoss: 0.015137\n",
      "Train Epoch: 28 [8560/11051 (77%)]\tLoss: 0.007904\n",
      "Train Epoch: 28 [8640/11051 (78%)]\tLoss: 0.029440\n",
      "Train Epoch: 28 [8720/11051 (79%)]\tLoss: 0.013777\n",
      "Train Epoch: 28 [8800/11051 (80%)]\tLoss: 0.028313\n",
      "Train Epoch: 28 [8880/11051 (80%)]\tLoss: 0.022122\n",
      "Train Epoch: 28 [8960/11051 (81%)]\tLoss: 0.023288\n",
      "Train Epoch: 28 [9040/11051 (82%)]\tLoss: 0.033261\n",
      "Train Epoch: 28 [9120/11051 (82%)]\tLoss: 0.011786\n",
      "Train Epoch: 28 [9200/11051 (83%)]\tLoss: 0.016481\n",
      "Train Epoch: 28 [9280/11051 (84%)]\tLoss: 0.042006\n",
      "Train Epoch: 28 [9360/11051 (85%)]\tLoss: 0.009737\n",
      "Train Epoch: 28 [9440/11051 (85%)]\tLoss: 0.026268\n",
      "Train Epoch: 28 [9520/11051 (86%)]\tLoss: 0.013723\n",
      "Train Epoch: 28 [9600/11051 (87%)]\tLoss: 0.029555\n",
      "Train Epoch: 28 [9680/11051 (88%)]\tLoss: 0.019868\n",
      "Train Epoch: 28 [9760/11051 (88%)]\tLoss: 0.018437\n",
      "Train Epoch: 28 [9840/11051 (89%)]\tLoss: 0.031533\n",
      "Train Epoch: 28 [9920/11051 (90%)]\tLoss: 0.016548\n",
      "Train Epoch: 28 [10000/11051 (90%)]\tLoss: 0.011831\n",
      "Train Epoch: 28 [10080/11051 (91%)]\tLoss: 0.026638\n",
      "Train Epoch: 28 [10160/11051 (92%)]\tLoss: 0.022002\n",
      "Train Epoch: 28 [10240/11051 (93%)]\tLoss: 0.018898\n",
      "Train Epoch: 28 [10320/11051 (93%)]\tLoss: 0.031126\n",
      "Train Epoch: 28 [10400/11051 (94%)]\tLoss: 0.017473\n",
      "Train Epoch: 28 [10480/11051 (95%)]\tLoss: 0.025344\n",
      "Train Epoch: 28 [10560/11051 (96%)]\tLoss: 0.014229\n",
      "Train Epoch: 28 [10640/11051 (96%)]\tLoss: 0.013566\n",
      "Train Epoch: 28 [10720/11051 (97%)]\tLoss: 0.020288\n",
      "Train Epoch: 28 [10800/11051 (98%)]\tLoss: 0.018483\n",
      "Train Epoch: 28 [10880/11051 (98%)]\tLoss: 0.006918\n",
      "Train Epoch: 28 [10960/11051 (99%)]\tLoss: 0.024484\n",
      "Train Epoch: 28 [11040/11051 (100%)]\tLoss: 0.027483\n",
      "====> Epoch: 28 Average loss: 0.0215\n",
      "\n",
      "Started training epoch no. 30\n",
      "Train Epoch: 29 [0/11051 (0%)]\tLoss: 0.021549\n",
      "Train Epoch: 29 [80/11051 (1%)]\tLoss: 0.021452\n",
      "Train Epoch: 29 [160/11051 (1%)]\tLoss: 0.044770\n",
      "Train Epoch: 29 [240/11051 (2%)]\tLoss: 0.016102\n",
      "Train Epoch: 29 [320/11051 (3%)]\tLoss: 0.025055\n",
      "Train Epoch: 29 [400/11051 (4%)]\tLoss: 0.011790\n",
      "Train Epoch: 29 [480/11051 (4%)]\tLoss: 0.005615\n",
      "Train Epoch: 29 [560/11051 (5%)]\tLoss: 0.013365\n",
      "Train Epoch: 29 [640/11051 (6%)]\tLoss: 0.044683\n",
      "Train Epoch: 29 [720/11051 (7%)]\tLoss: 0.009995\n",
      "Train Epoch: 29 [800/11051 (7%)]\tLoss: 0.021367\n",
      "Train Epoch: 29 [880/11051 (8%)]\tLoss: 0.010751\n",
      "Train Epoch: 29 [960/11051 (9%)]\tLoss: 0.012199\n",
      "Train Epoch: 29 [1040/11051 (9%)]\tLoss: 0.035342\n",
      "Train Epoch: 29 [1120/11051 (10%)]\tLoss: 0.041114\n",
      "Train Epoch: 29 [1200/11051 (11%)]\tLoss: 0.034620\n",
      "Train Epoch: 29 [1280/11051 (12%)]\tLoss: 0.029300\n",
      "Train Epoch: 29 [1360/11051 (12%)]\tLoss: 0.019340\n",
      "Train Epoch: 29 [1440/11051 (13%)]\tLoss: 0.019191\n",
      "Train Epoch: 29 [1520/11051 (14%)]\tLoss: 0.028831\n",
      "Train Epoch: 29 [1600/11051 (14%)]\tLoss: 0.014414\n",
      "Train Epoch: 29 [1680/11051 (15%)]\tLoss: 0.014061\n",
      "Train Epoch: 29 [1760/11051 (16%)]\tLoss: 0.021799\n",
      "Train Epoch: 29 [1840/11051 (17%)]\tLoss: 0.016994\n",
      "Train Epoch: 29 [1920/11051 (17%)]\tLoss: 0.023323\n",
      "Train Epoch: 29 [2000/11051 (18%)]\tLoss: 0.039113\n",
      "Train Epoch: 29 [2080/11051 (19%)]\tLoss: 0.023522\n",
      "Train Epoch: 29 [2160/11051 (20%)]\tLoss: 0.015791\n",
      "Train Epoch: 29 [2240/11051 (20%)]\tLoss: 0.021760\n",
      "Train Epoch: 29 [2320/11051 (21%)]\tLoss: 0.032096\n",
      "Train Epoch: 29 [2400/11051 (22%)]\tLoss: 0.007551\n",
      "Train Epoch: 29 [2480/11051 (22%)]\tLoss: 0.023521\n",
      "Train Epoch: 29 [2560/11051 (23%)]\tLoss: 0.010618\n",
      "Train Epoch: 29 [2640/11051 (24%)]\tLoss: 0.012132\n",
      "Train Epoch: 29 [2720/11051 (25%)]\tLoss: 0.060116\n",
      "Train Epoch: 29 [2800/11051 (25%)]\tLoss: 0.027303\n",
      "Train Epoch: 29 [2880/11051 (26%)]\tLoss: 0.032398\n",
      "Train Epoch: 29 [2960/11051 (27%)]\tLoss: 0.028788\n",
      "Train Epoch: 29 [3040/11051 (27%)]\tLoss: 0.038526\n",
      "Train Epoch: 29 [3120/11051 (28%)]\tLoss: 0.028018\n",
      "Train Epoch: 29 [3200/11051 (29%)]\tLoss: 0.014390\n",
      "Train Epoch: 29 [3280/11051 (30%)]\tLoss: 0.035745\n",
      "Train Epoch: 29 [3360/11051 (30%)]\tLoss: 0.031252\n",
      "Train Epoch: 29 [3440/11051 (31%)]\tLoss: 0.006727\n",
      "Train Epoch: 29 [3520/11051 (32%)]\tLoss: 0.015516\n",
      "Train Epoch: 29 [3600/11051 (33%)]\tLoss: 0.031464\n",
      "Train Epoch: 29 [3680/11051 (33%)]\tLoss: 0.035538\n",
      "Train Epoch: 29 [3760/11051 (34%)]\tLoss: 0.009487\n",
      "Train Epoch: 29 [3840/11051 (35%)]\tLoss: 0.013955\n",
      "Train Epoch: 29 [3920/11051 (35%)]\tLoss: 0.033238\n",
      "Train Epoch: 29 [4000/11051 (36%)]\tLoss: 0.009856\n",
      "Train Epoch: 29 [4080/11051 (37%)]\tLoss: 0.043648\n",
      "Train Epoch: 29 [4160/11051 (38%)]\tLoss: 0.031026\n",
      "Train Epoch: 29 [4240/11051 (38%)]\tLoss: 0.028132\n",
      "Train Epoch: 29 [4320/11051 (39%)]\tLoss: 0.018647\n",
      "Train Epoch: 29 [4400/11051 (40%)]\tLoss: 0.024118\n",
      "Train Epoch: 29 [4480/11051 (41%)]\tLoss: 0.017643\n",
      "Train Epoch: 29 [4560/11051 (41%)]\tLoss: 0.009641\n",
      "Train Epoch: 29 [4640/11051 (42%)]\tLoss: 0.030933\n",
      "Train Epoch: 29 [4720/11051 (43%)]\tLoss: 0.021170\n",
      "Train Epoch: 29 [4800/11051 (43%)]\tLoss: 0.019664\n",
      "Train Epoch: 29 [4880/11051 (44%)]\tLoss: 0.027423\n",
      "Train Epoch: 29 [4960/11051 (45%)]\tLoss: 0.047953\n",
      "Train Epoch: 29 [5040/11051 (46%)]\tLoss: 0.020299\n",
      "Train Epoch: 29 [5120/11051 (46%)]\tLoss: 0.018444\n",
      "Train Epoch: 29 [5200/11051 (47%)]\tLoss: 0.017051\n",
      "Train Epoch: 29 [5280/11051 (48%)]\tLoss: 0.013724\n",
      "Train Epoch: 29 [5360/11051 (48%)]\tLoss: 0.021311\n",
      "Train Epoch: 29 [5440/11051 (49%)]\tLoss: 0.013812\n",
      "Train Epoch: 29 [5520/11051 (50%)]\tLoss: 0.022196\n",
      "Train Epoch: 29 [5600/11051 (51%)]\tLoss: 0.020760\n",
      "Train Epoch: 29 [5680/11051 (51%)]\tLoss: 0.031137\n",
      "Train Epoch: 29 [5760/11051 (52%)]\tLoss: 0.036713\n",
      "Train Epoch: 29 [5840/11051 (53%)]\tLoss: 0.016161\n",
      "Train Epoch: 29 [5920/11051 (54%)]\tLoss: 0.004874\n",
      "Train Epoch: 29 [6000/11051 (54%)]\tLoss: 0.008396\n",
      "Train Epoch: 29 [6080/11051 (55%)]\tLoss: 0.015396\n",
      "Train Epoch: 29 [6160/11051 (56%)]\tLoss: 0.019603\n",
      "Train Epoch: 29 [6240/11051 (56%)]\tLoss: 0.036644\n",
      "Train Epoch: 29 [6320/11051 (57%)]\tLoss: 0.018272\n",
      "Train Epoch: 29 [6400/11051 (58%)]\tLoss: 0.016475\n",
      "Train Epoch: 29 [6480/11051 (59%)]\tLoss: 0.014650\n",
      "Train Epoch: 29 [6560/11051 (59%)]\tLoss: 0.007050\n",
      "Train Epoch: 29 [6640/11051 (60%)]\tLoss: 0.014562\n",
      "Train Epoch: 29 [6720/11051 (61%)]\tLoss: 0.007716\n",
      "Train Epoch: 29 [6800/11051 (62%)]\tLoss: 0.025268\n",
      "Train Epoch: 29 [6880/11051 (62%)]\tLoss: 0.019619\n",
      "Train Epoch: 29 [6960/11051 (63%)]\tLoss: 0.029061\n",
      "Train Epoch: 29 [7040/11051 (64%)]\tLoss: 0.021922\n",
      "Train Epoch: 29 [7120/11051 (64%)]\tLoss: 0.014420\n",
      "Train Epoch: 29 [7200/11051 (65%)]\tLoss: 0.016929\n",
      "Train Epoch: 29 [7280/11051 (66%)]\tLoss: 0.010973\n",
      "Train Epoch: 29 [7360/11051 (67%)]\tLoss: 0.005798\n",
      "Train Epoch: 29 [7440/11051 (67%)]\tLoss: 0.027365\n",
      "Train Epoch: 29 [7520/11051 (68%)]\tLoss: 0.019060\n",
      "Train Epoch: 29 [7600/11051 (69%)]\tLoss: 0.026487\n",
      "Train Epoch: 29 [7680/11051 (69%)]\tLoss: 0.013242\n",
      "Train Epoch: 29 [7760/11051 (70%)]\tLoss: 0.007330\n",
      "Train Epoch: 29 [7840/11051 (71%)]\tLoss: 0.017827\n",
      "Train Epoch: 29 [7920/11051 (72%)]\tLoss: 0.045508\n",
      "Train Epoch: 29 [8000/11051 (72%)]\tLoss: 0.029019\n",
      "Train Epoch: 29 [8080/11051 (73%)]\tLoss: 0.041477\n",
      "Train Epoch: 29 [8160/11051 (74%)]\tLoss: 0.009310\n",
      "Train Epoch: 29 [8240/11051 (75%)]\tLoss: 0.018174\n",
      "Train Epoch: 29 [8320/11051 (75%)]\tLoss: 0.027118\n",
      "Train Epoch: 29 [8400/11051 (76%)]\tLoss: 0.014412\n",
      "Train Epoch: 29 [8480/11051 (77%)]\tLoss: 0.030617\n",
      "Train Epoch: 29 [8560/11051 (77%)]\tLoss: 0.031860\n",
      "Train Epoch: 29 [8640/11051 (78%)]\tLoss: 0.012790\n",
      "Train Epoch: 29 [8720/11051 (79%)]\tLoss: 0.016434\n",
      "Train Epoch: 29 [8800/11051 (80%)]\tLoss: 0.021036\n",
      "Train Epoch: 29 [8880/11051 (80%)]\tLoss: 0.016797\n",
      "Train Epoch: 29 [8960/11051 (81%)]\tLoss: 0.017397\n",
      "Train Epoch: 29 [9040/11051 (82%)]\tLoss: 0.040026\n",
      "Train Epoch: 29 [9120/11051 (82%)]\tLoss: 0.017028\n",
      "Train Epoch: 29 [9200/11051 (83%)]\tLoss: 0.013482\n",
      "Train Epoch: 29 [9280/11051 (84%)]\tLoss: 0.016663\n",
      "Train Epoch: 29 [9360/11051 (85%)]\tLoss: 0.064436\n",
      "Train Epoch: 29 [9440/11051 (85%)]\tLoss: 0.020290\n",
      "Train Epoch: 29 [9520/11051 (86%)]\tLoss: 0.023859\n",
      "Train Epoch: 29 [9600/11051 (87%)]\tLoss: 0.029377\n",
      "Train Epoch: 29 [9680/11051 (88%)]\tLoss: 0.017737\n",
      "Train Epoch: 29 [9760/11051 (88%)]\tLoss: 0.020873\n",
      "Train Epoch: 29 [9840/11051 (89%)]\tLoss: 0.037970\n",
      "Train Epoch: 29 [9920/11051 (90%)]\tLoss: 0.023820\n",
      "Train Epoch: 29 [10000/11051 (90%)]\tLoss: 0.035742\n",
      "Train Epoch: 29 [10080/11051 (91%)]\tLoss: 0.003172\n",
      "Train Epoch: 29 [10160/11051 (92%)]\tLoss: 0.015101\n",
      "Train Epoch: 29 [10240/11051 (93%)]\tLoss: 0.018419\n",
      "Train Epoch: 29 [10320/11051 (93%)]\tLoss: 0.010365\n",
      "Train Epoch: 29 [10400/11051 (94%)]\tLoss: 0.011158\n",
      "Train Epoch: 29 [10480/11051 (95%)]\tLoss: 0.032479\n",
      "Train Epoch: 29 [10560/11051 (96%)]\tLoss: 0.019980\n",
      "Train Epoch: 29 [10640/11051 (96%)]\tLoss: 0.008259\n",
      "Train Epoch: 29 [10720/11051 (97%)]\tLoss: 0.021220\n",
      "Train Epoch: 29 [10800/11051 (98%)]\tLoss: 0.014086\n",
      "Train Epoch: 29 [10880/11051 (98%)]\tLoss: 0.023231\n",
      "Train Epoch: 29 [10960/11051 (99%)]\tLoss: 0.020121\n",
      "Train Epoch: 29 [11040/11051 (100%)]\tLoss: 0.024356\n",
      "====> Epoch: 29 Average loss: 0.0216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_log = train_epoch(train_loader,model,loss_function,optimizer,num_epochs= EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 0, 'train_loss': 0.5167462586677282},\n",
       " {'epoch': 1, 'train_loss': 0.28524630707210163},\n",
       " {'epoch': 2, 'train_loss': 0.22405595366617964},\n",
       " {'epoch': 3, 'train_loss': 0.20420735602451734},\n",
       " {'epoch': 4, 'train_loss': 0.19710183489188962},\n",
       " {'epoch': 5, 'train_loss': 0.1930222342170872},\n",
       " {'epoch': 6, 'train_loss': 0.19041850625951107},\n",
       " {'epoch': 7, 'train_loss': 0.18789771827296306},\n",
       " {'epoch': 8, 'train_loss': 0.1866707818032756},\n",
       " {'epoch': 9, 'train_loss': 0.18474156394811123},\n",
       " {'epoch': 10, 'train_loss': 0.18310086487478397},\n",
       " {'epoch': 11, 'train_loss': 0.18228452956417168},\n",
       " {'epoch': 12, 'train_loss': 0.1804202217783903},\n",
       " {'epoch': 13, 'train_loss': 0.17962252591533098},\n",
       " {'epoch': 14, 'train_loss': 0.17927590616494848},\n",
       " {'epoch': 15, 'train_loss': 0.17819420022919624},\n",
       " {'epoch': 16, 'train_loss': 0.17817324729213105},\n",
       " {'epoch': 17, 'train_loss': 0.17705244246557592},\n",
       " {'epoch': 18, 'train_loss': 0.17651008013177624},\n",
       " {'epoch': 19, 'train_loss': 0.1758238716555056},\n",
       " {'epoch': 20, 'train_loss': 0.1754280233151898},\n",
       " {'epoch': 21, 'train_loss': 0.1752233298578975},\n",
       " {'epoch': 22, 'train_loss': 0.1745453198075122},\n",
       " {'epoch': 23, 'train_loss': 0.17409851896143944},\n",
       " {'epoch': 24, 'train_loss': 0.1739082567180829},\n",
       " {'epoch': 25, 'train_loss': 0.17311459151534675},\n",
       " {'epoch': 26, 'train_loss': 0.1732620823004333},\n",
       " {'epoch': 27, 'train_loss': 0.1730565334867023},\n",
       " {'epoch': 28, 'train_loss': 0.17228411232672536},\n",
       " {'epoch': 29, 'train_loss': 0.17262710102694986}]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-defined VAE FEATURES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP/code/py_4/get_all_features.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['co_authors']=df.authors.apply( lambda x: [i['name'] for i in x] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old scaler\n",
      "['MESH NAME NOT FOUND: Mitophagy', 'MESH NAME NOT FOUND: Female', \"MESH NAME NOT FOUND: Practice Patterns, Physicians'\", 'MESH NAME NOT FOUND: Broadly Neutralizing Antibodies', 'MESH NAME NOT FOUND: B-Cell Lymphoma 3 Protein', 'MESH NAME NOT FOUND: Conditioning, Psychological', 'MESH NAME NOT FOUND: Inhibition, Psychological', 'MESH NAME NOT FOUND: Discrimination, Psychological', 'MESH NAME NOT FOUND: Chlorocebus aethiops', 'MESH NAME NOT FOUND: Confounding Factors, Epidemiologic', 'MESH NAME NOT FOUND: Male', 'MESH NAME NOT FOUND: Recognition, Psychology', 'MESH NAME NOT FOUND: Phosphoinositide-3 Kinase Inhibitors', 'MESH NAME NOT FOUND: Infections', 'MESH NAME NOT FOUND: Stimuli Responsive Polymers', 'MESH NAME NOT FOUND: Copper-Transporting ATPases']\n"
     ]
    }
   ],
   "source": [
    "test_set = ToyDS(df, selection_test, vae_features = train_set.__getvae__())\n",
    "test_loader=DataLoader(dataset= test_set, batch_size = batch_size, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2570, 64)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0255\n"
     ]
    }
   ],
   "source": [
    "x_hat_test = test_model(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2570, 64)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7127687     10\n",
      "10594466     8\n",
      "11990780     1\n",
      "Name: PI_IDS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "selection = [auth_usecase[109]]\n",
    "print(df[df['last_author_name'].isin(selection)][\"PI_IDS\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-defined VAE FEATURES\n",
      "Using old scaler\n",
      "['MESH NAME NOT FOUND: Female', \"MESH NAME NOT FOUND: Practice Patterns, Physicians'\", 'MESH NAME NOT FOUND: Phosphoinositide-3 Kinase Inhibitors', 'MESH NAME NOT FOUND: Chlorocebus aethiops', 'MESH NAME NOT FOUND: Male', 'MESH NAME NOT FOUND: Phospholipid Hydroperoxide Glutathione Peroxidase', 'MESH NAME NOT FOUND: Outcome Assessment, Health Care', 'MESH NAME NOT FOUND: Diet, Healthy', 'MESH NAME NOT FOUND: Copper-Transporting ATPases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP/code/py_4/get_all_features.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['co_authors']=df.authors.apply( lambda x: [i['name'] for i in x] )\n"
     ]
    }
   ],
   "source": [
    "train_vae = train_set.__getvae__()\n",
    "test_set = ToyDS(df, selection,train_vae)\n",
    "test_loader=DataLoader(dataset= test_set, batch_size = batch_size, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 32)\n",
      "(19, 64)\n"
     ]
    }
   ],
   "source": [
    "bottle_neck=[]\n",
    "recon_batchs = []\n",
    "for batch_idx, data in enumerate(test_loader):\n",
    "            data = data.to(device, dtype=torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar, _= model(data)\n",
    "            if cuda:\n",
    "                bottle_neck.extend(mu.cpu().detach().numpy())\n",
    "                recon_batchs.extend(recon_batch.cpu().detach().numpy())\n",
    "            else:\n",
    "                bottle_neck.extend(mu.detach().numpy())\n",
    "                recon_batchs.extend(recon_batch.detach().numpy())\n",
    "bottle_neck = np.array(bottle_neck)\n",
    "recon_batchs = np.array(recon_batchs)\n",
    "print(bottle_neck.shape)\n",
    "print(recon_batchs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 4 nearest neighbors...\n",
      "[t-SNE] Indexed 19 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 19 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 19 / 19\n",
      "[t-SNE] Mean sigma: 0.634541\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 72.133377\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.337330\n",
      "t-SNE done! Time elapsed: 0.14081168174743652 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=1, n_iter=1000,random_state=42)\n",
    "tsne_results = tsne.fit_transform(bottle_neck)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset=pd.DataFrame()\n",
    "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "df_subset['PI'] = list(df[df['last_author_name'].isin(selection)][\"PI_IDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = df_subset.groupby('PI')\n",
    "markers = ['o','v',\"x\"]\n",
    "colors = [\"red\",\"black\",\"green\"]\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    ax.set_title(\"TSNE of the Mesh Embeddings\")\n",
    "    ax.plot(group['tsne-2d-one'], group['tsne-2d-two'], marker=markers[i], linestyle='', ms=12, label=name, color=colors[i])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.tight_layout();\n",
    "\n",
    "# fig.savefig('code/img/TSNE_Plots/mesh.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANITY TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ON INPUT DIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 7 nearest neighbors...\n",
      "[t-SNE] Indexed 19 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 19 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 19 / 19\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 75.712021\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.240680\n",
      "t-SNE done! Time elapsed: 0.14115023612976074 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=2, n_iter=1000, random_state=42)\n",
    "tsne_inputs = tsne.fit_transform(test_set.features)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "input_testset=pd.DataFrame()\n",
    "input_testset['tsne-2d-one'] = tsne_inputs[:,0]\n",
    "input_testset['tsne-2d-two'] = tsne_inputs[:,1]\n",
    "input_testset['PI'] = list(df[df['last_author_name'].isin(selection)][\"PI_IDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = input_testset.groupby('PI')\n",
    "markers = ['o','v',\"x\"]\n",
    "colors = [\"red\",\"black\",\"green\"]\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    ax.set_title(\"TSNE of the Mesh Embeddings\")\n",
    "    ax.plot(group['tsne-2d-one'], group['tsne-2d-two'], marker=markers[i], linestyle='', ms=12, label=name, color=colors[i])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.tight_layout();\n",
    "\n",
    "# fig.savefig('code/img/TSNE_Plots/mesh.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT DIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 7 nearest neighbors...\n",
      "[t-SNE] Indexed 19 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 19 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 19 / 19\n",
      "[t-SNE] Mean sigma: 0.000622\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 69.051392\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.264274\n",
      "t-SNE done! Time elapsed: 0.14203095436096191 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=2, n_iter=1000,random_state=42)\n",
    "tsne_output = tsne.fit_transform(recon_batchs)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "output_testset=pd.DataFrame()\n",
    "output_testset['tsne-2d-one'] = tsne_output[:,0]\n",
    "output_testset['tsne-2d-two'] = tsne_output[:,1]\n",
    "output_testset['PI'] = list(df[df['last_author_name'].isin(selection)][\"PI_IDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = output_testset.groupby('PI')\n",
    "markers = ['o','v',\"x\"]\n",
    "colors = [\"red\",\"black\",\"green\"]\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    ax.set_title(\"TSNE of the Mesh Embeddings\")\n",
    "    ax.plot(group['tsne-2d-one'], group['tsne-2d-two'], marker=markers[i], linestyle='', ms=12, label=name, color=colors[i])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.tight_layout();\n",
    "\n",
    "# fig.savefig('code/img/TSNE_Plots/mesh.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO MANS LAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04696874, -0.00745973,  0.02219138, -0.01152058,  0.07750552,\n",
       "       -0.05652783,  0.0463714 ,  0.04440367,  0.01624255, -0.04389438,\n",
       "        0.00964473,  0.02568895,  0.0059289 ,  0.0445687 ,  0.01514376,\n",
       "       -0.00167513, -0.00547519, -0.12151582,  0.04825244,  0.01406206,\n",
       "       -0.0263554 , -0.05763874,  0.03104352, -0.0210744 ,  0.06944894,\n",
       "       -0.00496328, -0.00685616,  0.02910295, -0.0491034 , -0.01197104,\n",
       "        0.03281267,  0.03191182, -0.00297394,  0.00091517,  0.00926172,\n",
       "        0.05470938,  0.03836613, -0.05525067,  0.02519614, -0.02490205,\n",
       "        0.05078975, -0.06835574,  0.002885  ,  0.01828441, -0.03423334,\n",
       "        0.02593969,  0.01156024,  0.00615533, -0.02579997,  0.0045107 ,\n",
       "       -0.00383745, -0.04143269,  0.02803731,  0.05674217, -0.03228447,\n",
       "        0.01210703,  0.00111895,  0.00200509,  0.05567061, -0.02285567,\n",
       "        0.00041651, -0.00045409,  0.00020516,  0.01223247])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_set.features,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08774614,  0.02092163,  0.12571853, -0.14216335,  0.07720881,\n",
       "       -0.15472281,  0.14757884,  0.06413764,  0.02327864, -0.06940445,\n",
       "        0.06744818,  0.06021205,  0.05749616,  0.07097321, -0.00946512,\n",
       "        0.10901033,  0.04193162, -0.16763069,  0.05569793,  0.02644674,\n",
       "       -0.12107595, -0.01172395, -0.00654568,  0.02205859, -0.03049977,\n",
       "        0.00936542, -0.03355218,  0.06486404, -0.01426596, -0.03592556,\n",
       "        0.02135579,  0.03412693,  0.05554745, -0.12500796,  0.05881401,\n",
       "       -0.00335602,  0.09905915, -0.09426896,  0.05038181,  0.00551764,\n",
       "        0.0877351 , -0.02228821,  0.06608629, -0.07717218, -0.12320602,\n",
       "        0.04910495, -0.09248894,  0.08001823, -0.02947531, -0.02167947,\n",
       "       -0.07819159, -0.06418394, -0.00188549,  0.03362697, -0.17517133,\n",
       "        0.05104881,  0.02952922, -0.03766378,  0.07698775,  0.08726925,\n",
       "       -0.09581321,  0.04931774, -0.03959464,  0.03322336], dtype=float32)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_hat_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0439939456221106"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(test_set.features - x_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48664856,  0.5514309 ,  0.19388752, -0.42018056, -0.3232441 ,\n",
       "        0.67790425,  0.6908368 , -0.07615533, -0.1124628 , -0.14229044,\n",
       "        1.2183169 ,  0.86753476,  0.14473648,  0.08594865,  0.56901103,\n",
       "       -0.14818126,  0.10047996, -0.4192718 ,  0.0617242 ,  0.13661231,\n",
       "        0.52298164,  0.18177733, -0.2841749 ,  0.1585145 ,  0.03479505,\n",
       "        0.2453199 ,  0.04435132, -0.7162493 ,  0.4566654 ,  0.23147944,\n",
       "       -0.04918338, -0.00181377,  0.07591151, -0.09561956, -0.01763365,\n",
       "        0.23789018, -0.36632562,  0.11252811, -0.3439671 , -0.3704161 ,\n",
       "       -1.0691663 ,  0.6125722 ,  0.22568542,  0.03408474,  0.06912109,\n",
       "       -0.10103416, -1.7580557 , -1.5552766 ,  0.5394255 , -0.4705413 ,\n",
       "       -0.17830682,  0.39068723, -0.8519199 , -0.9528599 , -0.3563116 ,\n",
       "       -0.16227925, -0.26426497, -0.67982227,  0.32561398,  0.48522398,\n",
       "       -0.00678802, -0.17770773,  0.8744967 , -1.0235527 ], dtype=float32)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
