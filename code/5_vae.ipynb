{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Using cached elasticsearch-7.8.0-py2.py3-none-any.whl (188 kB)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch) (1.25.8)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.8.0\n",
      "Collecting elasticsearch_dsl\n",
      "  Using cached elasticsearch_dsl-7.2.1-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (1.14.0)\n",
      "Requirement already satisfied: elasticsearch<8.0.0,>=7.0.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (7.8.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch_dsl) (2.8.1)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch_dsl) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch_dsl) (1.25.8)\n",
      "Installing collected packages: elasticsearch-dsl\n",
      "Successfully installed elasticsearch-dsl-7.2.1\n",
      "Collecting pymed\n",
      "  Using cached pymed-0.8.9-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pymed) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.20.0->pymed) (3.0.4)\n",
      "Installing collected packages: pymed\n",
      "Successfully installed pymed-0.8.9\n",
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 29.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Using cached smart_open-2.1.0.tar.gz (116 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.14.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.5->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.5->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110317 sha256=de83e5c41cbe349a05fa85db4cd05793b9eb1c02dc87a18c399f3b5b0096e369\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a4/9b/d5/85705a7ab783cd6f7bd718f01d3b1396272f30044e3c36401a\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-2.1.0\n",
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "USE_SERVER = True\n",
    "if USE_SERVER:\n",
    "    !pip install elasticsearch\n",
    "    !pip install elasticsearch_dsl\n",
    "    !pip install pymed\n",
    "    !pip install gensim\n",
    "    !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import utils\n",
    "import os\n",
    "from utils import PROJECT_ROOT, DATA_PATH\n",
    "import yuval_module.paper_source as PaperSource\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import py_4.get_mesh_vec as get_mesh_vec\n",
    "import py_3.sim_matrix_3 as sim_matrix_3\n",
    "import py_4.get_all_features as get_all_features \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_embed=get_mesh_vec.MeshEmbeddings(PROJECT_ROOT + \"data/mesh_data/MeSHFeatureGeneratedByDeepWalk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PULLING FROM S3\n",
      "FILE PULLED\n"
     ]
    }
   ],
   "source": [
    "FILE = \"enriched_labeled_dataset_large\" \n",
    "if os.path.exists(PROJECT_ROOT + DATA_PATH + FILE):\n",
    "    print(\"READING FROM LOCAL\")\n",
    "    if FILE.split(\".\")[1] == \"json\":\n",
    "        df = pd.read_json(PROJECT_ROOT + DATA_PATH + FILE)\n",
    "    else:\n",
    "        df = pd.read_csv(PROJECT_ROOT + DATA_PATH + FILE)\n",
    "    #ps = PaperSource()\n",
    "else:\n",
    "    print(\"PULLING FROM S3\")\n",
    "    ps = sim_matrix_3.load_dataset(FILE)\n",
    "    df = ps.get_dataset()\n",
    "\n",
    "df.drop(columns=[\"last_author_country\"],inplace=True)\n",
    "df.rename(columns={'ORG_STATE':'last_author_country'},inplace=True)\n",
    "\n",
    "print(\"FILE PULLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_core = pd.read_csv(\"data/train_set_author_names.csv\")[\"0\"]\n",
    "auth_eps = pd.read_csv(\"data/val_set_author_names.csv\")[\"0\"]\n",
    "auth_usecase = pd.read_csv(\"data/test_set_author_names.csv\")[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = list(set(df['last_author_name']) - set(auth_usecase))[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#settings\n",
    "\n",
    "batch_size= 8\n",
    "epochs = 2\n",
    "cuda = torch.cuda.is_available()\n",
    "seed = 42\n",
    "log_interval = 5\n",
    "num_workers = 2\n",
    "\n",
    "#check for cuda\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDS(Dataset):\n",
    "    def __init__(self,df,selection,vae_features = None):\n",
    "        super().__init__()\n",
    "        self.df = df[df['last_author_name'].isin(selection)]\n",
    "        if vae_features is None:\n",
    "            print(\"Creating new VAE FEATURES\")\n",
    "            self.vae_features = get_all_features.VAE_Features(self.df)\n",
    "        else:\n",
    "            print(\"Using pre-defined VAE FEATURES\")\n",
    "            self.vae_features = vae_features\n",
    "        self.features = self.vae_features.get_all_features(self.df)\n",
    "        print(list(self.vae_features.mesh_features.mesh_missing))\n",
    "        self.input_dim = self.vae_features.input_dims\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        return features\n",
    "    \n",
    "    def __getvae__(self):\n",
    "        return self.vae_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new VAE FEATURES\n",
      "'/home/ubuntu/AYP/code/models/names_epochs_2_vectorSize_64_window_2.model' already exits. Using existing model to re-generate results.\n",
      "'/home/ubuntu/AYP/code/models/co_authors_epochs_2_vectorSize_64_window_2.model' already exits. Using existing model to re-generate results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP/code/py_4/get_all_features.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  count_coauth = []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining new scaler\n",
      "['MESH NAME NOT FOUND: Female', \"MESH NAME NOT FOUND: Practice Patterns, Physicians'\", 'MESH NAME NOT FOUND: Phosphoinositide-3 Kinase Inhibitors', 'MESH NAME NOT FOUND: Chlorocebus aethiops', 'MESH NAME NOT FOUND: Male', 'MESH NAME NOT FOUND: Phospholipid Hydroperoxide Glutathione Peroxidase', 'MESH NAME NOT FOUND: Outcome Assessment, Health Care', 'MESH NAME NOT FOUND: Diet, Healthy', 'MESH NAME NOT FOUND: Copper-Transporting ATPases']\n"
     ]
    }
   ],
   "source": [
    "train_set = ToyDS(df, auth_core)\n",
    "train_loader=DataLoader(dataset= train_set, batch_size = batch_size, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features:  64\n",
      "The number of train data:  2500\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of features: \",train_set.input_dim)\n",
    "print(\"The number of train data: \",train_set.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim = train_set.input_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = nn.Linear(self.input_dim, 64)\n",
    "        self.fc21 = nn.Linear(64, 32)\n",
    "        self.fc22 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 64)\n",
    "        self.fc4 = nn.Linear(64, self.input_dim)\n",
    "        \n",
    "        #Want to initialize logvar weights to 0\n",
    "#         self.fc22.weight.data.fill_(0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    #BCE = F.binary_cross_entropy(recon_x, x.view(-1, 64), reduction='sum')\n",
    "\n",
    "    MSE = F.mse_loss(recon_x, x.view(-1, train_set.input_dim))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]           4,160\n",
      "            Linear-2                   [-1, 32]           2,080\n",
      "            Linear-3                   [-1, 32]           2,080\n",
      "            Linear-4                   [-1, 64]           2,112\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "================================================================\n",
      "Total params: 14,592\n",
      "Trainable params: 14,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (20,train_set.input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(tr_loader, model, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    train_log = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Started training epoch no. {}\".format(epoch+1))\n",
    "        train_loss= 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data.to(device, dtype=torch.float32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar, z = model(data)\n",
    "            loss = criterion(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data)))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}\\n'.format(\n",
    "            epoch, train_loss / len(train_loader.dataset)))\n",
    "        train_log.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss / len(train_loader)})\n",
    "    return train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(epoch):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for i, (data, _) in enumerate(test_loader):\n",
    "#             data = data.to(device)\n",
    "#             recon_batch, mu, logvar = model(data)\n",
    "#             test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "#             if i == 0:\n",
    "#                 n = min(data.size(0), 8)\n",
    "#                 comparison = torch.cat([data[:n],\n",
    "#                                       recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "#                 save_image(comparison.cpu(),\n",
    "#                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training epoch no. 1\n",
      "Train Epoch: 0 [0/2500 (0%)]\tLoss: 1.469735\n",
      "Train Epoch: 0 [80/2500 (3%)]\tLoss: 1.684468\n",
      "Train Epoch: 0 [160/2500 (6%)]\tLoss: 0.479023\n",
      "Train Epoch: 0 [240/2500 (10%)]\tLoss: 0.482190\n",
      "Train Epoch: 0 [320/2500 (13%)]\tLoss: 0.360738\n",
      "Train Epoch: 0 [400/2500 (16%)]\tLoss: 0.416651\n",
      "Train Epoch: 0 [480/2500 (19%)]\tLoss: 0.429609\n",
      "Train Epoch: 0 [560/2500 (22%)]\tLoss: 0.338763\n",
      "Train Epoch: 0 [640/2500 (26%)]\tLoss: 0.232148\n",
      "Train Epoch: 0 [720/2500 (29%)]\tLoss: 0.465773\n",
      "Train Epoch: 0 [800/2500 (32%)]\tLoss: 0.177119\n",
      "Train Epoch: 0 [880/2500 (35%)]\tLoss: 0.163909\n",
      "Train Epoch: 0 [960/2500 (38%)]\tLoss: 0.568398\n",
      "Train Epoch: 0 [1040/2500 (42%)]\tLoss: 0.300856\n",
      "Train Epoch: 0 [1120/2500 (45%)]\tLoss: 0.354745\n",
      "Train Epoch: 0 [1200/2500 (48%)]\tLoss: 0.227633\n",
      "Train Epoch: 0 [1280/2500 (51%)]\tLoss: 0.386452\n",
      "Train Epoch: 0 [1360/2500 (54%)]\tLoss: 0.123944\n",
      "Train Epoch: 0 [1440/2500 (58%)]\tLoss: 0.342129\n",
      "Train Epoch: 0 [1520/2500 (61%)]\tLoss: 0.287151\n",
      "Train Epoch: 0 [1600/2500 (64%)]\tLoss: 0.191077\n",
      "Train Epoch: 0 [1680/2500 (67%)]\tLoss: 0.208516\n",
      "Train Epoch: 0 [1760/2500 (70%)]\tLoss: 0.227531\n",
      "Train Epoch: 0 [1840/2500 (73%)]\tLoss: 0.116473\n",
      "Train Epoch: 0 [1920/2500 (77%)]\tLoss: 0.338020\n",
      "Train Epoch: 0 [2000/2500 (80%)]\tLoss: 0.137101\n",
      "Train Epoch: 0 [2080/2500 (83%)]\tLoss: 0.225486\n",
      "Train Epoch: 0 [2160/2500 (86%)]\tLoss: 0.165930\n",
      "Train Epoch: 0 [2240/2500 (89%)]\tLoss: 0.146685\n",
      "Train Epoch: 0 [2320/2500 (93%)]\tLoss: 0.247144\n",
      "Train Epoch: 0 [2400/2500 (96%)]\tLoss: 0.138079\n",
      "Train Epoch: 0 [2480/2500 (99%)]\tLoss: 0.213477\n",
      "====> Epoch: 0 Average loss: 0.3354\n",
      "\n",
      "Started training epoch no. 2\n",
      "Train Epoch: 1 [0/2500 (0%)]\tLoss: 0.161754\n",
      "Train Epoch: 1 [80/2500 (3%)]\tLoss: 0.139453\n",
      "Train Epoch: 1 [160/2500 (6%)]\tLoss: 0.244768\n",
      "Train Epoch: 1 [240/2500 (10%)]\tLoss: 0.176933\n",
      "Train Epoch: 1 [320/2500 (13%)]\tLoss: 0.146813\n",
      "Train Epoch: 1 [400/2500 (16%)]\tLoss: 0.271611\n",
      "Train Epoch: 1 [480/2500 (19%)]\tLoss: 0.090894\n",
      "Train Epoch: 1 [560/2500 (22%)]\tLoss: 0.095715\n",
      "Train Epoch: 1 [640/2500 (26%)]\tLoss: 0.096399\n",
      "Train Epoch: 1 [720/2500 (29%)]\tLoss: 0.147663\n",
      "Train Epoch: 1 [800/2500 (32%)]\tLoss: 0.091582\n",
      "Train Epoch: 1 [880/2500 (35%)]\tLoss: 0.124159\n",
      "Train Epoch: 1 [960/2500 (38%)]\tLoss: 0.118780\n",
      "Train Epoch: 1 [1040/2500 (42%)]\tLoss: 0.065508\n",
      "Train Epoch: 1 [1120/2500 (45%)]\tLoss: 0.242817\n",
      "Train Epoch: 1 [1200/2500 (48%)]\tLoss: 0.055311\n",
      "Train Epoch: 1 [1280/2500 (51%)]\tLoss: 0.161552\n",
      "Train Epoch: 1 [1360/2500 (54%)]\tLoss: 0.132859\n",
      "Train Epoch: 1 [1440/2500 (58%)]\tLoss: 0.070930\n",
      "Train Epoch: 1 [1520/2500 (61%)]\tLoss: 0.137806\n",
      "Train Epoch: 1 [1600/2500 (64%)]\tLoss: 0.135238\n",
      "Train Epoch: 1 [1680/2500 (67%)]\tLoss: 0.168393\n",
      "Train Epoch: 1 [1760/2500 (70%)]\tLoss: 0.360492\n",
      "Train Epoch: 1 [1840/2500 (73%)]\tLoss: 0.063276\n",
      "Train Epoch: 1 [1920/2500 (77%)]\tLoss: 0.094620\n",
      "Train Epoch: 1 [2000/2500 (80%)]\tLoss: 0.102123\n",
      "Train Epoch: 1 [2080/2500 (83%)]\tLoss: 0.287748\n",
      "Train Epoch: 1 [2160/2500 (86%)]\tLoss: 0.081161\n",
      "Train Epoch: 1 [2240/2500 (89%)]\tLoss: 0.236757\n",
      "Train Epoch: 1 [2320/2500 (93%)]\tLoss: 0.123550\n",
      "Train Epoch: 1 [2400/2500 (96%)]\tLoss: 0.140191\n",
      "Train Epoch: 1 [2480/2500 (99%)]\tLoss: 0.125964\n",
      "====> Epoch: 1 Average loss: 0.1499\n",
      "\n",
      "Started training epoch no. 3\n",
      "Train Epoch: 2 [0/2500 (0%)]\tLoss: 0.145877\n",
      "Train Epoch: 2 [80/2500 (3%)]\tLoss: 0.133089\n",
      "Train Epoch: 2 [160/2500 (6%)]\tLoss: 0.115128\n",
      "Train Epoch: 2 [240/2500 (10%)]\tLoss: 0.227415\n",
      "Train Epoch: 2 [320/2500 (13%)]\tLoss: 0.087880\n",
      "Train Epoch: 2 [400/2500 (16%)]\tLoss: 0.202422\n",
      "Train Epoch: 2 [480/2500 (19%)]\tLoss: 0.096242\n",
      "Train Epoch: 2 [560/2500 (22%)]\tLoss: 0.085285\n",
      "Train Epoch: 2 [640/2500 (26%)]\tLoss: 0.146593\n",
      "Train Epoch: 2 [720/2500 (29%)]\tLoss: 0.139653\n",
      "Train Epoch: 2 [800/2500 (32%)]\tLoss: 0.175431\n",
      "Train Epoch: 2 [880/2500 (35%)]\tLoss: 0.171527\n",
      "Train Epoch: 2 [960/2500 (38%)]\tLoss: 0.093827\n",
      "Train Epoch: 2 [1040/2500 (42%)]\tLoss: 0.147637\n",
      "Train Epoch: 2 [1120/2500 (45%)]\tLoss: 0.158103\n",
      "Train Epoch: 2 [1200/2500 (48%)]\tLoss: 0.070886\n",
      "Train Epoch: 2 [1280/2500 (51%)]\tLoss: 0.075605\n",
      "Train Epoch: 2 [1360/2500 (54%)]\tLoss: 0.184193\n",
      "Train Epoch: 2 [1440/2500 (58%)]\tLoss: 0.184603\n",
      "Train Epoch: 2 [1520/2500 (61%)]\tLoss: 0.173259\n",
      "Train Epoch: 2 [1600/2500 (64%)]\tLoss: 0.101252\n",
      "Train Epoch: 2 [1680/2500 (67%)]\tLoss: 0.092106\n",
      "Train Epoch: 2 [1760/2500 (70%)]\tLoss: 0.089629\n",
      "Train Epoch: 2 [1840/2500 (73%)]\tLoss: 0.084479\n",
      "Train Epoch: 2 [1920/2500 (77%)]\tLoss: 0.073836\n",
      "Train Epoch: 2 [2000/2500 (80%)]\tLoss: 0.241661\n",
      "Train Epoch: 2 [2080/2500 (83%)]\tLoss: 0.098713\n",
      "Train Epoch: 2 [2160/2500 (86%)]\tLoss: 0.160935\n",
      "Train Epoch: 2 [2240/2500 (89%)]\tLoss: 0.134045\n",
      "Train Epoch: 2 [2320/2500 (93%)]\tLoss: 0.083421\n",
      "Train Epoch: 2 [2400/2500 (96%)]\tLoss: 0.075656\n",
      "Train Epoch: 2 [2480/2500 (99%)]\tLoss: 0.101113\n",
      "====> Epoch: 2 Average loss: 0.1337\n",
      "\n",
      "Started training epoch no. 4\n",
      "Train Epoch: 3 [0/2500 (0%)]\tLoss: 0.167053\n",
      "Train Epoch: 3 [80/2500 (3%)]\tLoss: 0.155939\n",
      "Train Epoch: 3 [160/2500 (6%)]\tLoss: 0.097795\n",
      "Train Epoch: 3 [240/2500 (10%)]\tLoss: 0.154872\n",
      "Train Epoch: 3 [320/2500 (13%)]\tLoss: 0.193716\n",
      "Train Epoch: 3 [400/2500 (16%)]\tLoss: 0.174723\n",
      "Train Epoch: 3 [480/2500 (19%)]\tLoss: 0.119041\n",
      "Train Epoch: 3 [560/2500 (22%)]\tLoss: 0.200301\n",
      "Train Epoch: 3 [640/2500 (26%)]\tLoss: 0.193095\n",
      "Train Epoch: 3 [720/2500 (29%)]\tLoss: 0.130453\n",
      "Train Epoch: 3 [800/2500 (32%)]\tLoss: 0.066308\n",
      "Train Epoch: 3 [880/2500 (35%)]\tLoss: 0.104152\n",
      "Train Epoch: 3 [960/2500 (38%)]\tLoss: 0.062858\n",
      "Train Epoch: 3 [1040/2500 (42%)]\tLoss: 0.119342\n",
      "Train Epoch: 3 [1120/2500 (45%)]\tLoss: 0.146537\n",
      "Train Epoch: 3 [1200/2500 (48%)]\tLoss: 0.095918\n",
      "Train Epoch: 3 [1280/2500 (51%)]\tLoss: 0.160498\n",
      "Train Epoch: 3 [1360/2500 (54%)]\tLoss: 0.117993\n",
      "Train Epoch: 3 [1440/2500 (58%)]\tLoss: 0.153737\n",
      "Train Epoch: 3 [1520/2500 (61%)]\tLoss: 0.159632\n",
      "Train Epoch: 3 [1600/2500 (64%)]\tLoss: 0.103066\n",
      "Train Epoch: 3 [1680/2500 (67%)]\tLoss: 0.150577\n",
      "Train Epoch: 3 [1760/2500 (70%)]\tLoss: 0.132903\n",
      "Train Epoch: 3 [1840/2500 (73%)]\tLoss: 0.078105\n",
      "Train Epoch: 3 [1920/2500 (77%)]\tLoss: 0.097911\n",
      "Train Epoch: 3 [2000/2500 (80%)]\tLoss: 0.106472\n",
      "Train Epoch: 3 [2080/2500 (83%)]\tLoss: 0.098972\n",
      "Train Epoch: 3 [2160/2500 (86%)]\tLoss: 0.094339\n",
      "Train Epoch: 3 [2240/2500 (89%)]\tLoss: 0.091757\n",
      "Train Epoch: 3 [2320/2500 (93%)]\tLoss: 0.188578\n",
      "Train Epoch: 3 [2400/2500 (96%)]\tLoss: 0.185841\n",
      "Train Epoch: 3 [2480/2500 (99%)]\tLoss: 0.064036\n",
      "====> Epoch: 3 Average loss: 0.1295\n",
      "\n",
      "Started training epoch no. 5\n",
      "Train Epoch: 4 [0/2500 (0%)]\tLoss: 0.247021\n",
      "Train Epoch: 4 [80/2500 (3%)]\tLoss: 0.160589\n",
      "Train Epoch: 4 [160/2500 (6%)]\tLoss: 0.093775\n",
      "Train Epoch: 4 [240/2500 (10%)]\tLoss: 0.102474\n",
      "Train Epoch: 4 [320/2500 (13%)]\tLoss: 0.114020\n",
      "Train Epoch: 4 [400/2500 (16%)]\tLoss: 0.118848\n",
      "Train Epoch: 4 [480/2500 (19%)]\tLoss: 0.072838\n",
      "Train Epoch: 4 [560/2500 (22%)]\tLoss: 0.074257\n",
      "Train Epoch: 4 [640/2500 (26%)]\tLoss: 0.113186\n",
      "Train Epoch: 4 [720/2500 (29%)]\tLoss: 0.114779\n",
      "Train Epoch: 4 [800/2500 (32%)]\tLoss: 0.124737\n",
      "Train Epoch: 4 [880/2500 (35%)]\tLoss: 0.180658\n",
      "Train Epoch: 4 [960/2500 (38%)]\tLoss: 0.163859\n",
      "Train Epoch: 4 [1040/2500 (42%)]\tLoss: 0.145372\n",
      "Train Epoch: 4 [1120/2500 (45%)]\tLoss: 0.189752\n",
      "Train Epoch: 4 [1200/2500 (48%)]\tLoss: 0.116951\n",
      "Train Epoch: 4 [1280/2500 (51%)]\tLoss: 0.139316\n",
      "Train Epoch: 4 [1360/2500 (54%)]\tLoss: 0.089490\n",
      "Train Epoch: 4 [1440/2500 (58%)]\tLoss: 0.204474\n",
      "Train Epoch: 4 [1520/2500 (61%)]\tLoss: 0.098264\n",
      "Train Epoch: 4 [1600/2500 (64%)]\tLoss: 0.111310\n",
      "Train Epoch: 4 [1680/2500 (67%)]\tLoss: 0.090360\n",
      "Train Epoch: 4 [1760/2500 (70%)]\tLoss: 0.076733\n",
      "Train Epoch: 4 [1840/2500 (73%)]\tLoss: 0.123062\n",
      "Train Epoch: 4 [1920/2500 (77%)]\tLoss: 0.086612\n",
      "Train Epoch: 4 [2000/2500 (80%)]\tLoss: 0.103742\n",
      "Train Epoch: 4 [2080/2500 (83%)]\tLoss: 0.060897\n",
      "Train Epoch: 4 [2160/2500 (86%)]\tLoss: 0.145194\n",
      "Train Epoch: 4 [2240/2500 (89%)]\tLoss: 0.137176\n",
      "Train Epoch: 4 [2320/2500 (93%)]\tLoss: 0.087990\n",
      "Train Epoch: 4 [2400/2500 (96%)]\tLoss: 0.082030\n",
      "Train Epoch: 4 [2480/2500 (99%)]\tLoss: 0.173408\n",
      "====> Epoch: 4 Average loss: 0.1276\n",
      "\n",
      "Started training epoch no. 6\n",
      "Train Epoch: 5 [0/2500 (0%)]\tLoss: 0.144832\n",
      "Train Epoch: 5 [80/2500 (3%)]\tLoss: 0.085901\n",
      "Train Epoch: 5 [160/2500 (6%)]\tLoss: 0.117566\n",
      "Train Epoch: 5 [240/2500 (10%)]\tLoss: 0.089250\n",
      "Train Epoch: 5 [320/2500 (13%)]\tLoss: 0.228683\n",
      "Train Epoch: 5 [400/2500 (16%)]\tLoss: 0.133507\n",
      "Train Epoch: 5 [480/2500 (19%)]\tLoss: 0.222785\n",
      "Train Epoch: 5 [560/2500 (22%)]\tLoss: 0.130751\n",
      "Train Epoch: 5 [640/2500 (26%)]\tLoss: 0.101927\n",
      "Train Epoch: 5 [720/2500 (29%)]\tLoss: 0.187754\n",
      "Train Epoch: 5 [800/2500 (32%)]\tLoss: 0.085535\n",
      "Train Epoch: 5 [880/2500 (35%)]\tLoss: 0.161360\n",
      "Train Epoch: 5 [960/2500 (38%)]\tLoss: 0.104501\n",
      "Train Epoch: 5 [1040/2500 (42%)]\tLoss: 0.121307\n",
      "Train Epoch: 5 [1120/2500 (45%)]\tLoss: 0.119123\n",
      "Train Epoch: 5 [1200/2500 (48%)]\tLoss: 0.109941\n",
      "Train Epoch: 5 [1280/2500 (51%)]\tLoss: 0.162636\n",
      "Train Epoch: 5 [1360/2500 (54%)]\tLoss: 0.217484\n",
      "Train Epoch: 5 [1440/2500 (58%)]\tLoss: 0.130415\n",
      "Train Epoch: 5 [1520/2500 (61%)]\tLoss: 0.127501\n",
      "Train Epoch: 5 [1600/2500 (64%)]\tLoss: 0.299201\n",
      "Train Epoch: 5 [1680/2500 (67%)]\tLoss: 0.116318\n",
      "Train Epoch: 5 [1760/2500 (70%)]\tLoss: 0.179272\n",
      "Train Epoch: 5 [1840/2500 (73%)]\tLoss: 0.104452\n",
      "Train Epoch: 5 [1920/2500 (77%)]\tLoss: 0.133282\n",
      "Train Epoch: 5 [2000/2500 (80%)]\tLoss: 0.118088\n",
      "Train Epoch: 5 [2080/2500 (83%)]\tLoss: 0.169767\n",
      "Train Epoch: 5 [2160/2500 (86%)]\tLoss: 0.120944\n",
      "Train Epoch: 5 [2240/2500 (89%)]\tLoss: 0.067616\n",
      "Train Epoch: 5 [2320/2500 (93%)]\tLoss: 0.115528\n",
      "Train Epoch: 5 [2400/2500 (96%)]\tLoss: 0.099094\n",
      "Train Epoch: 5 [2480/2500 (99%)]\tLoss: 0.119217\n",
      "====> Epoch: 5 Average loss: 0.1267\n",
      "\n",
      "Started training epoch no. 7\n",
      "Train Epoch: 6 [0/2500 (0%)]\tLoss: 0.075266\n",
      "Train Epoch: 6 [80/2500 (3%)]\tLoss: 0.095932\n",
      "Train Epoch: 6 [160/2500 (6%)]\tLoss: 0.110015\n",
      "Train Epoch: 6 [240/2500 (10%)]\tLoss: 0.059898\n",
      "Train Epoch: 6 [320/2500 (13%)]\tLoss: 0.158972\n",
      "Train Epoch: 6 [400/2500 (16%)]\tLoss: 0.071069\n",
      "Train Epoch: 6 [480/2500 (19%)]\tLoss: 0.166804\n",
      "Train Epoch: 6 [560/2500 (22%)]\tLoss: 0.112303\n",
      "Train Epoch: 6 [640/2500 (26%)]\tLoss: 0.168611\n",
      "Train Epoch: 6 [720/2500 (29%)]\tLoss: 0.172890\n",
      "Train Epoch: 6 [800/2500 (32%)]\tLoss: 0.132585\n",
      "Train Epoch: 6 [880/2500 (35%)]\tLoss: 0.122825\n",
      "Train Epoch: 6 [960/2500 (38%)]\tLoss: 0.100856\n",
      "Train Epoch: 6 [1040/2500 (42%)]\tLoss: 0.140049\n",
      "Train Epoch: 6 [1120/2500 (45%)]\tLoss: 0.111884\n",
      "Train Epoch: 6 [1200/2500 (48%)]\tLoss: 0.121998\n",
      "Train Epoch: 6 [1280/2500 (51%)]\tLoss: 0.111910\n",
      "Train Epoch: 6 [1360/2500 (54%)]\tLoss: 0.077524\n",
      "Train Epoch: 6 [1440/2500 (58%)]\tLoss: 0.161312\n",
      "Train Epoch: 6 [1520/2500 (61%)]\tLoss: 0.133498\n",
      "Train Epoch: 6 [1600/2500 (64%)]\tLoss: 0.063032\n",
      "Train Epoch: 6 [1680/2500 (67%)]\tLoss: 0.098019\n",
      "Train Epoch: 6 [1760/2500 (70%)]\tLoss: 0.162821\n",
      "Train Epoch: 6 [1840/2500 (73%)]\tLoss: 0.096358\n",
      "Train Epoch: 6 [1920/2500 (77%)]\tLoss: 0.142832\n",
      "Train Epoch: 6 [2000/2500 (80%)]\tLoss: 0.057476\n",
      "Train Epoch: 6 [2080/2500 (83%)]\tLoss: 0.138250\n",
      "Train Epoch: 6 [2160/2500 (86%)]\tLoss: 0.086539\n",
      "Train Epoch: 6 [2240/2500 (89%)]\tLoss: 0.092077\n",
      "Train Epoch: 6 [2320/2500 (93%)]\tLoss: 0.156744\n",
      "Train Epoch: 6 [2400/2500 (96%)]\tLoss: 0.095323\n",
      "Train Epoch: 6 [2480/2500 (99%)]\tLoss: 0.111335\n",
      "====> Epoch: 6 Average loss: 0.1261\n",
      "\n",
      "Started training epoch no. 8\n",
      "Train Epoch: 7 [0/2500 (0%)]\tLoss: 0.121397\n",
      "Train Epoch: 7 [80/2500 (3%)]\tLoss: 0.211725\n",
      "Train Epoch: 7 [160/2500 (6%)]\tLoss: 0.175053\n",
      "Train Epoch: 7 [240/2500 (10%)]\tLoss: 0.198307\n",
      "Train Epoch: 7 [320/2500 (13%)]\tLoss: 0.113765\n",
      "Train Epoch: 7 [400/2500 (16%)]\tLoss: 0.122894\n",
      "Train Epoch: 7 [480/2500 (19%)]\tLoss: 0.146430\n",
      "Train Epoch: 7 [560/2500 (22%)]\tLoss: 0.171445\n",
      "Train Epoch: 7 [640/2500 (26%)]\tLoss: 0.094356\n",
      "Train Epoch: 7 [720/2500 (29%)]\tLoss: 0.155429\n",
      "Train Epoch: 7 [800/2500 (32%)]\tLoss: 0.061676\n",
      "Train Epoch: 7 [880/2500 (35%)]\tLoss: 0.115220\n",
      "Train Epoch: 7 [960/2500 (38%)]\tLoss: 0.206893\n",
      "Train Epoch: 7 [1040/2500 (42%)]\tLoss: 0.167446\n",
      "Train Epoch: 7 [1120/2500 (45%)]\tLoss: 0.144252\n",
      "Train Epoch: 7 [1200/2500 (48%)]\tLoss: 0.091006\n",
      "Train Epoch: 7 [1280/2500 (51%)]\tLoss: 0.160068\n",
      "Train Epoch: 7 [1360/2500 (54%)]\tLoss: 0.242183\n",
      "Train Epoch: 7 [1440/2500 (58%)]\tLoss: 0.112777\n",
      "Train Epoch: 7 [1520/2500 (61%)]\tLoss: 0.113793\n",
      "Train Epoch: 7 [1600/2500 (64%)]\tLoss: 0.154781\n",
      "Train Epoch: 7 [1680/2500 (67%)]\tLoss: 0.117952\n",
      "Train Epoch: 7 [1760/2500 (70%)]\tLoss: 0.088333\n",
      "Train Epoch: 7 [1840/2500 (73%)]\tLoss: 0.097833\n",
      "Train Epoch: 7 [1920/2500 (77%)]\tLoss: 0.091337\n",
      "Train Epoch: 7 [2000/2500 (80%)]\tLoss: 0.173243\n",
      "Train Epoch: 7 [2080/2500 (83%)]\tLoss: 0.128248\n",
      "Train Epoch: 7 [2160/2500 (86%)]\tLoss: 0.086002\n",
      "Train Epoch: 7 [2240/2500 (89%)]\tLoss: 0.094650\n",
      "Train Epoch: 7 [2320/2500 (93%)]\tLoss: 0.053204\n",
      "Train Epoch: 7 [2400/2500 (96%)]\tLoss: 0.042444\n",
      "Train Epoch: 7 [2480/2500 (99%)]\tLoss: 0.137246\n",
      "====> Epoch: 7 Average loss: 0.1258\n",
      "\n",
      "Started training epoch no. 9\n",
      "Train Epoch: 8 [0/2500 (0%)]\tLoss: 0.098379\n",
      "Train Epoch: 8 [80/2500 (3%)]\tLoss: 0.078725\n",
      "Train Epoch: 8 [160/2500 (6%)]\tLoss: 0.096376\n",
      "Train Epoch: 8 [240/2500 (10%)]\tLoss: 0.089475\n",
      "Train Epoch: 8 [320/2500 (13%)]\tLoss: 0.098318\n",
      "Train Epoch: 8 [400/2500 (16%)]\tLoss: 0.117604\n",
      "Train Epoch: 8 [480/2500 (19%)]\tLoss: 0.066152\n",
      "Train Epoch: 8 [560/2500 (22%)]\tLoss: 0.172764\n",
      "Train Epoch: 8 [640/2500 (26%)]\tLoss: 0.094554\n",
      "Train Epoch: 8 [720/2500 (29%)]\tLoss: 0.089576\n",
      "Train Epoch: 8 [800/2500 (32%)]\tLoss: 0.140109\n",
      "Train Epoch: 8 [880/2500 (35%)]\tLoss: 0.073963\n",
      "Train Epoch: 8 [960/2500 (38%)]\tLoss: 0.081171\n",
      "Train Epoch: 8 [1040/2500 (42%)]\tLoss: 0.105605\n",
      "Train Epoch: 8 [1120/2500 (45%)]\tLoss: 0.127046\n",
      "Train Epoch: 8 [1200/2500 (48%)]\tLoss: 0.073171\n",
      "Train Epoch: 8 [1280/2500 (51%)]\tLoss: 0.210642\n",
      "Train Epoch: 8 [1360/2500 (54%)]\tLoss: 0.125182\n",
      "Train Epoch: 8 [1440/2500 (58%)]\tLoss: 0.141554\n",
      "Train Epoch: 8 [1520/2500 (61%)]\tLoss: 0.159585\n",
      "Train Epoch: 8 [1600/2500 (64%)]\tLoss: 0.081240\n",
      "Train Epoch: 8 [1680/2500 (67%)]\tLoss: 0.058270\n",
      "Train Epoch: 8 [1760/2500 (70%)]\tLoss: 0.138932\n",
      "Train Epoch: 8 [1840/2500 (73%)]\tLoss: 0.191078\n",
      "Train Epoch: 8 [1920/2500 (77%)]\tLoss: 0.184654\n",
      "Train Epoch: 8 [2000/2500 (80%)]\tLoss: 0.138824\n",
      "Train Epoch: 8 [2080/2500 (83%)]\tLoss: 0.133862\n",
      "Train Epoch: 8 [2160/2500 (86%)]\tLoss: 0.106345\n",
      "Train Epoch: 8 [2240/2500 (89%)]\tLoss: 0.067305\n",
      "Train Epoch: 8 [2320/2500 (93%)]\tLoss: 0.164953\n",
      "Train Epoch: 8 [2400/2500 (96%)]\tLoss: 0.181444\n",
      "Train Epoch: 8 [2480/2500 (99%)]\tLoss: 0.102267\n",
      "====> Epoch: 8 Average loss: 0.1257\n",
      "\n",
      "Started training epoch no. 10\n",
      "Train Epoch: 9 [0/2500 (0%)]\tLoss: 0.133332\n",
      "Train Epoch: 9 [80/2500 (3%)]\tLoss: 0.073168\n",
      "Train Epoch: 9 [160/2500 (6%)]\tLoss: 0.090922\n",
      "Train Epoch: 9 [240/2500 (10%)]\tLoss: 0.120616\n",
      "Train Epoch: 9 [320/2500 (13%)]\tLoss: 0.068067\n",
      "Train Epoch: 9 [400/2500 (16%)]\tLoss: 0.104031\n",
      "Train Epoch: 9 [480/2500 (19%)]\tLoss: 0.098028\n",
      "Train Epoch: 9 [560/2500 (22%)]\tLoss: 0.100166\n",
      "Train Epoch: 9 [640/2500 (26%)]\tLoss: 0.080748\n",
      "Train Epoch: 9 [720/2500 (29%)]\tLoss: 0.266767\n",
      "Train Epoch: 9 [800/2500 (32%)]\tLoss: 0.123438\n",
      "Train Epoch: 9 [880/2500 (35%)]\tLoss: 0.100321\n",
      "Train Epoch: 9 [960/2500 (38%)]\tLoss: 0.069884\n",
      "Train Epoch: 9 [1040/2500 (42%)]\tLoss: 0.070469\n",
      "Train Epoch: 9 [1120/2500 (45%)]\tLoss: 0.162879\n",
      "Train Epoch: 9 [1200/2500 (48%)]\tLoss: 0.126741\n",
      "Train Epoch: 9 [1280/2500 (51%)]\tLoss: 0.271531\n",
      "Train Epoch: 9 [1360/2500 (54%)]\tLoss: 0.141315\n",
      "Train Epoch: 9 [1440/2500 (58%)]\tLoss: 0.133423\n",
      "Train Epoch: 9 [1520/2500 (61%)]\tLoss: 0.123434\n",
      "Train Epoch: 9 [1600/2500 (64%)]\tLoss: 0.101323\n",
      "Train Epoch: 9 [1680/2500 (67%)]\tLoss: 0.216673\n",
      "Train Epoch: 9 [1760/2500 (70%)]\tLoss: 0.077056\n",
      "Train Epoch: 9 [1840/2500 (73%)]\tLoss: 0.110523\n",
      "Train Epoch: 9 [1920/2500 (77%)]\tLoss: 0.116118\n",
      "Train Epoch: 9 [2000/2500 (80%)]\tLoss: 0.109842\n",
      "Train Epoch: 9 [2080/2500 (83%)]\tLoss: 0.125228\n",
      "Train Epoch: 9 [2160/2500 (86%)]\tLoss: 0.151791\n",
      "Train Epoch: 9 [2240/2500 (89%)]\tLoss: 0.111582\n",
      "Train Epoch: 9 [2320/2500 (93%)]\tLoss: 0.135185\n",
      "Train Epoch: 9 [2400/2500 (96%)]\tLoss: 0.086235\n",
      "Train Epoch: 9 [2480/2500 (99%)]\tLoss: 0.136116\n",
      "====> Epoch: 9 Average loss: 0.1257\n",
      "\n",
      "Started training epoch no. 11\n",
      "Train Epoch: 10 [0/2500 (0%)]\tLoss: 0.104445\n",
      "Train Epoch: 10 [80/2500 (3%)]\tLoss: 0.185915\n",
      "Train Epoch: 10 [160/2500 (6%)]\tLoss: 0.072436\n",
      "Train Epoch: 10 [240/2500 (10%)]\tLoss: 0.055891\n",
      "Train Epoch: 10 [320/2500 (13%)]\tLoss: 0.147192\n",
      "Train Epoch: 10 [400/2500 (16%)]\tLoss: 0.195798\n",
      "Train Epoch: 10 [480/2500 (19%)]\tLoss: 0.115393\n",
      "Train Epoch: 10 [560/2500 (22%)]\tLoss: 0.058791\n",
      "Train Epoch: 10 [640/2500 (26%)]\tLoss: 0.070800\n",
      "Train Epoch: 10 [720/2500 (29%)]\tLoss: 0.122196\n",
      "Train Epoch: 10 [800/2500 (32%)]\tLoss: 0.127715\n",
      "Train Epoch: 10 [880/2500 (35%)]\tLoss: 0.169031\n",
      "Train Epoch: 10 [960/2500 (38%)]\tLoss: 0.124439\n",
      "Train Epoch: 10 [1040/2500 (42%)]\tLoss: 0.131611\n",
      "Train Epoch: 10 [1120/2500 (45%)]\tLoss: 0.167930\n",
      "Train Epoch: 10 [1200/2500 (48%)]\tLoss: 0.122733\n",
      "Train Epoch: 10 [1280/2500 (51%)]\tLoss: 0.112220\n",
      "Train Epoch: 10 [1360/2500 (54%)]\tLoss: 0.167681\n",
      "Train Epoch: 10 [1440/2500 (58%)]\tLoss: 0.164190\n",
      "Train Epoch: 10 [1520/2500 (61%)]\tLoss: 0.188041\n",
      "Train Epoch: 10 [1600/2500 (64%)]\tLoss: 0.094066\n",
      "Train Epoch: 10 [1680/2500 (67%)]\tLoss: 0.064011\n",
      "Train Epoch: 10 [1760/2500 (70%)]\tLoss: 0.113558\n",
      "Train Epoch: 10 [1840/2500 (73%)]\tLoss: 0.139391\n",
      "Train Epoch: 10 [1920/2500 (77%)]\tLoss: 0.124637\n",
      "Train Epoch: 10 [2000/2500 (80%)]\tLoss: 0.061276\n",
      "Train Epoch: 10 [2080/2500 (83%)]\tLoss: 0.062743\n",
      "Train Epoch: 10 [2160/2500 (86%)]\tLoss: 0.117574\n",
      "Train Epoch: 10 [2240/2500 (89%)]\tLoss: 0.084772\n",
      "Train Epoch: 10 [2320/2500 (93%)]\tLoss: 0.076380\n",
      "Train Epoch: 10 [2400/2500 (96%)]\tLoss: 0.101823\n",
      "Train Epoch: 10 [2480/2500 (99%)]\tLoss: 0.063080\n",
      "====> Epoch: 10 Average loss: 0.1254\n",
      "\n",
      "Started training epoch no. 12\n",
      "Train Epoch: 11 [0/2500 (0%)]\tLoss: 0.101989\n",
      "Train Epoch: 11 [80/2500 (3%)]\tLoss: 0.136476\n",
      "Train Epoch: 11 [160/2500 (6%)]\tLoss: 0.178370\n",
      "Train Epoch: 11 [240/2500 (10%)]\tLoss: 0.103703\n",
      "Train Epoch: 11 [320/2500 (13%)]\tLoss: 0.110293\n",
      "Train Epoch: 11 [400/2500 (16%)]\tLoss: 0.089902\n",
      "Train Epoch: 11 [480/2500 (19%)]\tLoss: 0.110403\n",
      "Train Epoch: 11 [560/2500 (22%)]\tLoss: 0.100287\n",
      "Train Epoch: 11 [640/2500 (26%)]\tLoss: 0.087746\n",
      "Train Epoch: 11 [720/2500 (29%)]\tLoss: 0.111690\n",
      "Train Epoch: 11 [800/2500 (32%)]\tLoss: 0.239560\n",
      "Train Epoch: 11 [880/2500 (35%)]\tLoss: 0.133257\n",
      "Train Epoch: 11 [960/2500 (38%)]\tLoss: 0.143231\n",
      "Train Epoch: 11 [1040/2500 (42%)]\tLoss: 0.142521\n",
      "Train Epoch: 11 [1120/2500 (45%)]\tLoss: 0.058315\n",
      "Train Epoch: 11 [1200/2500 (48%)]\tLoss: 0.122771\n",
      "Train Epoch: 11 [1280/2500 (51%)]\tLoss: 0.130052\n",
      "Train Epoch: 11 [1360/2500 (54%)]\tLoss: 0.097386\n",
      "Train Epoch: 11 [1440/2500 (58%)]\tLoss: 0.047578\n",
      "Train Epoch: 11 [1520/2500 (61%)]\tLoss: 0.115171\n",
      "Train Epoch: 11 [1600/2500 (64%)]\tLoss: 0.107477\n",
      "Train Epoch: 11 [1680/2500 (67%)]\tLoss: 0.038573\n",
      "Train Epoch: 11 [1760/2500 (70%)]\tLoss: 0.057899\n",
      "Train Epoch: 11 [1840/2500 (73%)]\tLoss: 0.153703\n",
      "Train Epoch: 11 [1920/2500 (77%)]\tLoss: 0.159985\n",
      "Train Epoch: 11 [2000/2500 (80%)]\tLoss: 0.210791\n",
      "Train Epoch: 11 [2080/2500 (83%)]\tLoss: 0.088794\n",
      "Train Epoch: 11 [2160/2500 (86%)]\tLoss: 0.107753\n",
      "Train Epoch: 11 [2240/2500 (89%)]\tLoss: 0.151413\n",
      "Train Epoch: 11 [2320/2500 (93%)]\tLoss: 0.090867\n",
      "Train Epoch: 11 [2400/2500 (96%)]\tLoss: 0.109748\n",
      "Train Epoch: 11 [2480/2500 (99%)]\tLoss: 0.143913\n",
      "====> Epoch: 11 Average loss: 0.1254\n",
      "\n",
      "Started training epoch no. 13\n",
      "Train Epoch: 12 [0/2500 (0%)]\tLoss: 0.210026\n",
      "Train Epoch: 12 [80/2500 (3%)]\tLoss: 0.072390\n",
      "Train Epoch: 12 [160/2500 (6%)]\tLoss: 0.124404\n",
      "Train Epoch: 12 [240/2500 (10%)]\tLoss: 0.074240\n",
      "Train Epoch: 12 [320/2500 (13%)]\tLoss: 0.077523\n",
      "Train Epoch: 12 [400/2500 (16%)]\tLoss: 0.278368\n",
      "Train Epoch: 12 [480/2500 (19%)]\tLoss: 0.136948\n",
      "Train Epoch: 12 [560/2500 (22%)]\tLoss: 0.218841\n",
      "Train Epoch: 12 [640/2500 (26%)]\tLoss: 0.136106\n",
      "Train Epoch: 12 [720/2500 (29%)]\tLoss: 0.106626\n",
      "Train Epoch: 12 [800/2500 (32%)]\tLoss: 0.168688\n",
      "Train Epoch: 12 [880/2500 (35%)]\tLoss: 0.240752\n",
      "Train Epoch: 12 [960/2500 (38%)]\tLoss: 0.106879\n",
      "Train Epoch: 12 [1040/2500 (42%)]\tLoss: 0.127803\n",
      "Train Epoch: 12 [1120/2500 (45%)]\tLoss: 0.095648\n",
      "Train Epoch: 12 [1200/2500 (48%)]\tLoss: 0.129998\n",
      "Train Epoch: 12 [1280/2500 (51%)]\tLoss: 0.113575\n",
      "Train Epoch: 12 [1360/2500 (54%)]\tLoss: 0.121040\n",
      "Train Epoch: 12 [1440/2500 (58%)]\tLoss: 0.109371\n",
      "Train Epoch: 12 [1520/2500 (61%)]\tLoss: 0.096274\n",
      "Train Epoch: 12 [1600/2500 (64%)]\tLoss: 0.116464\n",
      "Train Epoch: 12 [1680/2500 (67%)]\tLoss: 0.103573\n",
      "Train Epoch: 12 [1760/2500 (70%)]\tLoss: 0.103013\n",
      "Train Epoch: 12 [1840/2500 (73%)]\tLoss: 0.083479\n",
      "Train Epoch: 12 [1920/2500 (77%)]\tLoss: 0.113779\n",
      "Train Epoch: 12 [2000/2500 (80%)]\tLoss: 0.067936\n",
      "Train Epoch: 12 [2080/2500 (83%)]\tLoss: 0.124312\n",
      "Train Epoch: 12 [2160/2500 (86%)]\tLoss: 0.138215\n",
      "Train Epoch: 12 [2240/2500 (89%)]\tLoss: 0.057510\n",
      "Train Epoch: 12 [2320/2500 (93%)]\tLoss: 0.124374\n",
      "Train Epoch: 12 [2400/2500 (96%)]\tLoss: 0.137300\n",
      "Train Epoch: 12 [2480/2500 (99%)]\tLoss: 0.110835\n",
      "====> Epoch: 12 Average loss: 0.1255\n",
      "\n",
      "Started training epoch no. 14\n",
      "Train Epoch: 13 [0/2500 (0%)]\tLoss: 0.200039\n",
      "Train Epoch: 13 [80/2500 (3%)]\tLoss: 0.172021\n",
      "Train Epoch: 13 [160/2500 (6%)]\tLoss: 0.100297\n",
      "Train Epoch: 13 [240/2500 (10%)]\tLoss: 0.162906\n",
      "Train Epoch: 13 [320/2500 (13%)]\tLoss: 0.080531\n",
      "Train Epoch: 13 [400/2500 (16%)]\tLoss: 0.102361\n",
      "Train Epoch: 13 [480/2500 (19%)]\tLoss: 0.315252\n",
      "Train Epoch: 13 [560/2500 (22%)]\tLoss: 0.071845\n",
      "Train Epoch: 13 [640/2500 (26%)]\tLoss: 0.080500\n",
      "Train Epoch: 13 [720/2500 (29%)]\tLoss: 0.074532\n",
      "Train Epoch: 13 [800/2500 (32%)]\tLoss: 0.104294\n",
      "Train Epoch: 13 [880/2500 (35%)]\tLoss: 0.113823\n",
      "Train Epoch: 13 [960/2500 (38%)]\tLoss: 0.119519\n",
      "Train Epoch: 13 [1040/2500 (42%)]\tLoss: 0.069364\n",
      "Train Epoch: 13 [1120/2500 (45%)]\tLoss: 0.090166\n",
      "Train Epoch: 13 [1200/2500 (48%)]\tLoss: 0.211770\n",
      "Train Epoch: 13 [1280/2500 (51%)]\tLoss: 0.132102\n",
      "Train Epoch: 13 [1360/2500 (54%)]\tLoss: 0.106856\n",
      "Train Epoch: 13 [1440/2500 (58%)]\tLoss: 0.143905\n",
      "Train Epoch: 13 [1520/2500 (61%)]\tLoss: 0.145389\n",
      "Train Epoch: 13 [1600/2500 (64%)]\tLoss: 0.111646\n",
      "Train Epoch: 13 [1680/2500 (67%)]\tLoss: 0.104365\n",
      "Train Epoch: 13 [1760/2500 (70%)]\tLoss: 0.124104\n",
      "Train Epoch: 13 [1840/2500 (73%)]\tLoss: 0.126524\n",
      "Train Epoch: 13 [1920/2500 (77%)]\tLoss: 0.087466\n",
      "Train Epoch: 13 [2000/2500 (80%)]\tLoss: 0.163341\n",
      "Train Epoch: 13 [2080/2500 (83%)]\tLoss: 0.080676\n",
      "Train Epoch: 13 [2160/2500 (86%)]\tLoss: 0.124670\n",
      "Train Epoch: 13 [2240/2500 (89%)]\tLoss: 0.087373\n",
      "Train Epoch: 13 [2320/2500 (93%)]\tLoss: 0.219884\n",
      "Train Epoch: 13 [2400/2500 (96%)]\tLoss: 0.143705\n",
      "Train Epoch: 13 [2480/2500 (99%)]\tLoss: 0.164527\n",
      "====> Epoch: 13 Average loss: 0.1252\n",
      "\n",
      "Started training epoch no. 15\n",
      "Train Epoch: 14 [0/2500 (0%)]\tLoss: 0.105848\n",
      "Train Epoch: 14 [80/2500 (3%)]\tLoss: 0.116040\n",
      "Train Epoch: 14 [160/2500 (6%)]\tLoss: 0.245654\n",
      "Train Epoch: 14 [240/2500 (10%)]\tLoss: 0.076749\n",
      "Train Epoch: 14 [320/2500 (13%)]\tLoss: 0.105651\n",
      "Train Epoch: 14 [400/2500 (16%)]\tLoss: 0.127811\n",
      "Train Epoch: 14 [480/2500 (19%)]\tLoss: 0.122309\n",
      "Train Epoch: 14 [560/2500 (22%)]\tLoss: 0.172776\n",
      "Train Epoch: 14 [640/2500 (26%)]\tLoss: 0.085963\n",
      "Train Epoch: 14 [720/2500 (29%)]\tLoss: 0.118085\n",
      "Train Epoch: 14 [800/2500 (32%)]\tLoss: 0.161004\n",
      "Train Epoch: 14 [880/2500 (35%)]\tLoss: 0.192096\n",
      "Train Epoch: 14 [960/2500 (38%)]\tLoss: 0.066927\n",
      "Train Epoch: 14 [1040/2500 (42%)]\tLoss: 0.118561\n",
      "Train Epoch: 14 [1120/2500 (45%)]\tLoss: 0.075949\n",
      "Train Epoch: 14 [1200/2500 (48%)]\tLoss: 0.052829\n",
      "Train Epoch: 14 [1280/2500 (51%)]\tLoss: 0.131292\n",
      "Train Epoch: 14 [1360/2500 (54%)]\tLoss: 0.148541\n",
      "Train Epoch: 14 [1440/2500 (58%)]\tLoss: 0.128850\n",
      "Train Epoch: 14 [1520/2500 (61%)]\tLoss: 0.125053\n",
      "Train Epoch: 14 [1600/2500 (64%)]\tLoss: 0.087886\n",
      "Train Epoch: 14 [1680/2500 (67%)]\tLoss: 0.047612\n",
      "Train Epoch: 14 [1760/2500 (70%)]\tLoss: 0.138838\n",
      "Train Epoch: 14 [1840/2500 (73%)]\tLoss: 0.140485\n",
      "Train Epoch: 14 [1920/2500 (77%)]\tLoss: 0.093999\n",
      "Train Epoch: 14 [2000/2500 (80%)]\tLoss: 0.106383\n",
      "Train Epoch: 14 [2080/2500 (83%)]\tLoss: 0.158227\n",
      "Train Epoch: 14 [2160/2500 (86%)]\tLoss: 0.099100\n",
      "Train Epoch: 14 [2240/2500 (89%)]\tLoss: 0.067393\n",
      "Train Epoch: 14 [2320/2500 (93%)]\tLoss: 0.137505\n",
      "Train Epoch: 14 [2400/2500 (96%)]\tLoss: 0.057822\n",
      "Train Epoch: 14 [2480/2500 (99%)]\tLoss: 0.058267\n",
      "====> Epoch: 14 Average loss: 0.1252\n",
      "\n",
      "Started training epoch no. 16\n",
      "Train Epoch: 15 [0/2500 (0%)]\tLoss: 0.071489\n",
      "Train Epoch: 15 [80/2500 (3%)]\tLoss: 0.145830\n",
      "Train Epoch: 15 [160/2500 (6%)]\tLoss: 0.074856\n",
      "Train Epoch: 15 [240/2500 (10%)]\tLoss: 0.093670\n",
      "Train Epoch: 15 [320/2500 (13%)]\tLoss: 0.090349\n",
      "Train Epoch: 15 [400/2500 (16%)]\tLoss: 0.093993\n",
      "Train Epoch: 15 [480/2500 (19%)]\tLoss: 0.052512\n",
      "Train Epoch: 15 [560/2500 (22%)]\tLoss: 0.127592\n",
      "Train Epoch: 15 [640/2500 (26%)]\tLoss: 0.118570\n",
      "Train Epoch: 15 [720/2500 (29%)]\tLoss: 0.195190\n",
      "Train Epoch: 15 [800/2500 (32%)]\tLoss: 0.193597\n",
      "Train Epoch: 15 [880/2500 (35%)]\tLoss: 0.112333\n",
      "Train Epoch: 15 [960/2500 (38%)]\tLoss: 0.161160\n",
      "Train Epoch: 15 [1040/2500 (42%)]\tLoss: 0.104696\n",
      "Train Epoch: 15 [1120/2500 (45%)]\tLoss: 0.039872\n",
      "Train Epoch: 15 [1200/2500 (48%)]\tLoss: 0.134522\n",
      "Train Epoch: 15 [1280/2500 (51%)]\tLoss: 0.071672\n",
      "Train Epoch: 15 [1360/2500 (54%)]\tLoss: 0.101521\n",
      "Train Epoch: 15 [1440/2500 (58%)]\tLoss: 0.146054\n",
      "Train Epoch: 15 [1520/2500 (61%)]\tLoss: 0.133859\n",
      "Train Epoch: 15 [1600/2500 (64%)]\tLoss: 0.086523\n",
      "Train Epoch: 15 [1680/2500 (67%)]\tLoss: 0.062769\n",
      "Train Epoch: 15 [1760/2500 (70%)]\tLoss: 0.112335\n",
      "Train Epoch: 15 [1840/2500 (73%)]\tLoss: 0.173403\n",
      "Train Epoch: 15 [1920/2500 (77%)]\tLoss: 0.153123\n",
      "Train Epoch: 15 [2000/2500 (80%)]\tLoss: 0.139581\n",
      "Train Epoch: 15 [2080/2500 (83%)]\tLoss: 0.128947\n",
      "Train Epoch: 15 [2160/2500 (86%)]\tLoss: 0.128037\n",
      "Train Epoch: 15 [2240/2500 (89%)]\tLoss: 0.110342\n",
      "Train Epoch: 15 [2320/2500 (93%)]\tLoss: 0.085581\n",
      "Train Epoch: 15 [2400/2500 (96%)]\tLoss: 0.138541\n",
      "Train Epoch: 15 [2480/2500 (99%)]\tLoss: 0.068595\n",
      "====> Epoch: 15 Average loss: 0.1252\n",
      "\n",
      "Started training epoch no. 17\n",
      "Train Epoch: 16 [0/2500 (0%)]\tLoss: 0.065132\n",
      "Train Epoch: 16 [80/2500 (3%)]\tLoss: 0.131027\n",
      "Train Epoch: 16 [160/2500 (6%)]\tLoss: 0.092502\n",
      "Train Epoch: 16 [240/2500 (10%)]\tLoss: 0.109534\n",
      "Train Epoch: 16 [320/2500 (13%)]\tLoss: 0.094082\n",
      "Train Epoch: 16 [400/2500 (16%)]\tLoss: 0.072762\n",
      "Train Epoch: 16 [480/2500 (19%)]\tLoss: 0.121761\n",
      "Train Epoch: 16 [560/2500 (22%)]\tLoss: 0.118392\n",
      "Train Epoch: 16 [640/2500 (26%)]\tLoss: 0.102265\n",
      "Train Epoch: 16 [720/2500 (29%)]\tLoss: 0.091639\n",
      "Train Epoch: 16 [800/2500 (32%)]\tLoss: 0.157955\n",
      "Train Epoch: 16 [880/2500 (35%)]\tLoss: 0.069298\n",
      "Train Epoch: 16 [960/2500 (38%)]\tLoss: 0.078559\n",
      "Train Epoch: 16 [1040/2500 (42%)]\tLoss: 0.110799\n",
      "Train Epoch: 16 [1120/2500 (45%)]\tLoss: 0.241570\n",
      "Train Epoch: 16 [1200/2500 (48%)]\tLoss: 0.065747\n",
      "Train Epoch: 16 [1280/2500 (51%)]\tLoss: 0.196934\n",
      "Train Epoch: 16 [1360/2500 (54%)]\tLoss: 0.148991\n",
      "Train Epoch: 16 [1440/2500 (58%)]\tLoss: 0.116797\n",
      "Train Epoch: 16 [1520/2500 (61%)]\tLoss: 0.121374\n",
      "Train Epoch: 16 [1600/2500 (64%)]\tLoss: 0.195843\n",
      "Train Epoch: 16 [1680/2500 (67%)]\tLoss: 0.050699\n",
      "Train Epoch: 16 [1760/2500 (70%)]\tLoss: 0.157623\n",
      "Train Epoch: 16 [1840/2500 (73%)]\tLoss: 0.080890\n",
      "Train Epoch: 16 [1920/2500 (77%)]\tLoss: 0.128798\n",
      "Train Epoch: 16 [2000/2500 (80%)]\tLoss: 0.181889\n",
      "Train Epoch: 16 [2080/2500 (83%)]\tLoss: 0.174678\n",
      "Train Epoch: 16 [2160/2500 (86%)]\tLoss: 0.105156\n",
      "Train Epoch: 16 [2240/2500 (89%)]\tLoss: 0.058015\n",
      "Train Epoch: 16 [2320/2500 (93%)]\tLoss: 0.188570\n",
      "Train Epoch: 16 [2400/2500 (96%)]\tLoss: 0.075898\n",
      "Train Epoch: 16 [2480/2500 (99%)]\tLoss: 0.243011\n",
      "====> Epoch: 16 Average loss: 0.1253\n",
      "\n",
      "Started training epoch no. 18\n",
      "Train Epoch: 17 [0/2500 (0%)]\tLoss: 0.257003\n",
      "Train Epoch: 17 [80/2500 (3%)]\tLoss: 0.112012\n",
      "Train Epoch: 17 [160/2500 (6%)]\tLoss: 0.128213\n",
      "Train Epoch: 17 [240/2500 (10%)]\tLoss: 0.067118\n",
      "Train Epoch: 17 [320/2500 (13%)]\tLoss: 0.193344\n",
      "Train Epoch: 17 [400/2500 (16%)]\tLoss: 0.099212\n",
      "Train Epoch: 17 [480/2500 (19%)]\tLoss: 0.073212\n",
      "Train Epoch: 17 [560/2500 (22%)]\tLoss: 0.114888\n",
      "Train Epoch: 17 [640/2500 (26%)]\tLoss: 0.187180\n",
      "Train Epoch: 17 [720/2500 (29%)]\tLoss: 0.097472\n",
      "Train Epoch: 17 [800/2500 (32%)]\tLoss: 0.087316\n",
      "Train Epoch: 17 [880/2500 (35%)]\tLoss: 0.154458\n",
      "Train Epoch: 17 [960/2500 (38%)]\tLoss: 0.117646\n",
      "Train Epoch: 17 [1040/2500 (42%)]\tLoss: 0.134281\n",
      "Train Epoch: 17 [1120/2500 (45%)]\tLoss: 0.092166\n",
      "Train Epoch: 17 [1200/2500 (48%)]\tLoss: 0.142626\n",
      "Train Epoch: 17 [1280/2500 (51%)]\tLoss: 0.124814\n",
      "Train Epoch: 17 [1360/2500 (54%)]\tLoss: 0.147732\n",
      "Train Epoch: 17 [1440/2500 (58%)]\tLoss: 0.111618\n",
      "Train Epoch: 17 [1520/2500 (61%)]\tLoss: 0.141285\n",
      "Train Epoch: 17 [1600/2500 (64%)]\tLoss: 0.155882\n",
      "Train Epoch: 17 [1680/2500 (67%)]\tLoss: 0.088900\n",
      "Train Epoch: 17 [1760/2500 (70%)]\tLoss: 0.207220\n",
      "Train Epoch: 17 [1840/2500 (73%)]\tLoss: 0.063001\n",
      "Train Epoch: 17 [1920/2500 (77%)]\tLoss: 0.119482\n",
      "Train Epoch: 17 [2000/2500 (80%)]\tLoss: 0.090805\n",
      "Train Epoch: 17 [2080/2500 (83%)]\tLoss: 0.081745\n",
      "Train Epoch: 17 [2160/2500 (86%)]\tLoss: 0.168043\n",
      "Train Epoch: 17 [2240/2500 (89%)]\tLoss: 0.147827\n",
      "Train Epoch: 17 [2320/2500 (93%)]\tLoss: 0.063909\n",
      "Train Epoch: 17 [2400/2500 (96%)]\tLoss: 0.160654\n",
      "Train Epoch: 17 [2480/2500 (99%)]\tLoss: 0.112312\n",
      "====> Epoch: 17 Average loss: 0.1252\n",
      "\n",
      "Started training epoch no. 19\n",
      "Train Epoch: 18 [0/2500 (0%)]\tLoss: 0.114305\n",
      "Train Epoch: 18 [80/2500 (3%)]\tLoss: 0.192029\n",
      "Train Epoch: 18 [160/2500 (6%)]\tLoss: 0.175581\n",
      "Train Epoch: 18 [240/2500 (10%)]\tLoss: 0.072242\n",
      "Train Epoch: 18 [320/2500 (13%)]\tLoss: 0.096855\n",
      "Train Epoch: 18 [400/2500 (16%)]\tLoss: 0.109127\n",
      "Train Epoch: 18 [480/2500 (19%)]\tLoss: 0.225146\n",
      "Train Epoch: 18 [560/2500 (22%)]\tLoss: 0.100288\n",
      "Train Epoch: 18 [640/2500 (26%)]\tLoss: 0.140252\n",
      "Train Epoch: 18 [720/2500 (29%)]\tLoss: 0.069558\n",
      "Train Epoch: 18 [800/2500 (32%)]\tLoss: 0.263559\n",
      "Train Epoch: 18 [880/2500 (35%)]\tLoss: 0.139272\n",
      "Train Epoch: 18 [960/2500 (38%)]\tLoss: 0.165704\n",
      "Train Epoch: 18 [1040/2500 (42%)]\tLoss: 0.058856\n",
      "Train Epoch: 18 [1120/2500 (45%)]\tLoss: 0.143674\n",
      "Train Epoch: 18 [1200/2500 (48%)]\tLoss: 0.078390\n",
      "Train Epoch: 18 [1280/2500 (51%)]\tLoss: 0.186260\n",
      "Train Epoch: 18 [1360/2500 (54%)]\tLoss: 0.215754\n",
      "Train Epoch: 18 [1440/2500 (58%)]\tLoss: 0.089576\n",
      "Train Epoch: 18 [1520/2500 (61%)]\tLoss: 0.105177\n",
      "Train Epoch: 18 [1600/2500 (64%)]\tLoss: 0.081390\n",
      "Train Epoch: 18 [1680/2500 (67%)]\tLoss: 0.151295\n",
      "Train Epoch: 18 [1760/2500 (70%)]\tLoss: 0.077272\n",
      "Train Epoch: 18 [1840/2500 (73%)]\tLoss: 0.086162\n",
      "Train Epoch: 18 [1920/2500 (77%)]\tLoss: 0.066863\n",
      "Train Epoch: 18 [2000/2500 (80%)]\tLoss: 0.082027\n",
      "Train Epoch: 18 [2080/2500 (83%)]\tLoss: 0.177532\n",
      "Train Epoch: 18 [2160/2500 (86%)]\tLoss: 0.089494\n",
      "Train Epoch: 18 [2240/2500 (89%)]\tLoss: 0.174700\n",
      "Train Epoch: 18 [2320/2500 (93%)]\tLoss: 0.093029\n",
      "Train Epoch: 18 [2400/2500 (96%)]\tLoss: 0.095815\n",
      "Train Epoch: 18 [2480/2500 (99%)]\tLoss: 0.109508\n",
      "====> Epoch: 18 Average loss: 0.1252\n",
      "\n",
      "Started training epoch no. 20\n",
      "Train Epoch: 19 [0/2500 (0%)]\tLoss: 0.094549\n",
      "Train Epoch: 19 [80/2500 (3%)]\tLoss: 0.084564\n",
      "Train Epoch: 19 [160/2500 (6%)]\tLoss: 0.234991\n",
      "Train Epoch: 19 [240/2500 (10%)]\tLoss: 0.088747\n",
      "Train Epoch: 19 [320/2500 (13%)]\tLoss: 0.174816\n",
      "Train Epoch: 19 [400/2500 (16%)]\tLoss: 0.146128\n",
      "Train Epoch: 19 [480/2500 (19%)]\tLoss: 0.102605\n",
      "Train Epoch: 19 [560/2500 (22%)]\tLoss: 0.066583\n",
      "Train Epoch: 19 [640/2500 (26%)]\tLoss: 0.149694\n",
      "Train Epoch: 19 [720/2500 (29%)]\tLoss: 0.135614\n",
      "Train Epoch: 19 [800/2500 (32%)]\tLoss: 0.074260\n",
      "Train Epoch: 19 [880/2500 (35%)]\tLoss: 0.071450\n",
      "Train Epoch: 19 [960/2500 (38%)]\tLoss: 0.143508\n",
      "Train Epoch: 19 [1040/2500 (42%)]\tLoss: 0.072540\n",
      "Train Epoch: 19 [1120/2500 (45%)]\tLoss: 0.165693\n",
      "Train Epoch: 19 [1200/2500 (48%)]\tLoss: 0.165937\n",
      "Train Epoch: 19 [1280/2500 (51%)]\tLoss: 0.058890\n",
      "Train Epoch: 19 [1360/2500 (54%)]\tLoss: 0.195580\n",
      "Train Epoch: 19 [1440/2500 (58%)]\tLoss: 0.108014\n",
      "Train Epoch: 19 [1520/2500 (61%)]\tLoss: 0.155838\n",
      "Train Epoch: 19 [1600/2500 (64%)]\tLoss: 0.119702\n",
      "Train Epoch: 19 [1680/2500 (67%)]\tLoss: 0.165195\n",
      "Train Epoch: 19 [1760/2500 (70%)]\tLoss: 0.070673\n",
      "Train Epoch: 19 [1840/2500 (73%)]\tLoss: 0.128492\n",
      "Train Epoch: 19 [1920/2500 (77%)]\tLoss: 0.094031\n",
      "Train Epoch: 19 [2000/2500 (80%)]\tLoss: 0.115662\n",
      "Train Epoch: 19 [2080/2500 (83%)]\tLoss: 0.057203\n",
      "Train Epoch: 19 [2160/2500 (86%)]\tLoss: 0.066918\n",
      "Train Epoch: 19 [2240/2500 (89%)]\tLoss: 0.100958\n",
      "Train Epoch: 19 [2320/2500 (93%)]\tLoss: 0.098243\n",
      "Train Epoch: 19 [2400/2500 (96%)]\tLoss: 0.160392\n",
      "Train Epoch: 19 [2480/2500 (99%)]\tLoss: 0.088917\n",
      "====> Epoch: 19 Average loss: 0.1253\n",
      "\n",
      "Started training epoch no. 21\n",
      "Train Epoch: 20 [0/2500 (0%)]\tLoss: 0.090428\n",
      "Train Epoch: 20 [80/2500 (3%)]\tLoss: 0.112677\n",
      "Train Epoch: 20 [160/2500 (6%)]\tLoss: 0.196474\n",
      "Train Epoch: 20 [240/2500 (10%)]\tLoss: 0.166174\n",
      "Train Epoch: 20 [320/2500 (13%)]\tLoss: 0.214231\n",
      "Train Epoch: 20 [400/2500 (16%)]\tLoss: 0.075001\n",
      "Train Epoch: 20 [480/2500 (19%)]\tLoss: 0.080438\n",
      "Train Epoch: 20 [560/2500 (22%)]\tLoss: 0.094071\n",
      "Train Epoch: 20 [640/2500 (26%)]\tLoss: 0.107669\n",
      "Train Epoch: 20 [720/2500 (29%)]\tLoss: 0.209449\n",
      "Train Epoch: 20 [800/2500 (32%)]\tLoss: 0.124994\n",
      "Train Epoch: 20 [880/2500 (35%)]\tLoss: 0.134660\n",
      "Train Epoch: 20 [960/2500 (38%)]\tLoss: 0.177164\n",
      "Train Epoch: 20 [1040/2500 (42%)]\tLoss: 0.081439\n",
      "Train Epoch: 20 [1120/2500 (45%)]\tLoss: 0.084781\n",
      "Train Epoch: 20 [1200/2500 (48%)]\tLoss: 0.237136\n",
      "Train Epoch: 20 [1280/2500 (51%)]\tLoss: 0.079170\n",
      "Train Epoch: 20 [1360/2500 (54%)]\tLoss: 0.134135\n",
      "Train Epoch: 20 [1440/2500 (58%)]\tLoss: 0.163178\n",
      "Train Epoch: 20 [1520/2500 (61%)]\tLoss: 0.069501\n",
      "Train Epoch: 20 [1600/2500 (64%)]\tLoss: 0.137920\n",
      "Train Epoch: 20 [1680/2500 (67%)]\tLoss: 0.148350\n",
      "Train Epoch: 20 [1760/2500 (70%)]\tLoss: 0.086968\n",
      "Train Epoch: 20 [1840/2500 (73%)]\tLoss: 0.186450\n",
      "Train Epoch: 20 [1920/2500 (77%)]\tLoss: 0.111891\n",
      "Train Epoch: 20 [2000/2500 (80%)]\tLoss: 0.074387\n",
      "Train Epoch: 20 [2080/2500 (83%)]\tLoss: 0.167469\n",
      "Train Epoch: 20 [2160/2500 (86%)]\tLoss: 0.117598\n",
      "Train Epoch: 20 [2240/2500 (89%)]\tLoss: 0.147440\n",
      "Train Epoch: 20 [2320/2500 (93%)]\tLoss: 0.056571\n",
      "Train Epoch: 20 [2400/2500 (96%)]\tLoss: 0.208353\n",
      "Train Epoch: 20 [2480/2500 (99%)]\tLoss: 0.165583\n",
      "====> Epoch: 20 Average loss: 0.1253\n",
      "\n",
      "Started training epoch no. 22\n",
      "Train Epoch: 21 [0/2500 (0%)]\tLoss: 0.128742\n",
      "Train Epoch: 21 [80/2500 (3%)]\tLoss: 0.115613\n",
      "Train Epoch: 21 [160/2500 (6%)]\tLoss: 0.155437\n",
      "Train Epoch: 21 [240/2500 (10%)]\tLoss: 0.138269\n",
      "Train Epoch: 21 [320/2500 (13%)]\tLoss: 0.089956\n",
      "Train Epoch: 21 [400/2500 (16%)]\tLoss: 0.142705\n",
      "Train Epoch: 21 [480/2500 (19%)]\tLoss: 0.077571\n",
      "Train Epoch: 21 [560/2500 (22%)]\tLoss: 0.172590\n",
      "Train Epoch: 21 [640/2500 (26%)]\tLoss: 0.119401\n",
      "Train Epoch: 21 [720/2500 (29%)]\tLoss: 0.182993\n",
      "Train Epoch: 21 [800/2500 (32%)]\tLoss: 0.071416\n",
      "Train Epoch: 21 [880/2500 (35%)]\tLoss: 0.157458\n",
      "Train Epoch: 21 [960/2500 (38%)]\tLoss: 0.157649\n",
      "Train Epoch: 21 [1040/2500 (42%)]\tLoss: 0.095333\n",
      "Train Epoch: 21 [1120/2500 (45%)]\tLoss: 0.162272\n",
      "Train Epoch: 21 [1200/2500 (48%)]\tLoss: 0.095845\n",
      "Train Epoch: 21 [1280/2500 (51%)]\tLoss: 0.132659\n",
      "Train Epoch: 21 [1360/2500 (54%)]\tLoss: 0.078420\n",
      "Train Epoch: 21 [1440/2500 (58%)]\tLoss: 0.147994\n",
      "Train Epoch: 21 [1520/2500 (61%)]\tLoss: 0.070427\n",
      "Train Epoch: 21 [1600/2500 (64%)]\tLoss: 0.109681\n",
      "Train Epoch: 21 [1680/2500 (67%)]\tLoss: 0.189194\n",
      "Train Epoch: 21 [1760/2500 (70%)]\tLoss: 0.172797\n",
      "Train Epoch: 21 [1840/2500 (73%)]\tLoss: 0.080168\n",
      "Train Epoch: 21 [1920/2500 (77%)]\tLoss: 0.166903\n",
      "Train Epoch: 21 [2000/2500 (80%)]\tLoss: 0.126119\n",
      "Train Epoch: 21 [2080/2500 (83%)]\tLoss: 0.181913\n",
      "Train Epoch: 21 [2160/2500 (86%)]\tLoss: 0.120702\n",
      "Train Epoch: 21 [2240/2500 (89%)]\tLoss: 0.047623\n",
      "Train Epoch: 21 [2320/2500 (93%)]\tLoss: 0.157474\n",
      "Train Epoch: 21 [2400/2500 (96%)]\tLoss: 0.119082\n",
      "Train Epoch: 21 [2480/2500 (99%)]\tLoss: 0.095657\n",
      "====> Epoch: 21 Average loss: 0.1251\n",
      "\n",
      "Started training epoch no. 23\n",
      "Train Epoch: 22 [0/2500 (0%)]\tLoss: 0.124772\n",
      "Train Epoch: 22 [80/2500 (3%)]\tLoss: 0.101366\n",
      "Train Epoch: 22 [160/2500 (6%)]\tLoss: 0.105412\n",
      "Train Epoch: 22 [240/2500 (10%)]\tLoss: 0.208177\n",
      "Train Epoch: 22 [320/2500 (13%)]\tLoss: 0.126826\n",
      "Train Epoch: 22 [400/2500 (16%)]\tLoss: 0.087000\n",
      "Train Epoch: 22 [480/2500 (19%)]\tLoss: 0.195273\n",
      "Train Epoch: 22 [560/2500 (22%)]\tLoss: 0.081554\n",
      "Train Epoch: 22 [640/2500 (26%)]\tLoss: 0.113413\n",
      "Train Epoch: 22 [720/2500 (29%)]\tLoss: 0.123553\n",
      "Train Epoch: 22 [800/2500 (32%)]\tLoss: 0.087334\n",
      "Train Epoch: 22 [880/2500 (35%)]\tLoss: 0.256166\n",
      "Train Epoch: 22 [960/2500 (38%)]\tLoss: 0.137778\n",
      "Train Epoch: 22 [1040/2500 (42%)]\tLoss: 0.110811\n",
      "Train Epoch: 22 [1120/2500 (45%)]\tLoss: 0.092885\n",
      "Train Epoch: 22 [1200/2500 (48%)]\tLoss: 0.128605\n",
      "Train Epoch: 22 [1280/2500 (51%)]\tLoss: 0.110754\n",
      "Train Epoch: 22 [1360/2500 (54%)]\tLoss: 0.099943\n",
      "Train Epoch: 22 [1440/2500 (58%)]\tLoss: 0.131104\n",
      "Train Epoch: 22 [1520/2500 (61%)]\tLoss: 0.063714\n",
      "Train Epoch: 22 [1600/2500 (64%)]\tLoss: 0.134920\n",
      "Train Epoch: 22 [1680/2500 (67%)]\tLoss: 0.123798\n",
      "Train Epoch: 22 [1760/2500 (70%)]\tLoss: 0.114262\n",
      "Train Epoch: 22 [1840/2500 (73%)]\tLoss: 0.078942\n",
      "Train Epoch: 22 [1920/2500 (77%)]\tLoss: 0.166114\n",
      "Train Epoch: 22 [2000/2500 (80%)]\tLoss: 0.081994\n",
      "Train Epoch: 22 [2080/2500 (83%)]\tLoss: 0.086174\n",
      "Train Epoch: 22 [2160/2500 (86%)]\tLoss: 0.057681\n",
      "Train Epoch: 22 [2240/2500 (89%)]\tLoss: 0.144027\n",
      "Train Epoch: 22 [2320/2500 (93%)]\tLoss: 0.119243\n",
      "Train Epoch: 22 [2400/2500 (96%)]\tLoss: 0.053812\n",
      "Train Epoch: 22 [2480/2500 (99%)]\tLoss: 0.129710\n",
      "====> Epoch: 22 Average loss: 0.1254\n",
      "\n",
      "Started training epoch no. 24\n",
      "Train Epoch: 23 [0/2500 (0%)]\tLoss: 0.086746\n",
      "Train Epoch: 23 [80/2500 (3%)]\tLoss: 0.125822\n",
      "Train Epoch: 23 [160/2500 (6%)]\tLoss: 0.107283\n",
      "Train Epoch: 23 [240/2500 (10%)]\tLoss: 0.106093\n",
      "Train Epoch: 23 [320/2500 (13%)]\tLoss: 0.113590\n",
      "Train Epoch: 23 [400/2500 (16%)]\tLoss: 0.098393\n",
      "Train Epoch: 23 [480/2500 (19%)]\tLoss: 0.132662\n",
      "Train Epoch: 23 [560/2500 (22%)]\tLoss: 0.109186\n",
      "Train Epoch: 23 [640/2500 (26%)]\tLoss: 0.083712\n",
      "Train Epoch: 23 [720/2500 (29%)]\tLoss: 0.119272\n",
      "Train Epoch: 23 [800/2500 (32%)]\tLoss: 0.125665\n",
      "Train Epoch: 23 [880/2500 (35%)]\tLoss: 0.061554\n",
      "Train Epoch: 23 [960/2500 (38%)]\tLoss: 0.089690\n",
      "Train Epoch: 23 [1040/2500 (42%)]\tLoss: 0.162417\n",
      "Train Epoch: 23 [1120/2500 (45%)]\tLoss: 0.189888\n",
      "Train Epoch: 23 [1200/2500 (48%)]\tLoss: 0.058365\n",
      "Train Epoch: 23 [1280/2500 (51%)]\tLoss: 0.123453\n",
      "Train Epoch: 23 [1360/2500 (54%)]\tLoss: 0.124991\n",
      "Train Epoch: 23 [1440/2500 (58%)]\tLoss: 0.183436\n",
      "Train Epoch: 23 [1520/2500 (61%)]\tLoss: 0.070337\n",
      "Train Epoch: 23 [1600/2500 (64%)]\tLoss: 0.153077\n",
      "Train Epoch: 23 [1680/2500 (67%)]\tLoss: 0.120188\n",
      "Train Epoch: 23 [1760/2500 (70%)]\tLoss: 0.189555\n",
      "Train Epoch: 23 [1840/2500 (73%)]\tLoss: 0.125296\n",
      "Train Epoch: 23 [1920/2500 (77%)]\tLoss: 0.117166\n",
      "Train Epoch: 23 [2000/2500 (80%)]\tLoss: 0.075377\n",
      "Train Epoch: 23 [2080/2500 (83%)]\tLoss: 0.105818\n",
      "Train Epoch: 23 [2160/2500 (86%)]\tLoss: 0.123808\n",
      "Train Epoch: 23 [2240/2500 (89%)]\tLoss: 0.154095\n",
      "Train Epoch: 23 [2320/2500 (93%)]\tLoss: 0.149405\n",
      "Train Epoch: 23 [2400/2500 (96%)]\tLoss: 0.161645\n",
      "Train Epoch: 23 [2480/2500 (99%)]\tLoss: 0.091791\n",
      "====> Epoch: 23 Average loss: 0.1251\n",
      "\n",
      "Started training epoch no. 25\n",
      "Train Epoch: 24 [0/2500 (0%)]\tLoss: 0.071394\n",
      "Train Epoch: 24 [80/2500 (3%)]\tLoss: 0.122113\n",
      "Train Epoch: 24 [160/2500 (6%)]\tLoss: 0.097880\n",
      "Train Epoch: 24 [240/2500 (10%)]\tLoss: 0.098736\n",
      "Train Epoch: 24 [320/2500 (13%)]\tLoss: 0.119468\n",
      "Train Epoch: 24 [400/2500 (16%)]\tLoss: 0.150835\n",
      "Train Epoch: 24 [480/2500 (19%)]\tLoss: 0.086571\n",
      "Train Epoch: 24 [560/2500 (22%)]\tLoss: 0.096695\n",
      "Train Epoch: 24 [640/2500 (26%)]\tLoss: 0.086321\n",
      "Train Epoch: 24 [720/2500 (29%)]\tLoss: 0.181434\n",
      "Train Epoch: 24 [800/2500 (32%)]\tLoss: 0.127878\n",
      "Train Epoch: 24 [880/2500 (35%)]\tLoss: 0.124397\n",
      "Train Epoch: 24 [960/2500 (38%)]\tLoss: 0.077162\n",
      "Train Epoch: 24 [1040/2500 (42%)]\tLoss: 0.122309\n",
      "Train Epoch: 24 [1120/2500 (45%)]\tLoss: 0.138060\n",
      "Train Epoch: 24 [1200/2500 (48%)]\tLoss: 0.171140\n",
      "Train Epoch: 24 [1280/2500 (51%)]\tLoss: 0.067874\n",
      "Train Epoch: 24 [1360/2500 (54%)]\tLoss: 0.127862\n",
      "Train Epoch: 24 [1440/2500 (58%)]\tLoss: 0.203501\n",
      "Train Epoch: 24 [1520/2500 (61%)]\tLoss: 0.189033\n",
      "Train Epoch: 24 [1600/2500 (64%)]\tLoss: 0.152854\n",
      "Train Epoch: 24 [1680/2500 (67%)]\tLoss: 0.175292\n",
      "Train Epoch: 24 [1760/2500 (70%)]\tLoss: 0.150840\n",
      "Train Epoch: 24 [1840/2500 (73%)]\tLoss: 0.121228\n",
      "Train Epoch: 24 [1920/2500 (77%)]\tLoss: 0.196236\n",
      "Train Epoch: 24 [2000/2500 (80%)]\tLoss: 0.094650\n",
      "Train Epoch: 24 [2080/2500 (83%)]\tLoss: 0.055280\n",
      "Train Epoch: 24 [2160/2500 (86%)]\tLoss: 0.118868\n",
      "Train Epoch: 24 [2240/2500 (89%)]\tLoss: 0.118430\n",
      "Train Epoch: 24 [2320/2500 (93%)]\tLoss: 0.120080\n",
      "Train Epoch: 24 [2400/2500 (96%)]\tLoss: 0.282449\n",
      "Train Epoch: 24 [2480/2500 (99%)]\tLoss: 0.062567\n",
      "====> Epoch: 24 Average loss: 0.1252\n",
      "\n",
      "Started training epoch no. 26\n",
      "Train Epoch: 25 [0/2500 (0%)]\tLoss: 0.194567\n",
      "Train Epoch: 25 [80/2500 (3%)]\tLoss: 0.140874\n",
      "Train Epoch: 25 [160/2500 (6%)]\tLoss: 0.110804\n",
      "Train Epoch: 25 [240/2500 (10%)]\tLoss: 0.217959\n",
      "Train Epoch: 25 [320/2500 (13%)]\tLoss: 0.159181\n",
      "Train Epoch: 25 [400/2500 (16%)]\tLoss: 0.092118\n",
      "Train Epoch: 25 [480/2500 (19%)]\tLoss: 0.146616\n",
      "Train Epoch: 25 [560/2500 (22%)]\tLoss: 0.127583\n",
      "Train Epoch: 25 [640/2500 (26%)]\tLoss: 0.078423\n",
      "Train Epoch: 25 [720/2500 (29%)]\tLoss: 0.213832\n",
      "Train Epoch: 25 [800/2500 (32%)]\tLoss: 0.095351\n",
      "Train Epoch: 25 [880/2500 (35%)]\tLoss: 0.166789\n",
      "Train Epoch: 25 [960/2500 (38%)]\tLoss: 0.129693\n",
      "Train Epoch: 25 [1040/2500 (42%)]\tLoss: 0.114854\n",
      "Train Epoch: 25 [1120/2500 (45%)]\tLoss: 0.085227\n",
      "Train Epoch: 25 [1200/2500 (48%)]\tLoss: 0.078949\n",
      "Train Epoch: 25 [1280/2500 (51%)]\tLoss: 0.163888\n",
      "Train Epoch: 25 [1360/2500 (54%)]\tLoss: 0.062357\n",
      "Train Epoch: 25 [1440/2500 (58%)]\tLoss: 0.114593\n",
      "Train Epoch: 25 [1520/2500 (61%)]\tLoss: 0.183795\n",
      "Train Epoch: 25 [1600/2500 (64%)]\tLoss: 0.060157\n",
      "Train Epoch: 25 [1680/2500 (67%)]\tLoss: 0.171076\n",
      "Train Epoch: 25 [1760/2500 (70%)]\tLoss: 0.075686\n",
      "Train Epoch: 25 [1840/2500 (73%)]\tLoss: 0.064946\n",
      "Train Epoch: 25 [1920/2500 (77%)]\tLoss: 0.110526\n",
      "Train Epoch: 25 [2000/2500 (80%)]\tLoss: 0.146169\n",
      "Train Epoch: 25 [2080/2500 (83%)]\tLoss: 0.171687\n",
      "Train Epoch: 25 [2160/2500 (86%)]\tLoss: 0.122644\n",
      "Train Epoch: 25 [2240/2500 (89%)]\tLoss: 0.174975\n",
      "Train Epoch: 25 [2320/2500 (93%)]\tLoss: 0.113335\n",
      "Train Epoch: 25 [2400/2500 (96%)]\tLoss: 0.105770\n",
      "Train Epoch: 25 [2480/2500 (99%)]\tLoss: 0.068911\n",
      "====> Epoch: 25 Average loss: 0.1251\n",
      "\n",
      "Started training epoch no. 27\n",
      "Train Epoch: 26 [0/2500 (0%)]\tLoss: 0.098727\n",
      "Train Epoch: 26 [80/2500 (3%)]\tLoss: 0.252203\n",
      "Train Epoch: 26 [160/2500 (6%)]\tLoss: 0.109404\n",
      "Train Epoch: 26 [240/2500 (10%)]\tLoss: 0.138537\n",
      "Train Epoch: 26 [320/2500 (13%)]\tLoss: 0.129306\n",
      "Train Epoch: 26 [400/2500 (16%)]\tLoss: 0.096714\n",
      "Train Epoch: 26 [480/2500 (19%)]\tLoss: 0.083519\n",
      "Train Epoch: 26 [560/2500 (22%)]\tLoss: 0.079436\n",
      "Train Epoch: 26 [640/2500 (26%)]\tLoss: 0.114774\n",
      "Train Epoch: 26 [720/2500 (29%)]\tLoss: 0.145558\n",
      "Train Epoch: 26 [800/2500 (32%)]\tLoss: 0.262270\n",
      "Train Epoch: 26 [880/2500 (35%)]\tLoss: 0.112905\n",
      "Train Epoch: 26 [960/2500 (38%)]\tLoss: 0.252706\n",
      "Train Epoch: 26 [1040/2500 (42%)]\tLoss: 0.084801\n",
      "Train Epoch: 26 [1120/2500 (45%)]\tLoss: 0.152888\n",
      "Train Epoch: 26 [1200/2500 (48%)]\tLoss: 0.110329\n",
      "Train Epoch: 26 [1280/2500 (51%)]\tLoss: 0.100889\n",
      "Train Epoch: 26 [1360/2500 (54%)]\tLoss: 0.132260\n",
      "Train Epoch: 26 [1440/2500 (58%)]\tLoss: 0.170868\n",
      "Train Epoch: 26 [1520/2500 (61%)]\tLoss: 0.082736\n",
      "Train Epoch: 26 [1600/2500 (64%)]\tLoss: 0.078981\n",
      "Train Epoch: 26 [1680/2500 (67%)]\tLoss: 0.065002\n",
      "Train Epoch: 26 [1760/2500 (70%)]\tLoss: 0.123480\n",
      "Train Epoch: 26 [1840/2500 (73%)]\tLoss: 0.077835\n",
      "Train Epoch: 26 [1920/2500 (77%)]\tLoss: 0.130502\n",
      "Train Epoch: 26 [2000/2500 (80%)]\tLoss: 0.114152\n",
      "Train Epoch: 26 [2080/2500 (83%)]\tLoss: 0.073703\n",
      "Train Epoch: 26 [2160/2500 (86%)]\tLoss: 0.041653\n",
      "Train Epoch: 26 [2240/2500 (89%)]\tLoss: 0.231502\n",
      "Train Epoch: 26 [2320/2500 (93%)]\tLoss: 0.141265\n",
      "Train Epoch: 26 [2400/2500 (96%)]\tLoss: 0.148510\n",
      "Train Epoch: 26 [2480/2500 (99%)]\tLoss: 0.111965\n",
      "====> Epoch: 26 Average loss: 0.1252\n",
      "\n",
      "Started training epoch no. 28\n",
      "Train Epoch: 27 [0/2500 (0%)]\tLoss: 0.154908\n",
      "Train Epoch: 27 [80/2500 (3%)]\tLoss: 0.171572\n",
      "Train Epoch: 27 [160/2500 (6%)]\tLoss: 0.180735\n",
      "Train Epoch: 27 [240/2500 (10%)]\tLoss: 0.074720\n",
      "Train Epoch: 27 [320/2500 (13%)]\tLoss: 0.165281\n",
      "Train Epoch: 27 [400/2500 (16%)]\tLoss: 0.129856\n",
      "Train Epoch: 27 [480/2500 (19%)]\tLoss: 0.113195\n",
      "Train Epoch: 27 [560/2500 (22%)]\tLoss: 0.111543\n",
      "Train Epoch: 27 [640/2500 (26%)]\tLoss: 0.093736\n",
      "Train Epoch: 27 [720/2500 (29%)]\tLoss: 0.132875\n",
      "Train Epoch: 27 [800/2500 (32%)]\tLoss: 0.065810\n",
      "Train Epoch: 27 [880/2500 (35%)]\tLoss: 0.153935\n",
      "Train Epoch: 27 [960/2500 (38%)]\tLoss: 0.095254\n",
      "Train Epoch: 27 [1040/2500 (42%)]\tLoss: 0.093531\n",
      "Train Epoch: 27 [1120/2500 (45%)]\tLoss: 0.122934\n",
      "Train Epoch: 27 [1200/2500 (48%)]\tLoss: 0.093534\n",
      "Train Epoch: 27 [1280/2500 (51%)]\tLoss: 0.233353\n",
      "Train Epoch: 27 [1360/2500 (54%)]\tLoss: 0.126768\n",
      "Train Epoch: 27 [1440/2500 (58%)]\tLoss: 0.148215\n",
      "Train Epoch: 27 [1520/2500 (61%)]\tLoss: 0.302500\n",
      "Train Epoch: 27 [1600/2500 (64%)]\tLoss: 0.241859\n",
      "Train Epoch: 27 [1680/2500 (67%)]\tLoss: 0.123349\n",
      "Train Epoch: 27 [1760/2500 (70%)]\tLoss: 0.117409\n",
      "Train Epoch: 27 [1840/2500 (73%)]\tLoss: 0.049624\n",
      "Train Epoch: 27 [1920/2500 (77%)]\tLoss: 0.084505\n",
      "Train Epoch: 27 [2000/2500 (80%)]\tLoss: 0.151359\n",
      "Train Epoch: 27 [2080/2500 (83%)]\tLoss: 0.137800\n",
      "Train Epoch: 27 [2160/2500 (86%)]\tLoss: 0.118617\n",
      "Train Epoch: 27 [2240/2500 (89%)]\tLoss: 0.090717\n",
      "Train Epoch: 27 [2320/2500 (93%)]\tLoss: 0.150342\n",
      "Train Epoch: 27 [2400/2500 (96%)]\tLoss: 0.121553\n",
      "Train Epoch: 27 [2480/2500 (99%)]\tLoss: 0.071594\n",
      "====> Epoch: 27 Average loss: 0.1252\n",
      "\n",
      "Started training epoch no. 29\n",
      "Train Epoch: 28 [0/2500 (0%)]\tLoss: 0.107621\n",
      "Train Epoch: 28 [80/2500 (3%)]\tLoss: 0.180199\n",
      "Train Epoch: 28 [160/2500 (6%)]\tLoss: 0.168002\n",
      "Train Epoch: 28 [240/2500 (10%)]\tLoss: 0.131991\n",
      "Train Epoch: 28 [320/2500 (13%)]\tLoss: 0.202511\n",
      "Train Epoch: 28 [400/2500 (16%)]\tLoss: 0.200223\n",
      "Train Epoch: 28 [480/2500 (19%)]\tLoss: 0.111773\n",
      "Train Epoch: 28 [560/2500 (22%)]\tLoss: 0.170474\n",
      "Train Epoch: 28 [640/2500 (26%)]\tLoss: 0.126069\n",
      "Train Epoch: 28 [720/2500 (29%)]\tLoss: 0.132455\n",
      "Train Epoch: 28 [800/2500 (32%)]\tLoss: 0.106939\n",
      "Train Epoch: 28 [880/2500 (35%)]\tLoss: 0.110750\n",
      "Train Epoch: 28 [960/2500 (38%)]\tLoss: 0.125132\n",
      "Train Epoch: 28 [1040/2500 (42%)]\tLoss: 0.056213\n",
      "Train Epoch: 28 [1120/2500 (45%)]\tLoss: 0.066151\n",
      "Train Epoch: 28 [1200/2500 (48%)]\tLoss: 0.100189\n",
      "Train Epoch: 28 [1280/2500 (51%)]\tLoss: 0.093699\n",
      "Train Epoch: 28 [1360/2500 (54%)]\tLoss: 0.131919\n",
      "Train Epoch: 28 [1440/2500 (58%)]\tLoss: 0.126974\n",
      "Train Epoch: 28 [1520/2500 (61%)]\tLoss: 0.098097\n",
      "Train Epoch: 28 [1600/2500 (64%)]\tLoss: 0.122538\n",
      "Train Epoch: 28 [1680/2500 (67%)]\tLoss: 0.141317\n",
      "Train Epoch: 28 [1760/2500 (70%)]\tLoss: 0.119342\n",
      "Train Epoch: 28 [1840/2500 (73%)]\tLoss: 0.161113\n",
      "Train Epoch: 28 [1920/2500 (77%)]\tLoss: 0.173369\n",
      "Train Epoch: 28 [2000/2500 (80%)]\tLoss: 0.179894\n",
      "Train Epoch: 28 [2080/2500 (83%)]\tLoss: 0.121012\n",
      "Train Epoch: 28 [2160/2500 (86%)]\tLoss: 0.201325\n",
      "Train Epoch: 28 [2240/2500 (89%)]\tLoss: 0.168127\n",
      "Train Epoch: 28 [2320/2500 (93%)]\tLoss: 0.149860\n",
      "Train Epoch: 28 [2400/2500 (96%)]\tLoss: 0.135496\n",
      "Train Epoch: 28 [2480/2500 (99%)]\tLoss: 0.124423\n",
      "====> Epoch: 28 Average loss: 0.1253\n",
      "\n",
      "Started training epoch no. 30\n",
      "Train Epoch: 29 [0/2500 (0%)]\tLoss: 0.134880\n",
      "Train Epoch: 29 [80/2500 (3%)]\tLoss: 0.129372\n",
      "Train Epoch: 29 [160/2500 (6%)]\tLoss: 0.087017\n",
      "Train Epoch: 29 [240/2500 (10%)]\tLoss: 0.138020\n",
      "Train Epoch: 29 [320/2500 (13%)]\tLoss: 0.115034\n",
      "Train Epoch: 29 [400/2500 (16%)]\tLoss: 0.173983\n",
      "Train Epoch: 29 [480/2500 (19%)]\tLoss: 0.108976\n",
      "Train Epoch: 29 [560/2500 (22%)]\tLoss: 0.124203\n",
      "Train Epoch: 29 [640/2500 (26%)]\tLoss: 0.145170\n",
      "Train Epoch: 29 [720/2500 (29%)]\tLoss: 0.202891\n",
      "Train Epoch: 29 [800/2500 (32%)]\tLoss: 0.143162\n",
      "Train Epoch: 29 [880/2500 (35%)]\tLoss: 0.098813\n",
      "Train Epoch: 29 [960/2500 (38%)]\tLoss: 0.084767\n",
      "Train Epoch: 29 [1040/2500 (42%)]\tLoss: 0.117337\n",
      "Train Epoch: 29 [1120/2500 (45%)]\tLoss: 0.084492\n",
      "Train Epoch: 29 [1200/2500 (48%)]\tLoss: 0.069319\n",
      "Train Epoch: 29 [1280/2500 (51%)]\tLoss: 0.170458\n",
      "Train Epoch: 29 [1360/2500 (54%)]\tLoss: 0.087306\n",
      "Train Epoch: 29 [1440/2500 (58%)]\tLoss: 0.110599\n",
      "Train Epoch: 29 [1520/2500 (61%)]\tLoss: 0.089174\n",
      "Train Epoch: 29 [1600/2500 (64%)]\tLoss: 0.101571\n",
      "Train Epoch: 29 [1680/2500 (67%)]\tLoss: 0.226110\n",
      "Train Epoch: 29 [1760/2500 (70%)]\tLoss: 0.232018\n",
      "Train Epoch: 29 [1840/2500 (73%)]\tLoss: 0.196160\n",
      "Train Epoch: 29 [1920/2500 (77%)]\tLoss: 0.170142\n",
      "Train Epoch: 29 [2000/2500 (80%)]\tLoss: 0.140690\n",
      "Train Epoch: 29 [2080/2500 (83%)]\tLoss: 0.123542\n",
      "Train Epoch: 29 [2160/2500 (86%)]\tLoss: 0.098971\n",
      "Train Epoch: 29 [2240/2500 (89%)]\tLoss: 0.157640\n",
      "Train Epoch: 29 [2320/2500 (93%)]\tLoss: 0.132205\n",
      "Train Epoch: 29 [2400/2500 (96%)]\tLoss: 0.173748\n",
      "Train Epoch: 29 [2480/2500 (99%)]\tLoss: 0.091416\n",
      "====> Epoch: 29 Average loss: 0.1251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_log = train_epoch(train_loader,model,loss_function,optimizer,num_epochs= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 0, 'train_loss': 2.678865297819479},\n",
       " {'epoch': 1, 'train_loss': 1.1976502431086458},\n",
       " {'epoch': 2, 'train_loss': 1.0680429128031381},\n",
       " {'epoch': 3, 'train_loss': 1.0341266369857727},\n",
       " {'epoch': 4, 'train_loss': 1.0194465192362143},\n",
       " {'epoch': 5, 'train_loss': 1.0122760945615676},\n",
       " {'epoch': 6, 'train_loss': 1.0070156514073332},\n",
       " {'epoch': 7, 'train_loss': 1.0046762004256629},\n",
       " {'epoch': 8, 'train_loss': 1.0038238088734233},\n",
       " {'epoch': 9, 'train_loss': 1.0040661465055265},\n",
       " {'epoch': 10, 'train_loss': 1.0013142653737968},\n",
       " {'epoch': 11, 'train_loss': 1.0019235621435574},\n",
       " {'epoch': 12, 'train_loss': 1.002016035988689},\n",
       " {'epoch': 13, 'train_loss': 1.000093188243933},\n",
       " {'epoch': 14, 'train_loss': 1.0001908958719943},\n",
       " {'epoch': 15, 'train_loss': 1.0001192818434474},\n",
       " {'epoch': 16, 'train_loss': 1.0010630025650367},\n",
       " {'epoch': 17, 'train_loss': 1.0001688158740631},\n",
       " {'epoch': 18, 'train_loss': 1.000037825621736},\n",
       " {'epoch': 19, 'train_loss': 1.0006708033359089},\n",
       " {'epoch': 20, 'train_loss': 1.0009906447162262},\n",
       " {'epoch': 21, 'train_loss': 0.99953651266357},\n",
       " {'epoch': 22, 'train_loss': 1.0015760066981514},\n",
       " {'epoch': 23, 'train_loss': 0.9995862149392454},\n",
       " {'epoch': 24, 'train_loss': 1.0003385381957592},\n",
       " {'epoch': 25, 'train_loss': 0.9995113510293321},\n",
       " {'epoch': 26, 'train_loss': 1.0000924933642243},\n",
       " {'epoch': 27, 'train_loss': 1.0001070502276619},\n",
       " {'epoch': 28, 'train_loss': 1.0007143641432252},\n",
       " {'epoch': 29, 'train_loss': 0.9995449272017128}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = [auth_usecase[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-defined VAE FEATURES\n",
      "Using old scaler\n",
      "['MESH NAME NOT FOUND: Female', \"MESH NAME NOT FOUND: Practice Patterns, Physicians'\", 'MESH NAME NOT FOUND: Phosphoinositide-3 Kinase Inhibitors', 'MESH NAME NOT FOUND: Chlorocebus aethiops', 'MESH NAME NOT FOUND: Male', 'MESH NAME NOT FOUND: Phospholipid Hydroperoxide Glutathione Peroxidase', 'MESH NAME NOT FOUND: Outcome Assessment, Health Care', 'MESH NAME NOT FOUND: Diet, Healthy', 'MESH NAME NOT FOUND: Copper-Transporting ATPases']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AYP/code/py_4/get_all_features.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['co_authors']=df.authors.apply( lambda x: [i['name'] for i in x] )\n"
     ]
    }
   ],
   "source": [
    "train_vae = train_set.__getvae__()\n",
    "test_set = ToyDS(df, selection,train_vae)\n",
    "test_loader=DataLoader(dataset= test_set, batch_size = batch_size, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 32)\n",
      "(18, 64)\n"
     ]
    }
   ],
   "source": [
    "bottle_neck=[]\n",
    "recon_batchs = []\n",
    "for batch_idx, data in enumerate(test_loader):\n",
    "            data = data.to(device, dtype=torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar, z= model(data)\n",
    "            if cuda:\n",
    "                bottle_neck.extend(z.cpu().detach().numpy())\n",
    "                recon_batchs.extend(recon_batch.cpu().detach().numpy())\n",
    "            else:\n",
    "                bottle_neck.extend(z.detach().numpy())\n",
    "                recon_batchs.extend(recon_batch.detach().numpy())\n",
    "bottle_neck = np.array(bottle_neck)\n",
    "recon_batchs = np.array(recon_batchs)\n",
    "print(bottle_neck.shape)\n",
    "print(recon_batchs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 4 nearest neighbors...\n",
      "[t-SNE] Indexed 18 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 18 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 18 / 18\n",
      "[t-SNE] Mean sigma: 0.669660\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.040962\n",
      "[t-SNE] KL divergence after 300 iterations: 0.987678\n",
      "t-SNE done! Time elapsed: 0.045522451400756836 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=1, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(bottle_neck)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset=pd.DataFrame()\n",
    "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "df_subset['PI'] = list(df[df['last_author_name'].isin(selection)][\"PI_IDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = df_subset.groupby('PI')\n",
    "markers = ['o','v',\"x\"]\n",
    "colors = [\"red\",\"black\",\"green\"]\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    ax.set_title(\"TSNE of the Mesh Embeddings\")\n",
    "    ax.plot(group['tsne-2d-one'], group['tsne-2d-two'], marker=markers[i], linestyle='', ms=12, label=name, color=colors[i])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.tight_layout();\n",
    "\n",
    "# fig.savefig('code/img/TSNE_Plots/mesh.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANITY TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ON INPUT DIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 7 nearest neighbors...\n",
      "[t-SNE] Indexed 18 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 18 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 18 / 18\n",
      "[t-SNE] Mean sigma: 0.553659\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 81.470879\n",
      "[t-SNE] KL divergence after 300 iterations: 1.363446\n",
      "t-SNE done! Time elapsed: 0.060804128646850586 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=2, n_iter=300)\n",
    "tsne_inputs = tsne.fit_transform(test_set.features)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "input_testset=pd.DataFrame()\n",
    "input_testset['tsne-2d-one'] = tsne_inputs[:,0]\n",
    "input_testset['tsne-2d-two'] = tsne_inputs[:,1]\n",
    "input_testset['PI'] = list(df[df['last_author_name'].isin(selection)][\"PI_IDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = input_testset.groupby('PI')\n",
    "markers = ['o','v',\"x\"]\n",
    "colors = [\"red\",\"black\",\"green\"]\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    ax.set_title(\"TSNE of the Mesh Embeddings\")\n",
    "    ax.plot(group['tsne-2d-one'], group['tsne-2d-two'], marker=markers[i], linestyle='', ms=12, label=name, color=colors[i])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.tight_layout();\n",
    "\n",
    "# fig.savefig('code/img/TSNE_Plots/mesh.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT DIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 7 nearest neighbors...\n",
      "[t-SNE] Indexed 18 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 18 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 18 / 18\n",
      "[t-SNE] Mean sigma: 0.060664\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 71.467384\n",
      "[t-SNE] KL divergence after 300 iterations: 1.997437\n",
      "t-SNE done! Time elapsed: 0.03989529609680176 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=2, n_iter=300)\n",
    "tsne_output = tsne.fit_transform(recon_batchs)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "output_testset=pd.DataFrame()\n",
    "output_testset['tsne-2d-one'] = tsne_output[:,0]\n",
    "output_testset['tsne-2d-two'] = tsne_output[:,1]\n",
    "output_testset['PI'] = list(df[df['last_author_name'].isin(selection)][\"PI_IDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groups = output_testset.groupby('PI')\n",
    "markers = ['o','v',\"x\"]\n",
    "colors = [\"red\",\"black\",\"green\"]\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    ax.set_title(\"TSNE of the Mesh Embeddings\")\n",
    "    ax.plot(group['tsne-2d-one'], group['tsne-2d-two'], marker=markers[i], linestyle='', ms=12, label=name, color=colors[i])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.tight_layout();\n",
    "\n",
    "# fig.savefig('code/img/TSNE_Plots/mesh.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
