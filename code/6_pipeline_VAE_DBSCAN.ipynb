{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SERVER = True\n",
    "if USE_SERVER:\n",
    "    !pip install elasticsearch\n",
    "    !pip install elasticsearch_dsl\n",
    "    !pip install pymed\n",
    "    !pip install gensim\n",
    "    !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import utils\n",
    "import os\n",
    "from utils import PROJECT_ROOT, DATA_PATH\n",
    "import yuval_module.paper_source as PaperSource\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import py_4.get_mesh_vec as get_mesh_vec\n",
    "import py_3.sim_matrix_3 as sim_matrix_3\n",
    "import py_4.get_all_features as get_all_features \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_embed=get_mesh_vec.MeshEmbeddings(PROJECT_ROOT + \"data/mesh_data/MeSHFeatureGeneratedByDeepWalk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"enriched_labeled_dataset_large\" \n",
    "if os.path.exists(PROJECT_ROOT + DATA_PATH + FILE):\n",
    "    print(\"READING FROM LOCAL\")\n",
    "    if FILE.split(\".\")[1] == \"json\":\n",
    "        df = pd.read_json(PROJECT_ROOT + DATA_PATH + FILE)\n",
    "    else:\n",
    "        df = pd.read_csv(PROJECT_ROOT + DATA_PATH + FILE)\n",
    "    #ps = PaperSource()\n",
    "else:\n",
    "    print(\"PULLING FROM S3\")\n",
    "    ps = sim_matrix_3.load_dataset(FILE)\n",
    "    df = ps.get_dataset()\n",
    "\n",
    "df.drop(columns=[\"last_author_country\"],inplace=True)\n",
    "df.rename(columns={'ORG_STATE':'last_author_country'},inplace=True)\n",
    "\n",
    "print(\"FILE PULLED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETER OPTIMIZATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings\n",
    "\n",
    "BATCH_SIZE= 32\n",
    "EPOCHS = 30\n",
    "cuda = torch.cuda.is_available()\n",
    "seed = 42\n",
    "log_interval = 10\n",
    "num_workers = 2\n",
    "\n",
    "#check for cuda\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: GET THE VAL DATA ###\n",
    "\n",
    "# Get the VAE MODEL\n",
    "\n",
    "model = torch.load(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "#Get the VAE Feat\n",
    "vae_feat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db__eps = np.linspace(.40,.60,40)\n",
    "\n",
    "best_eps = None\n",
    "best_f1 = 0.0\n",
    "\n",
    "for db_eps in db__eps:\n",
    "    df_all_cases = []\n",
    "\n",
    "    for auth in dict_auth:\n",
    "        df_auth = df[df['last_author_name'] == auth] \n",
    "        data = ToyDS(df, selection_test, vae_features = vae_feat.__getvae__())\n",
    "        data = torch.from_numpy(data).to(device,dtype=torch.float32)\n",
    "        _, mu, __ = model(data)\n",
    "        latent_feat = mu.detach().cpu().numpy()\n",
    "        df_all_cases.append([df_auth,latent_feat])\n",
    "        \n",
    "    y_hat_comb = []\n",
    "\n",
    "        \n",
    "    for case in df_all_cases:\n",
    "        df_clus, latent_feat = case\n",
    "        y_hat = DBS(eps=db_eps,min_samples=1,metric=\"euclidean\").fit(latent_feat)\n",
    "        df_clus = df_clus[[\"pmid\",\"PI_IDS\"]]\n",
    "        df_clus['cluster_pred'] = y_hat.labels_\n",
    "        y_hat_comb.append(df_clus)\n",
    "\n",
    "        \n",
    "    f1, precision, recall, df_eval = metric_eval_2.get_metrics_many(y_hat_comb)\n",
    "\n",
    "    if f1 > best_F1:\n",
    "        best_f1 = f1\n",
    "        best_eps = db_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_eps)\n",
    "print(best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET SCORES FROM USE CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase = list(pd.read_csv(\"../data/test_set_author_names.csv\").drop(columns=[\"Unnamed: 0\"])[\"0\"])\n",
    "df_usecase = df[df['last_author_name'].isin(usecase)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_metric = []\n",
    "use_cases = ['2_da_same','2_da_dif','3_da', '1_da','mix_bag']\n",
    "\n",
    "for case in use_cases:\n",
    "    y_hat_comb, num_cases, num_papers = db_scan_3.db_multiple(ps,df,scaler = scaler, authors=usecase,use_case=case,weights=weights,bias=bias,epsilon=epsilon)\n",
    "    f1, precision, recall, df_eval = metric_eval_2.get_metrics_many(y_hat_comb)\n",
    "    total_metric.append([case,f1,precision,recall,df_eval, num_cases, num_papers])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_pytorch_p36)",
   "language": "python",
   "name": "conda_aws_neuron_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
